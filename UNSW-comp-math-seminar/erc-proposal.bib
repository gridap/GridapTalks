% This file was created with JabRef 2.7b.
% Encoding: UTF8
% This file was created with JabRef 2.10.
% Encoding: MacRoman

%finite elements and numerical analysis:

%badia_analysis_2006, badia_convergence_2007, codina_time_2007, badia_long-term_2010, badia_stabilized_2010, badia_finite_2011, badia_stokes_2012, badia_stabilized_2012, codina_design_2013, badia_stability_2014, badia_error_2014, badia_convergence_2014, badia_monotonicity-preserving_2014, espinoza_time_2015, espinoza_sommerfeld_2014

%CFD:

%codina_pressure_2006, badia_nodal-based_2012, badia_combined_2011, badia_unified_2009, badia_overview_2011, colomes_assessment_2015,colomes_segregated_2016


%multi-physics:

%badia_fluidstructure_2007, badia_fluidstructure_2008, badia_modular_2008, badia_splitting_2008, badia_robinrobin_2009, badia_coupling_2009, planas_approximation_2011, badia_unconditionally_2013, badia_unconditionally_2013-1, espinoza_sommerfeld_2014, badia_discrete_2015, badia_analysis_2015, smolentsev_approach_2015

%multi-scale:

%badia_force-based_2007, fish_concurrent_2007, badia_atomistic--continuum_2008, badia_multiscale_2009, badia_adaptive_2013, badia_analysis_2015

%large-scale computing:
%solvers:

%badia_algebraic_2008, badia_pressure_2008, badia_enhanced_2013, BadiaMartinPrincipe2013, Badia2014, badia_block_2014, badia_scalability_2015, art-mlbddc

@article{Verdugo2019b,
author = {Verdugo, Francesc and Mart{\'{i}}n, Alberto F. and Badia, Santiago},
doi = {10.1016/j.cma.2019.112583},
issn = {00457825},
journal = {Computer Methods in Applied Mechanics and Engineering},
month = {dec},
pages = {112583},
title = {{Distributed-memory parallelization of the aggregated unfitted finite element method}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782519304542},
volume = {357},
year = {2019}
}
@article{Ricker2019,
author = {Ricker, Richard E. and Heigel, Jarred C. and Lane, Brandon M. and Zhirnov, Ivan and Levine, Lyle E.},
doi = {10.1007/s40192-019-00157-0},
issn = {2193-9764},
journal = {Integrating Materials and Manufacturing Innovation},
mendeley-groups = {AdditiveManufacturing},
month = {oct},
publisher = {Springer Science and Business Media LLC},
title = {{Topographic Measurement of Individual Laser Tracks in Alloy 625 Bare Plates}},
year = {2019}
}

@article{Daubechies2019,
abstract = {This article is concerned with the approximation and expressive powers of deep neural networks. This is an active research area currently producing many interesting papers. The results most commonly found in the literature prove that neural networks approximate functions with classical smoothness to the same accuracy as classical linear methods of approximation, e.g. approximation by polynomials or by piecewise polynomials on prescribed partitions. However, approximation by neural networks depending on n parameters is a form of nonlinear approximation and as such should be compared with other nonlinear methods such as variable knot splines or n-term approximation from dictionaries. The performance of neural networks in targeted applications such as machine learning indicate that they actually possess even greater approximation power than these traditional methods of nonlinear approximation. The main results of this article prove that this is indeed the case. This is done by exhibiting large classes of functions which can be efficiently captured by neural networks where classical nonlinear methods fall short of the task. The present article purposefully limits itself to studying the approximation of univariate functions by ReLU networks. Many generalizations to functions of several variables and other activation functions can be envisioned. However, even in this simplest of settings considered here, a theory that completely quantifies the approximation power of neural networks is still lacking.},
archivePrefix = {arXiv},
arxivId = {1905.02199},
author = {Daubechies, I. and DeVore, R. and Foucart, S. and Hanin, B. and Petrova, G.},
journal = {arXiv:1905.02199 [cs.LG]},
eprint = {1905.02199},
file = {:home/santiago/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Daubechies et al. - 2019 - Nonlinear Approximation and (Deep) ReLU Networks.pdf:pdf},
keywords = {approximation,expressiveness,neural networks,rectified linear unit,relu},
mendeley-groups = {Deep-learning-and-PDEs},
title = {{Nonlinear Approximation and (Deep) ReLU Networks}},
url = {http://arxiv.org/abs/1905.02199},
year = {2019}
}
@article{Siegel2019,
abstract = {We prove two new results concerning the approximation properties of neural networks. Our first result gives conditions under which the outputs of the neurons in a two layer neural network are linearly independent functions. Our second result concerns the rate of approximation of a two layer neural network as the number of neurons increases. We improve upon existing results in the literature by significantly relaxing the required assumptions on the activation function and by providing a better rate of approximation. We also provide a simplified proof that the class of functions represented by a two-layer neural network is dense in any compact set if the activation function is not a polynomial.},
archivePrefix = {arXiv},
arxivId = {1904.02311},
author = {Siegel, Jonathan W. and Xu, Jinchao},
journal = {arXiv:1904.02311 [math.CA]},
eprint = {1904.02311},
file = {:home/santiago/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Siegel, Xu - 2019 - On the Approximation Properties of Neural Networks.pdf:pdf},
mendeley-groups = {Deep-learning-and-PDEs},
title = {{On the Approximation Properties of Neural Networks}},
url = {http://arxiv.org/abs/1904.02311},
year = {2019}
}
@article{He2018,
abstract = {In this paper, we investigate the relationship between deep neural networks (DNN) with rectified linear unit (ReLU) function as the activation function and continuous piecewise linear (CPWL) functions, especially CPWL functions from the simplicial linear finite element method (FEM). We first consider the special case of FEM. By exploring the DNN representation of its nodal basis functions, we present a ReLU DNN representation of CPWL in FEM. We theoretically establish that at least {\$}2{\$} hidden layers are needed in a ReLU DNN to represent any linear finite element functions in {\$}\backslashOmega \backslashsubseteq \backslashmathbb{\{}R{\}}{\^{}}d{\$} when {\$}d\backslashge2{\$}. Consequently, for {\$}d=2,3{\$} which are often encountered in scientific and engineering computing, the minimal number of two hidden layers are necessary and sufficient for any CPWL function to be represented by a ReLU DNN. Then we include a detailed account on how a general CPWL in {\$}\backslashmathbb R{\^{}}d{\$} can be represented by a ReLU DNN with at most {\$}\backslashlceil\backslashlog{\_}2(d+1)\backslashrceil{\$} hidden layers and we also give an estimation of the number of neurons in DNN that are needed in such a representation. Furthermore, using the relationship between DNN and FEM, we theoretically argue that a special class of DNN models with low bit-width are still expected to have an adequate representation power in applications. Finally, as a proof of concept, we present some numerical results for using ReLU DNNs to solve a two point boundary problem to demonstrate the potential of applying DNN for numerical solution of partial differential equations.},
archivePrefix = {arXiv},
arxivId = {1807.03973},
journal = {arXiv:1807.03973 [math.NA]},
author = {He, Juncai and Li, Lin and Xu, Jinchao and Zheng, Chunyue},
eprint = {1807.03973},
file = {:home/santiago/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2018 - ReLU Deep Neural Networks and Linear Finite Elements.pdf:pdf},
mendeley-groups = {Deep-learning-and-PDEs},
title = {{ReLU Deep Neural Networks and Linear Finite Elements}},
url = {http://arxiv.org/abs/1807.03973},
year = {2018}
}
@article{Opschoor2019,
abstract = {Approximation rate bounds for expressions of real-valued functio ns on intervals by deep neural networks (DNNs for short) are established. The approximation results are giv en for DNNs based on ReLU activation functions, and the approximation error is measured with respect to Sobolev norms. It is shown that ReLU DNNs allow for essentially the same approximation rates as n onlinear, variable-order, free-knot (or so-called “ hp -adaptive”) spline approximations and spectral approximatio ns, for a wide range of Sobolev and Besov spaces. In particular, exponential convergence rates in terms of the DNN size for piecewise Gevrey functions with point singularities are established. Com bined with recent results on ReLU DNN expression of rational, oscillatory, and high-dimensional fun ctions, this corroborates that ReLU DNNs match the approximation power of “best in class” schemes for a w ide range of approximations.},
author = {Opschoor, J A A and Petersen, Philipp and Schwab, Christoph},
journal = {SAM 07},
file = {:home/santiago/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Opschoor, Petersen, Schwab - 2019 - Deep ReLU Networks and High-Order Finite Element Methods.pdf:pdf},
keywords = {adaptivity,deep neural networks,finite element methods,function approximation},
mendeley-groups = {Deep-learning-and-PDEs},
pages = {28},
title = {{Deep ReLU Networks and High-Order Finite Element Methods}},
url = {https://www.sam.math.ethz.ch/sam{\_}reports/reports{\_}final/reports2019/2019-07.pdf},
year = {2019}
}
@article{Badia2019-fempar,
author = {Badia, Santiago and Mart{\'{i}}n, Alberto F.},
doi = {10.1016/j.cpc.2019.107059},
issn = {00104655},
journal = {Computer Physics Communications},
mendeley-groups = {Members{\_}libraries/SB},
pages = {107059},
title = {{A tutorial-driven introduction to the parallel finite element library FEMPAR v1.0.0}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0010465519303832},
year = {2019}
}

@article{Neiva2020,
abstract = {A R T I C L E I N F O Keywords: Additive manufacturing (AM) Powder-bed fusion (PBF) Selective laser melting (SLM) Finite elements (FE) Thermal analysis High performance computing (HPC) A B S T R A C T Among metal additive manufacturing technologies, powder-bed fusion features very thin layers and rapid solid-ification rates, leading to long build jobs and a highly localized process. Many efforts are being devoted to accelerate simulation times for practical industrial applications. The new approach suggested here, the virtual domain approximation, is a physics-based rationale for spatial reduction of the domain in the thermal finite-element analysis at the part scale. Computational experiments address, among others, validation against a large physical experiment of 17.5 [cm 3 ] of deposited volume in 647 layers. For fast and automatic parameter estimation at such level of complexity, a high-performance computing framework is employed. It couples FEMPAR-AM, a specialized parallel finite-element software, with Dakota, for the parametric exploration. Compared to previous state-of-the-art, this formulation provides higher accuracy at the same computational cost. This sets the path to a fully virtualized model, considering an upwards-moving domain covering the last printed layers.},
author = {Neiva, Eric and Chiumenti, Michele and Cervera, Miguel and Salsi, Emilio and Piscopo, Gabriele and Badia, Santiago and Mart{\'{i}}n, Alberto F and Chen, Zhuoer and Lee, Caroline and Davies, Christopher},
doi = {10.1016/j.finel.2019.103343},
file = {::},
journal = {Finite Elements in Analysis and Design},
pages = {103343},
title = {{Numerical modelling of heat transfer and experimental validation in powder-bed fusion with the virtual domain approximation}},
url = {https://doi.org/10.1016/j.finel.2019.103343},
volume = {168},
year = {2020}
}
@article{Chiumenti2017d,
abstract = {In this work a finite-element framework for the numerical simulation of the heat transfer analysis of additive manufacturing processes by powder-bed technologies, such as Selective Laser Melting, is presented. These kind of technologies allow for a layer-by-layer metal deposition process to cost-effectively create, directly from a CAD model, complex functional parts such as turbine blades, fuel injectors, heat exchangers, medical implants, among others. The numerical model proposed accounts for different heat dissipation mechanisms through the surrounding environment and is supplemented by a finite-element activation strategy, based on the born-dead elements technique, to follow the growth of the geometry driven by the metal deposition process, in such a way that the same scanning pattern sent to the numerical control system of the AM machine is used. An experimental campaign has been carried out at the Monash Centre for Additive Manufacturing using an EOSINT-M280 machine where it was possible to fabricate different benchmark geometries, as well as to record the temperature measurements at different thermocouple locations. The experiment consisted in the simultaneous printing of two walls with a total deposition volume of 107 cm3in 992 layers and about 33,500 s build time. A large number of numerical simulations have been carried out to calibrate the thermal FE framework in terms of the thermophysical properties of both solid and powder materials and suitable boundary conditions. Furthermore, the large size of the experiment motivated the investigation of two different model reduction strategies: exclusion of the powder-bed from the computational domain and simplified scanning strategies. All these methods are analysed in terms of accuracy, computational effort and suitable applications.},
author = {Chiumenti, Michele and Neiva, Eric and Salsi, Emilio and Cervera, Miguel and Badia, Santiago and Moya, Joan and Chen, Zhuoer and Lee, Caroline and Davies, Christopher},
doi = {10.1016/j.addma.2017.09.002},
issn = {22148604},
journal = {Additive Manufacturing},
keywords = {Additive manufacturing (AM) process,Finite-element (FE) modelling,Heat transfer analysis,Metal deposition (MD) process,No JCR indexed,Powder-bed technologies,Selective Laser Melting (SLM)},
mendeley-tags = {No JCR indexed},
pages = {171--185},
title = {{Numerical modelling and experimental validation in Selective Laser Melting}},
volume = {18},
year = {2017}
}

@article{Neiva2019a,
author = {Neiva, Eric and Badia, Santiago and Mart{\'{i}}n, Alberto F. and Chiumenti, Michele},
doi = {10.1002/nme.6085},
issn = {0029-5981},
journal = {International Journal for Numerical Methods in Engineering},
mendeley-groups = {Members{\_}libraries/SB},
month = {sep},
number = {11},
pages = {1098--1125},
title = {{A scalable parallel finite element framework for growing geometries. Application to metal additive manufacturing}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.6085},
volume = {119},
year = {2019}
}

@article{Badia2019a,
abstract = {We present highly scalable parallel distributed-memory algorithms and associated data structures for a generic finite element framework that supports {\$}h{\$}-adaptivity on computational domains represented as multiple connected adaptive trees, thus providing multi-scale resolution on problems governed by partial differential equations. The framework is grounded on a rich representation of the adaptive mesh suitable for generic finite elements that is built on top of a low-level, light-weight forest-of-trees data structure handled by a specialized, highly parallel adaptive meshing engine. Along the way, we have identified the requirements that the forest-of-trees layer must fulfill to be coupled into our framework. Essentially, it must be able to describe neighboring relationships between cells in the adapted mesh (apart from hierarchical relationships) across the lower-dimensional objects at the boundary of the cells. Atop this two-layered mesh representation, we build the rest of data structures required for the numerical integration and assembly of the discrete system of linear equations. We consider algorithms that are suitable for both subassembled and fully-assembled distributed data layouts of linear system matrices. The proposed framework has been implemented within the FEMPAR scientific software library, using p4est as a practical forest-of-octrees demonstrator. A comprehensive strong scaling study of this implementation when applied to Poisson and Maxwell problems reveals remarkable scalability up to 32.2K CPU cores and 482.2M degrees of freedom. Besides, the implementation in FEMPAR of the proposed approach is up to 2.6 and 3.4 times faster than the state-of-the-art deal.ii finite element software in the {\$}h{\$}-adaptive approximation of a Poisson problem with first- and second-order Lagrangian finite elements, respectively (excluding the linear solver step from the comparison).},
archivePrefix = {arXiv},
arxivId = {1907.03709},
author = {Badia, Santiago and Mart{\'{i}}n, Alberto F. and Neiva, Eric and Verdugo, Francesc},
eprint = {1907.03709},
file = {::},
mendeley-groups = {Members{\_}libraries/SB},
month = {jul},
title = {{A generic finite element framework on parallel tree-based adaptive meshes}},
journal = {Submitted, arXiv:1907.03709 [cs.MS]},
url = {http://arxiv.org/abs/1907.03709},
year = {2019}
}
@book{Ainsworth2000,
abstract = {"A Wiley-Interscience publication." Front Matter -- Introduction -- Explicit A Posteriori Estimators -- Implicit A Posteriori Estimators -- Recovery-Based Error Estimators -- Estimators, Indicators, and Hierarchic Bases -- The Equilibrated Residual Method -- Methodology for the Comparison of Estimators -- Estimation of the Errors in Quantities of Interest -- Some Extensions -- References -- Index -- Pure and Applied Mathematics. A Posteriori Error Estimation: The Setting -- Status and Scope -- Finite Element Nomenclature -- Sobolev Spaces -- Inverse Estimates -- Finite Element Partitions -- Finite Element Spaces on Triangles -- Finite Element Spaces on Quadrilaterals -- Properties of Lagrange Basis Functions -- Finite Element Interpolation -- Patches of Elements -- Regularized Approximation Operators -- Model Problem -- Properties of A Posteriori Error Estimators -- Explicit A Posteriori Estimators -- A Simple A Posteriori Error Estimate -- Efficiency of Estimator -- Bubble Functions -- Bounds on the Residuals -- Proof of Two-Sided Bounds on the Error -- A Simple Explicit Least Squares Error Estimator -- Estimates for the Pointwise Error -- Regularized Point Load -- Regularized Green's Function -- Two-Sided Bounds on the Pointwise Error -- Implicit A Posteriori Estimators -- The Subdomain Residual Method -- Formulation of Subdomain Residual Problem -- Preliminaries -- Equivalence of Estimator -- Treatment of Residual Problems -- The Element Residual Method -- Formulation of Local Residual Problem -- Solvability of the Local Problems -- The Classical Element Residual Method -- Relationship with Explicit Error Estimators -- Efficiency and Reliability of the Estimator -- The Influence and Selection of Subspaces -- Exact Solution of Element Residual Problem -- Analysis and Selection of Approximate Subspaces -- Recovery-Based Error Estimators -- Examples of Recovery-Based Estimators -- An Error Estimator for a Model Problem in One Dimension.},
author = {Ainsworth, Mark and Oden, J. Tinsley},
isbn = {9781118032824},
mendeley-groups = {Articles{\_}libraries/art036},
pages = {240},
publisher = {Wiley},
title = {{A posteriori error estimation in finite element analysis}},
year = {2000}
}
@article{evans_isogeometric_2013,
abstract = {Divergence-conforming B-splines are developed for application to the incompressible Navier–Stokes equations on geometrically mapped domains. These enable smooth, pointwise divergence-free solutions and thus satisfy mass conservation in the strongest possible sense. Semi-discrete methods based on divergence-conforming B-splines are shown to conserve linear and angular momentum and satisfy balance laws for energy, vorticity, enstrophy, and helicity. These are geometric structure-preserving quantities and numerical simulations that are sensitive to them are shown to be qualitatively correct and quantitatively accurate. The methods developed are anticipated to open new doors to the practical calculation of complex flows and to studies of their physical behavior.},
author = {Evans, John A and Hughes, Thomas J R},
doi = {10.1016/j.jcp.2013.01.006},
file = {::},
issn = {0021-9991},
journal = {Journal of Computational Physics},
keywords = {B-splines,Divergence-conforming discretizations,Structure-preserving discretizations,incompressible Navier–Stokes equations,isogeometric analysis},
mendeley-groups = {DivFree},
month = {may},
pages = {141--167},
title = {{Isogeometric divergence-conforming {B}-splines for the unsteady {Navier}–{Stokes} equations}},
url = {http://www.sciencedirect.com/science/article/pii/S0021999113000363},
volume = {241},
year = {2013}
}
@article{Bonilla2019b,
abstract = {In this work we propose a nonlinear stabilization technique for convection–diffusion–reaction and pure transport problems discretized with space–time isogeometric analysis. The stabilization is based on a graph-theoretic artificial diffusion operator and a novel shock detector for isogeometric analysis. Stabilization in time and space directions are performed similarly, which allow us to use high-order discretizations in time without any CFL-like condition. The method is proven to yield solutions that satisfy the discrete maximum principle (DMP) unconditionally for arbitrary order. In addition, the stabilization is linearity preserving in a space–time sense. Moreover, the scheme is proven to be Lipschitz continuous ensuring that the nonlinear problem is well-posed. Solving large problems using a space–time discretization can become highly costly. Therefore, we also propose a partitioned space–time scheme that allows us to select the length of every time slab, and solve sequentially for every subdomain. As a result, the computational cost is reduced while the stability and convergence properties of the scheme remain unaltered. In addition, we propose a twice differentiable version of the stabilization scheme, which enjoys the same stability properties while the nonlinear convergence is significantly improved. Finally, the proposed schemes are assessed with numerical experiments. In particular, we considered steady and transient pure convection and convection–diffusion problems in one and two dimensions.},
author = {Bonilla, Jes{\'{u}}s and Badia, Santiago},
doi = {10.1016/J.CMA.2019.05.042},
issn = {0045-7825},
journal = {Computer Methods in Applied Mechanics and Engineering},
mendeley-groups = {Members{\_}libraries,Members{\_}libraries/SB},
month = {sep},
pages = {422--440},
publisher = {North-Holland},
title = {{Maximum-principle preserving space–time isogeometric analysis}},
url = {https://www-sciencedirect-com.ezproxy.lib.monash.edu.au/science/article/pii/S0045782519303123},
volume = {354},
year = {2019}
}
@article{Verdugo2019a,
annote = {Submitted},
archivePrefix = {arXiv},
arxivId = {1910.01412},
author = {Verdugo, Francesc and Badia, Santiago},
eprint = {1910.01412},
mendeley-groups = {Members{\_}libraries/SB},
title = {{A user-guide to Gridap -- grid-based approximation of partial differential equations in Julia}},
journal = {Submitted, arXiv:1910.01412 [cs.MS]},
url = {https://arxiv.org/abs/1910.01412},
year = {2019}
}
@article{Fries2017,
abstract = {An accurate implicit description of geometries is enabled by the level-set method. Level-set data is given at the nodes of a higher-order background mesh and the interpolated zero-level sets imply boundaries of the domain or interfaces within. The higher-order accurate integration of elements cut by the zero-level sets is described. The proposed strategy relies on an automatic meshing of the cut elements. Firstly, the zero-level sets are identified and meshed by higher-order surface elements. Secondly, the cut elements are decomposed into conforming sub-elements on the two sides of the zero-level sets. Any quadrature rule may then be employed within the sub-elements. The approach is described in two and three dimensions without any requirements on the background meshes. Special attention is given to the consideration of corners and edges of the implicit geometries.},
archivePrefix = {arXiv},
arxivId = {1706.00578},
author = {Fries, T. P. and Omerovi{\'{c}}, S. and Sch{\"{o}}llhammer, D. and Steidl, J.},
doi = {10.1016/j.cma.2016.10.019},
eprint = {1706.00578},
issn = {00457825},
journal = {Computer Methods in Applied Mechanics and Engineering},
keywords = {Fictitious domain method,GFEM,Interface capturing,Level-set method,Numerical integration,XFEM},
mendeley-groups = {Embedded/High Order Unfitted},
month = {jan},
pages = {759--784},
publisher = {Elsevier B.V.},
title = {{Higher-order meshing of implicit geometries—Part I: Integration and interpolation in cut elements}},
volume = {313},
year = {2017}
}
@article{Engvall2017,
abstract = {A well-known problem in the field of isogeometric analysis is that of surface-to-volume parameterization. In CAD packages, solid objects are represented by a collection of NURBS or T-spline surfaces, but to perform engineering analysis for many real world problems, we must find a way to parameterize the volumes of these objects as well. This has proven to be difficult using traditional isogeometric methods, as the tensor-product nature of trivariate NURBS and T-splines limit their ability to provide analysis suitable parameterizations of arbitrarily complex volumes. To overcome the limitations of trivariate NURBS and T-splines, we propose the use of unstructured Bernstein–B{\'{e}}zier discretizations. In earlier work, we demonstrated the feasibility of this approach in two dimensions through the construction of an automatic mesh generation environment capable of generating geometrically exact unstructured meshes comprised of rational B{\'{e}}zier triangles. This paper extends the concepts presented in our previous work to unstructured tetrahedral and mixed-element meshes in three dimensions. The main contributions of this paper are three-fold. First, we present a framework for creating geometrically exact meshes comprised of rational B{\'{e}}zier hexahedra, tetrahedra, wedges and pyramids through degree elevation of suitable linear meshes. We additionally discuss how our approach may be applied to higher-order mesh generation for more traditional finite element approaches. Next, we propose two quality metrics for three-dimensional curvilinear meshes, and discuss some important considerations for mesh quality of higher-order meshes. Finally, we demonstrate the analysis suitability of the meshes constructed using our framework through numerical examples.},
author = {Engvall, Luke and Evans, John A.},
doi = {10.1016/j.cma.2017.02.017},
issn = {00457825},
journal = {Computer Methods in Applied Mechanics and Engineering},
keywords = {Higher-order mesh generation,Isogeometric analysis},
mendeley-groups = {Embedded/High Order Unfitted},
month = {jun},
pages = {83--123},
publisher = {Elsevier B.V.},
title = {{Isogeometric unstructured tetrahedral and mixed-element Bernstein–B{\'{e}}zier discretizations}},
volume = {319},
year = {2017}
}

@article{roters_damask:_2012,
	series = {{IUTAM} {Symposium} on {Linking} {Scales} in {Computations}: {From} {Microstructure} to {Macro}-scale {Properties}},
	title = {{DAMASK}: the {Düsseldorf} {Advanced} {MAterial} {Simulation} {Kit} for studying crystal plasticity using an {FE} based or a spectral numerical solver},
	volume = {3},
	issn = {2210-9838},
	shorttitle = {{DAMASK}},
	url = {http://www.sciencedirect.com/science/article/pii/S2210983812000028},
	doi = {10.1016/j.piutam.2012.03.001},
	abstract = {The solution of a continuum mechanical boundary value problem requires a constitutive response that connects deformation and stress at each material point. Such connection can be regarded as three separate hierarchical problems. At the top-most level, partitioning of the (mean) boundary values of the material point among its microstructural constituents and the associated homogenization of their response is required, provided there is more than one constituent present. Second, based on an elastoplastic decomposition of (finite strain) deformation, these responses follow from explicit or implicit time integration of the plastic deformation rate per constituent. Third, to establish the latter, a state variable-based constitutive law needs to be interrogated and its state updated.The Düsseldorf Advanced MAterial Simulation Kit (DAMASK) reflects this hierarchy as it is built in a strictly modular way. This modular structure makes it easy to add additional constitutive models as well as homogenization schemes. Moreover it interfaces with a number of FE solvers as well as a spectral solver using an FFT.We demonstrate the versatility of such a modular framework by considering three scenarios: Selective refinement of the constitutive material description within a single geometry, component-scale forming simulations comparing di\_erent homogenization schemes, and comparison of representative volume element simulations based on the FEM and the spectral solver.},
	urldate = {2017-01-31},
	journal = {Procedia IUTAM},
	author = {Roters, F. and Eisenlohr, P. and Kords, C. and Tjahjanto, D. D. and Diehl, M. and Raabe, D.},
	year = {2012},
	keywords = {constitutive model, CPFEM, FFT, Spectral method},
	pages = {3--10},
	file = {1-s2.0-S2210983812000028-main.pdf:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/HDKHU5G2/1-s2.0-S2210983812000028-main.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/AG7HN5GN/S2210983812000028.html:text/html}
}

@article{roters_overview_2010,
	title = {Overview of constitutive laws, kinematics, homogenization and multiscale methods in crystal plasticity finite-element modeling: {Theory}, experiments, applications},
	volume = {58},
	issn = {1359-6454},
	shorttitle = {Overview of constitutive laws, kinematics, homogenization and multiscale methods in crystal plasticity finite-element modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S1359645409007617},
	doi = {10.1016/j.actamat.2009.10.058},
	abstract = {This article reviews continuum-based variational formulations for describing the elastic–plastic deformation of anisotropic heterogeneous crystalline matter. These approaches, commonly referred to as crystal plasticity finite-element models, are important both for basic microstructure-based mechanical predictions as well as for engineering design and performance simulations involving anisotropic media. Besides the discussion of the constitutive laws, kinematics, homogenization schemes and multiscale approaches behind these methods, we also present some examples, including, in particular, comparisons of the predictions with experiments. The applications stem from such diverse fields as orientation stability, microbeam bending, single-crystal and bicrystal deformation, nanoindentation, recrystallization, multiphase steel (TRIP) deformation, and damage prediction for the microscopic and mesoscopic scales and multiscale predictions of rolling textures, cup drawing, Lankfort (r) values and stamping simulations for the macroscopic scale.},
	number = {4},
	urldate = {2017-01-30},
	journal = {Acta Materialia},
	author = {Roters, F. and Eisenlohr, P. and Hantcherli, L. and Tjahjanto, D. D. and Bieler, T. R. and Raabe, D.},
	year = {2010},
	keywords = {Anisotropy, Crystal plasticity, Indentation, Multiphase steel, Twinning},
	pages = {1152--1211},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/2FV8ATKM/Roters et al. - 2010 - Overview of constitutive laws, kinematics, homogen.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/G4NWIHWM/S1359645409007617.html:text/html}
}

@article{meier_crystal-plasticity_2014,
	title = {Crystal-plasticity based thermo-mechanical modeling of {Al}-components in integrated circuits},
	volume = {94},
	issn = {0927-0256},
	url = {https://www.sciencedirect.com/science/article/pii/S0927025614001712},
	doi = {10.1016/j.commatsci.2014.03.020},
	series = {{IWCMM}23 Special Issue},
	abstract = {Growing demands on performance and durability of integrated circuits ({ICs}) require an understanding of possible failure mechanisms. One main cause for damage of {ICs} arises from thermo-mechanical loads of the involved materials as a result of current pulses. The mismatch in thermal expansion leads to stresses which cause crack initiation and, consequently, short circuits and the loss of functionality of the assembly.
This study presents a series of numerical simulations using a three dimensional model assembly considering aluminum conductors sputtered on a silicon substrate, surrounded by an interlayer dielectric and covered by an aluminum metallization plate with passivation layer. Different conductor path geometries are investigated. The model assembly is loaded by a cyclic heat flow. The thermo-mechanical problem is solved utilizing the Abaqus/Standard solver in combination with a user-defined material routine which takes into account the microstructure and the grain orientation of the aluminum, heat flow, thermal expansion and temperature dependent material behavior. The development of stresses and strains within the interlayer dielectric and within the different conductor path geometries is investigated and compared with experimental results. Observations like surface roughening and crack initiation can be predicted qualitatively correct.},
	pages = {122--131},
	journal = {Computational Materials Science},
	shortjournal = {Computational Materials Science},
	author = {Meier, Felix and Schwarz, Cornelia and Werner, Ewald},
	urldate = {2017-01-30},
	year = {2014},
	keywords = {Active cycling, Aluminum, Anisotropy, Crystal plasticity, reliability, Thermo-mechanical analysis},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/WH2NDUVD/Meier et al. - 2014 - Crystal-plasticity based thermo-mechanical modelin.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/DP34KMW4/S0927025614001712.html:text/html}
}

@article{rude_research_2016,
	title = {Research and Education in Computational Science and Engineering},
	url = {http://arxiv.org/abs/1610.02608},
	abstract = {Over the past two decades the field of computational science and engineering ({CSE}) has penetrated both basic and applied research in academia, industry, and laboratories to advance discovery, optimize systems, support decision-makers, and educate the scientific and engineering workforce. Informed by centuries of theory and experiment, {CSE} performs computational experiments to answer questions that neither theory nor experiment alone is equipped to answer. {CSE} provides scientists and engineers of all persuasions with algorithmic inventions and software systems that transcend disciplines and scales. Carried on a wave of digital technology, {CSE} brings the power of parallelism to bear on troves of data. Mathematics-based advanced computing has become a prevalent means of discovery and innovation in essentially all areas of science, engineering, technology, and society; and the {CSE} community is at the core of this transformation. However, a combination of disruptive developments---including the architectural complexity of extreme-scale computing, the data revolution that engulfs the planet, and the specialization required to follow the applications to new frontiers---is redefining the scope and reach of the {CSE} endeavor. This report describes the rapid expansion of {CSE} and the challenges to sustaining its bold advances. The report also presents strategies and directions for {CSE} research and education for the next decade.},
	journal = {{arXiv}:1610.02608 [cs, math, stat]},
	author = {Rüde, Ulrich and Willcox, Karen and {McInnes}, Lois Curfman and De Sterck, Hans and Biros, George and Bungartz, Hans and Corones, James and Cramer, Evin and Crowley, James and Ghattas, Omar and Gunzburger, Max and Hanke, Michael and Harrison, Robert and Heroux, Michael and Hesthaven, Jan and Jimack, Peter and Johnson, Chris and Jordan, Kirk E. and Keyes, David E. and Krause, Rolf and Kumar, Vipin and Mayer, Stefan and Meza, Juan and Mørken, Knut Martin and Oden, J. Tinsley and Petzold, Linda and Raghavan, Padma and Shontz, Suzanne M. and Trefethen, Anne and Turner, Peter and Voevodin, Vladimir and Wohlmuth, Barbara and Woodward, Carol S.},
	urldate = {2017-01-27},
	year = {2016},
	eprinttype = {arxiv},
	eprint = {1610.02608},
	keywords = {00A72, 62-07, 68U20, 68W01, 68W10, 97A99, 97M10, 97N80, 97R20, 97R30, Computer Science - Computational Engineering, Finance, and Science, G.0, G.4, I.6, J.0, J.2, J.3, J.4, J.6, J.7, K.3.2, Mathematics - History and Overview, Statistics - Other Statistics},
	file = {arXiv.org Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/TCNP22J6/1610.html:text/html}
}


@article{scheunemann_design_2015,
	title = {Design of 3D statistically similar Representative Volume Elements based on Minkowski functionals},
	volume = {90},
	issn = {0167-6636},
	url = {//www.sciencedirect.com/science/article/pii/S0167663615000745},
	doi = {10.1016/j.mechmat.2015.03.005},
	series = {Proceedings of the {IUTAM} Symposium on Micromechanics of Defects in Solids},
	abstract = {In this paper an extended optimization procedure is proposed for the construction of statistically similar {RVEs} ({SSRVEs}) which are defined as artificial microstructures showing a lower complexity than the associated real microstructures. This enables a computationally efficient discretization required for numerical calculations of microscopic boundary value problems and leads therefore to more efficient computational two-scale schemes. The optimization procedure is staggered and consists of an outer and an inner optimization problem. The outer problem treats different types of morphology parameterizations, different sets of statistical measures and different sets of weighting factors needed in the inner problem to minimize differences of mechanical errors that compare the response of the {SSRVE} with a target (real) microstructure. The inner problem minimizes differences of statistical measures describing the microstructure morphology for fixed parameterization type, statistical measures and weighting factors. The main contribution here is the analysis of new microstructure descriptors based on tensor-valued Minkowski functionals, whose numerical calculation requires less time compared to e.g. lineal-path functions. Thereby, a more efficient inner optimization problem can be realized and thus, an automated solution of the outer optimization problem becomes more practicable. Representative examples demonstrate the performance of the proposed method. It turns out that the evaluation of objective functions formulated in terms of the Minkowski functionals is almost 2000 times faster than functions taking into account lineal-path functions.},
	pages = {185--201},
	journal = {Mechanics of Materials},
	shortjournal = {Mechanics of Materials},
	author = {Scheunemann, L. and Balzani, D. and Brands, D. and Schröder, J.},
	urldate = {2017-01-27},
	year = {2015},
	keywords = {{DP} steel, Lineal-path function, Minkowski functionals, Multiscale modeling, Spectral density, Statistical measures},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/G44HKEK9/Scheunemann et al. - 2015 - Design of 3D statistically similar Representative .pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/I82AR4TE/S0167663615000745.html:text/html}
}

@article{fritzen_finite_2016,
	title = {The finite element square reduced ({FE}2R) method with {GPU} acceleration: towards three-dimensional two-scale simulations},
	volume = {107},
	issn = {1097-0207},
	url = {http://onlinelibrary.wiley.com.recursos.biblioteca.upc.edu/doi/10.1002/nme.5188/abstract},
	doi = {10.1002/nme.5188},
	shorttitle = {The finite element square reduced ({FE}2R) method with {GPU} acceleration},
	abstract = {The {FE}2 method is a renown computational multiscale simulation technique for solid materials with fine-scale microstructure. It allows for the accurate prediction of the mechanical behavior of structures made of heterogeneous materials with nonlinear material behavior. However, the {FE}2 method leads to excessive {CPU} time and storage requirements, even for academic two-dimensional problems. In order to allow for realistic three-dimensional two-scale simulations, a significant reduction of the {CPU} and memory usage is required. For this purpose, the authors have recently proposed a reduced basis homogenization scheme based on a mixed incremental variational principle. The approach exploits the potential structure of generalized standard materials. Thereby, important speed-ups and memory savings can be achieved. Using high-performance {GPUs}, the reduced-basis method can be further accelerated. In the present contribution, our previous works are combined and extended to form the {FE}2-reduced method: the {FE}2R. The {FE}2R can be used to simulate three-dimensional structural problems with consideration of the nonlinearity and microstructure of the underlying material at acceptable computational cost. Thereby, it allows for a new level of complexity in nonlinear multiscale simulations. Numerical examples illustrate the capabilities of the chosen approach. Copyright © 2016 John Wiley \& Sons, Ltd.},
	pages = {853--881},
	number = {10},
	journal = {International Journal for Numerical Methods in Engineering},
	shortjournal = {Int. J. Numer. Meth. Engng},
	author = {Fritzen, Felix and Hodapp, Max},
	urldate = {2017-01-27},
	year = {2016},
	langid = {english},
	keywords = {Computational homogenization, finite element square reduced ({FE}2R), generalized standard material ({GSM}), graphics processing unit ({GPU}), mixed incremental variational approach, Nvidia {CUDA}, potential based reduced basis model order reduction ({pRBMOR})},
	file = {Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/ZCR3APVE/Fritzen and Hodapp - 2016 - The finite element square reduced (FE2R) method wi.pdf:application/pdf;Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/ZSW8B25R/abstract.html:text/html}
}


@article{lim_application_2013,
	title = {Application of generalized non-{S}chmid yield law to low-temperature plasticity in bcc transition metals},
	volume = {21},
	issn = {0965-0393},
	url = {http://stacks.iop.org/0965-0393/21/i=4/a=045015},
	doi = {10.1088/0965-0393/21/4/045015},
	abstract = {In this work, a generalized yield criterion that captures non-Schmid effects is proposed and implemented into a finite element crystal plasticity model to simulate plastic deformation of single and polycrystals. The parameters required for the constitutive formulation were calibrated to deformation experiments on single crystals. This model is used to investigate the effects of non-Schmid effects on the predictions of the stress–strain response and texture evolution in body-centered-cubic (bcc) metals. The non-Schmid contributions are required to accurately predict the stress–strain response of single crystals, and the concomitant non-associativity of the flow also increases the tendency of localization in polycrystal deformations.},
	pages = {045015},
	number = {4},
	journal = {Modelling and Simulation in Materials Science and Engineering},
	shortjournal = {Modelling Simul. Mater. Sci. Eng.},
	author = {Lim, H. and Weinberger, C. R. and Battaile, C. C. and Buchheit, T. E.},
	urldate = {2017-01-27},
	year = {2013},
	langid = {english}
}

@article{abdeljawad_connecting_2014,
	title = {Connecting microstructural coarsening processes to electrochemical performance in solid oxide fuel cells: An integrated modeling approach},
	volume = {250},
	issn = {0378-7753},
	url = {//www.sciencedirect.com/science/article/pii/S0378775313017874},
	doi = {10.1016/j.jpowsour.2013.10.121},
	shorttitle = {Connecting microstructural coarsening processes to electrochemical performance in solid oxide fuel cells},
	abstract = {In solid oxide fuel cells ({SOFCs}), Ni coarsening in porous anodes that are comprised of Ni and yttria stabilized zirconia ({YSZ}) leads to changes in several microstructural attributes, which affect the electrochemical performance. Herein we present an integrated modeling approach, where a dynamic mesoscale phase field model is linked with a stationary macroscale electrochemical cell level model in order to assess the role of Ni coarsening on the performance of {SOFCs}. The phase field model is capable of capturing the morphological evolution of Ni and accounting for its polycrystalline nature, while the electrochemical model encompasses the entire set of processes of gas transport, electronic and ionic conduction as well as the electrochemical reactions. Microstructural features are extracted from the phase field model as anode systems evolve over time and employed as effective properties in the electrochemical model. Simulation results highlight the importance of Ni and {YSZ} particle size and ratio on both the microstructural stability and electrochemical performance of {SOFCs}. In particular, it is shown that, for the classes of microstructures employed in this work, coarsening of Ni particles can either improve or diminish the maximum power density relative to the as-sintered ones, depending on the initial particle size.},
	pages = {319--331},
	journal = {Journal of Power Sources},
	shortjournal = {Journal of Power Sources},
	author = {Abdeljawad, Fadi and Völker, Benjamin and Davis, Ryan and {McMeeking}, Robert M. and Haataja, Mikko},
	urldate = {2017-01-27},
	year = {2014},
	keywords = {Electrochemical performance, Microstructure, Ni coarsening, Phase field, Solid oxide fuel cell ({SOFC}), Topological evolution},
	file = {ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/P5FQ8XXG/S0378775313017874.html:text/html}
}

@article{lim_grain-scale_2014,
	title = {Grain-scale experimental validation of crystal plasticity finite element simulations of tantalum oligocrystals},
	volume = {60},
	issn = {0749-6419},
	url = {//www.sciencedirect.com/science/article/pii/S0749641914001004},
	doi = {10.1016/j.ijplas.2014.05.004},
	abstract = {In this work, the grain-scale elastoplastic deformation behavior of coarse-grained body centered cubic ({BCC}) tantalum was simulated using a crystal plasticity finite element method ({CP}-{FEM}) and compared to experimental measurements of intragranular strain and rotation fields. To mitigate the effects of unknown subsurface microstructure, tantalum tensile specimens with millimeter-sized grains provided nearly constant microstructure through the thickness of the tensile bar. Experimental validation was performed in three ways: (1) electron backscatter diffraction ({EBSD}) to map intragranular rotation, (2) high-resolution digital image correlation ({HR}-{DIC}) to map the surface strain field, and (3) surface profilometry to map the out-of-plane topographic distortion. To ensure a direct apples-to-apples comparison to experiments, the details of the initial microstructure and boundary conditions were carefully replicated in the model. The deformation predictions using this novel {BCC} {CP}-{FEM} model for tantalum agree reasonably well with the experimental measurements. In addition, the model successfully predicted the failure location of a specimen subjected to large plastic strains. Several model parameters were explored that influence the {BCC} {CP}-{FEM} predictions such as the mesh dependence, the choice of active slip planes in {BCC} metals and the assignment of initial crystal orientations.},
	pages = {1--18},
	journal = {International Journal of Plasticity},
	shortjournal = {International Journal of Plasticity},
	author = {Lim, H. and Carroll, J. D. and Battaile, C. C. and Buchheit, T. E. and Boyce, B. L. and Weinberger, C. R.},
	urldate = {2017-01-27},
	year = {2014},
	keywords = {A. Microstructures, B. Crystal plasticity, C. Digital image correlation, C. Electron microscopy, C. Finite elements},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/N9Z5MC7D/Lim et al. - 2014 - Grain-scale experimental validation of crystal pla.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/J2R65GKR/S0749641914001004.html:text/html}
}

@article{lim_incorporating_2016,
	title = {Incorporating physically-based microstructures in materials modeling: Bridging phase field and crystal plasticity frameworks},
	volume = {24},
	issn = {0965-0393},
	url = {http://stacks.iop.org/0965-0393/24/i=4/a=045016},
	doi = {10.1088/0965-0393/24/4/045016},
	shorttitle = {Incorporating physically-based microstructures in materials modeling},
	abstract = {The mechanical properties of materials systems are highly influenced by various features at the microstructural level. The ability to capture these heterogeneities and incorporate them into continuum-scale frameworks of the deformation behavior is considered a key step in the development of complex non-local models of failure. In this study, we present a modeling framework that incorporates physically-based realizations of polycrystalline aggregates from a phase field ({PF}) model into a crystal plasticity finite element ({CP}-{FE}) framework. Simulated annealing via the {PF} model yields ensembles of materials microstructures with various grain sizes and shapes. With the aid of a novel {FE} meshing technique, {FE} discretizations of these microstructures are generated, where several key features, such as conformity to interfaces, and triple junction angles, are preserved. The discretizations are then used in the {CP}-{FE} framework to simulate the mechanical response of polycrystalline α -iron. It is shown that the conformal discretization across interfaces reduces artificial stress localization commonly observed in non-conformal {FE} discretizations. The work presented herein is a first step towards incorporating physically-based microstructures in lieu of the overly simplified representations that are commonly used. In broader terms, the proposed framework provides future avenues to explore bridging models of materials processes, e.g. additive manufacturing and microstructure evolution of multi-phase multi-component systems, into continuum-scale frameworks of the mechanical properties.},
	pages = {045016},
	number = {4},
	journal = {Modelling and Simulation in Materials Science and Engineering},
	shortjournal = {Modelling Simul. Mater. Sci. Eng.},
	author = {Lim, Hojun and Abdeljawad, Fadi and Owen, Steven J. and Hanks, Byron W. and Foulk, James W. and Battaile, Corbett C.},
	urldate = {2017-01-27},
	year = {2016},
	langid = {english},
	file = {msms_24_4_045016.pdf:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/P3GAQV8H/msms_24_4_045016.pdf:application/pdf}
}

@article{abdeljawad_stabilization_2015,
	title = {Stabilization of nanocrystalline alloys via grain boundary segregation: A diffuse interface model},
	volume = {101},
	issn = {1359-6454},
	url = {//www.sciencedirect.com/science/article/pii/S1359645415005418},
	doi = {10.1016/j.actamat.2015.07.058},
	shorttitle = {Stabilization of nanocrystalline alloys via grain boundary segregation},
	abstract = {Recent experimental and theoretical findings suggest that nanocrystalline binary alloys can be stabilized against interface-driven homogenization processes via grain boundary ({GB}) solute segregation mechanism. However, a detailed understanding of this process requires detangling the thermodynamic aspect, {GB} energy, from the kinetic one, {GB} mobility. In this work, we present a diffuse-interface model of {GB} segregation in binary metallic alloys that is capable of accounting for bulk thermodynamics, interfacial energies, and the interaction of alloying elements with {GBs}. In addition, the model presented herein extends current treatments by independently treating solute–solute interactions within both the bulk grain and {GB} regions, allowing for deviations from dilute and ideal systems and the ability to account for phase separation processes occurring in conjunction with grain growth. Starting with the analytical treatment of one-dimensional (1D) systems, we investigate the dependence of the {GB} energy, and subsequently the driving force for grain growth, on the segregation model parameters. More specifically, classic {GB} segregation isotherms are recovered in the limit of 1D infinite grains. Simulation results of two-dimensional systems reveal regimes of increased thermal stability, and highlight the importance of the thermodynamic model parameters of both bulk grain and {GBs} on grain growth processes. In broader terms, our modeling approach provides further avenues to explore {GB} solute segregation and its role in stabilizing polycrystalline aggregates.},
	pages = {159--171},
	journal = {Acta Materialia},
	shortjournal = {Acta Materialia},
	author = {Abdeljawad, Fadi and Foiles, Stephen M.},
	urldate = {2017-01-27},
	year = {2015},
	keywords = {Grain boundary segregation, grain growth, Nanocrystalline materials, Phase field model, Thermodynamic stability},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/CC4K7WF4/Abdeljawad and Foiles - 2015 - Stabilization of nanocrystalline alloys via grain .pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/7GXFC58D/S1359645415005418.html:text/html}
}

@misc{3d_op,
  title =	 {{3D} opportunity: {Additive} manufacturing paths to
                  performance, innovation, and growth},
  url =
                  {about:reader?url=http%3A%2F%2Fdupress.com%2Farticles%2Fdr14-3d-opportunity%2F},
  urldate =	 {2015-12-29},
  file =	 {3D opportunity\: Additive manufacturing paths to
                  performance, innovation, and
                  growth:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/DA9EE6T9/reader.html:text/html}
}

@Article{AinsworthOden1992,
  Title =	 {A procedure for a posteriori error estimation for
                  h-p finite element methods},
  Author =	 {Ainsworth, M. and Oden, J. T.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 1992,
  Number =	 1,
  Pages =	 {73--96},
  Volume =	 101,
  Doi =		 {10.1016/0045-7825(92)90016-D},
  File =	 {AinsworthOden1992.pdf:AinsworthOden1992.pdf:PDF}
}

@Article{AlexeSandu2014,
  Title =	 {Space-time adaptive solution of inverse problems
                  with the discrete adjoint method},
  Author =	 {Alexe, M. and Sandu, A.},
  Journal =	 {J. Comput. Phys.},
  Year =	 2014,
  Pages =	 {21--39},
  Volume =	 270,
  Doi =		 {10.1016/j.jcp.2014.03.042},
  File =	 {AlexeSandu2014.pdf:AlexeSandu2014.pdf:PDF}
}

@Article{Alvarez-AramberriPardoBarucq2013,
  Title =	 {Inversion of Magnetotelluric Measurements Using
                  Multigoal Oriented hp-adaptivity},
  Author =	 {Alvarez-Aramberri, J. and Pardo, D. and Barucq, H.},
  Journal =	 {Procedia Comput. Sci.},
  Year =	 2013,
  Pages =	 {1564--1573},
  Volume =	 18,
  Doi =		 {10.1016/j.procs.2013.05.324},
  File =
                  {Alvarez-AramberriPardoBarucq2013.pdf:Alvarez-AramberriPardoBarucq2013.pdf:PDF}
}

@TECHREPORT{AmaHLPQSSLYBDKS11,
  author =	 {S. Amarasinghe and M. Hall and R. Lethin and
                  K. Pingali and D. Quinlan and V. Sarkar and J. Shalf
                  and R. Lucas and K. Yelick and P. Balaji and
                  P. C. Diniz and A. Koniges and M. Snir},
  title =	 {{ASCR} {P}rogramming {C}hallenges for {E}xascale
                  {C}omputing},
  institution =	 {Sponsored by {U}.{S}. {D}epartment {O}f {E}nergy,
                  {O}ffice of {S}cience, {O}ffice of {A}dvanced
                  {S}cientific {Co}mputing {R}esearch ({ASCR}) },
  year =	 2011,
  type =	 {{W}orkshop's {R}eport},
  address =	 {Marina del Rey, CA, U.S.},
  owner =	 {principe},
  timestamp =	 {2014.03.21}
}

@TECHREPORT{AmaHLPQSSLYBDKS11,
  author =	 {S. Amarasinghe and M. Hall and R. Lethin and
                  K. Pingali and D. Quinlan and V. Sarkar and J. Shalf
                  and R. Lucas and K. Yelick and P. Balaji and
                  P. C. Diniz and A. Koniges and M. Snir},
  title =	 {{ASCR} {P}rogramming {C}hallenges for {E}xascale
                  {C}omputing},
  institution =	 {Sponsored by {U}.{S}. {D}epartment {O}f {E}nergy,
                  {O}ffice of {S}cience, {O}ffice of {A}dvanced
                  {S}cientific {Co}mputing {R}esearch ({ASCR}) },
  year =	 2011,
  type =	 {{W}orkshop's {R}eport},
  address =	 {Marina del Rey, CA, U.S.},
  owner =	 {principe},
  timestamp =	 {2014.03.21}
}

@Article{AsnerTavenerKay2012,
  Title =	 {Adjoint-Based a Posteriori Error Estimation for
                  Coupled Time-Dependent Systems},
  Author =	 {Asner, L. and Tavener, S. and Kay, D.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2012,
  Number =	 4,
  Pages =	 {A2394--A2419},
  Volume =	 34,
  Doi =		 {10.1137/110858458},
  File =	 {AsnerTavenerKay2012.pdf:AsnerTavenerKay2012.pdf:PDF}
}

@Article{BaderBoeckSchwaigerVigh2010,
  Title =	 {Dynamically Adaptive Simulations with Minimal Memory
                  Requirement---{S}olving the Shallow Water Equations
                  Using Sierpinski Curves},
  Author =	 {Bader, M. and B{\"o}ck, C. and Schwaiger, J. and
                  Vigh, C.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2010,
  Number =	 1,
  Pages =	 {212--228},
  Volume =	 32,
  Doi =		 {10.1137/080728871},
  File =
                  {BaderBoeckSchwaigerVigh2010.pdf:BaderBoeckSchwaigerVigh2010.pdf:PDF}
}

@ARTICLE{Badia2013a,
  author =	 {S. Badia and A.F. Mart\'in and J.Principe},
  title =	 {Enhanced balancing Neumann-Neumann preconditioning
                  in computational fluid and solid mechanics},
  journal =	 {International Journal for Numerical Methods in
                  Engineering},
  year =	 2013,
  volume =	 96,
  pages =	 {203--230},
  abstract =	 {In this work, we propose an enhanced implementation
                  of balancing Neumann-Neumann (BNN) preconditioning
                  together with a detailed numerical comparison
                  against the balancing domain decomposition by
                  constraints (BDDC) preconditioner. As model
                  problems, we consider the Poisson and linear
                  elasticity problems. On one hand, we propose a novel
                  way to deal with singular matrices and
                  pseudo-inverses appearing in local solvers.  It is
                  based on a kernel identication strategy that
                  allows us to eciently compute the action of the
                  pseudo-inverse via local indenite solvers. We
                  further show how, identifying a minimum set of
                  degrees of freedom to bexed, an equivalent
                  denite system can be solved instead, even in the
                  elastic case. On the other hand, we propose a simple
                  modication of the preconditioned conjugate
                  gradient (PCG) algorithm that reduces the number of
                  Dirichlet solvers to only one per iteration, leading
                  to similar computational cost as additive
                  methods. After these improvements of the BNN PCG
                  algorithm, we compare its performance against that
                  of the BDDC preconditioners on a pair of large-scale
                  distributed-memory platforms. The enhanced BNN
                  method is a competitive preconditioner for
                  three-dimensional Poisson and elasticity problems,
                  and outperforms the BDDC method in many cases.},
  file =
                  {:/home/principe/Javier/Admin/Curriculum/Justif/Journal-Articles/preprint_ddm1.pdf:PDF},
  owner =	 {principe},
  timestamp =	 {2012.08.04}
}

@ARTICLE{Badia2013a,
  author =	 {S. Badia and A.F. Mart\'in and J.Principe},
  title =	 {Enhanced balancing Neumann-Neumann preconditioning
                  in computational fluid and solid mechanics},
  journal =	 {International Journal for Numerical Methods in
                  Engineering},
  year =	 2013,
  volume =	 96,
  pages =	 {203--230},
  abstract =	 {In this work, we propose an enhanced implementation
                  of balancing Neumann-Neumann (BNN) preconditioning
                  together with a detailed numerical comparison
                  against the balancing domain decomposition by
                  constraints (BDDC) preconditioner. As model
                  problems, we consider the Poisson and linear
                  elasticity problems. On one hand, we propose a novel
                  way to deal with singular matrices and
                  pseudo-inverses appearing in local solvers.  It is
                  based on a kernel identication strategy that
                  allows us to eciently compute the action of the
                  pseudo-inverse via local indenite solvers. We
                  further show how, identifying a minimum set of
                  degrees of freedom to bexed, an equivalent
                  denite system can be solved instead, even in the
                  elastic case. On the other hand, we propose a simple
                  modication of the preconditioned conjugate
                  gradient (PCG) algorithm that reduces the number of
                  Dirichlet solvers to only one per iteration, leading
                  to similar computational cost as additive
                  methods. After these improvements of the BNN PCG
                  algorithm, we compare its performance against that
                  of the BDDC preconditioners on a pair of large-scale
                  distributed-memory platforms. The enhanced BNN
                  method is a competitive preconditioner for
                  three-dimensional Poisson and elasticity problems,
                  and outperforms the BDDC method in many cases.},
  file =
                  {:/home/principe/Javier/Admin/Curriculum/Justif/Journal-Articles/preprint_ddm1.pdf:PDF},
  owner =	 {principe},
  timestamp =	 {2012.08.04}
}

@ARTICLE{Badia2013b,
  author =	 {S. Badia and A. F. Mart{\'i}n and J. Principe},
  title =	 {Implementation and Scalability Analysis of Balancing
                  Domain Decomposition Methods},
  journal =	 {Archives of Computational Methods in Engineering},
  year =	 2013,
  volume =	 20,
  pages =	 {239-262},
  number =	 3,
  abstract =	 {In this paper we present a detailed description of a
                  high-performance distributed memory implementation
                  of balancing domain decomposition preconditioning
                  techniques. This coverage provides a pool of
                  implementation hints and considerations that can be
                  very useful for scientists that are willing to
                  tackle large-scale distributed-memory machines using
                  these methods. On the other hand, the paper includes
                  a comprehensive performance and scalability study of
                  the resulting codes when they are applied for the
                  solution of the Poisson problem on a large-scale
                  multicore-based distributed-memory machine with up
                  to 4096 cores.  Well-known theoretical results
                  guarantee the optimality (algorithmic scalability)
                  of these preconditioning techniques for weak scaling
                  scenarios, as they are able to keep the condition
                  number of the preconditioned operator bounded by a
                  constant withxed load per core and increasing
                  number of cores. The experimental study presented in
                  the paper complements this mathematical analysis and
                  answers how far can these methods go in the number
                  of cores and the scale of the problem to still be
                  within reasonable ranges of eciency on current
                  distributed-memory machines. Besides, for those
                  scenarios where poor scalability is expected, the
                  study precisely identies, quanties and justies
                  which are the main sources of inefficiency.},
  doi =		 {10.1007/s11831-013-9086-4},
  file =
                  {:/home/principe/Javier/Admin/Curriculum/Justif/Journal-Articles/preprint_ddm2.pdf:PDF},
  owner =	 {principe},
  timestamp =	 {2012.08.04}
}

@article{Badia2014,
  title =	 {A Highly Scalable Parallel Implementation of
                  Balancing Domain Decomposition by Constraints},
  issn =	 {1064-8275},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/130931989},
  doi =		 {10.1137/130931989},
  abstract =	 {In this work we propose a novel parallelization
                  approach of two-level balancing domain decomposition
                  by constraints preconditioning based on overlapping
                  of fine-grid and coarse-grid duties in time. The
                  global set of {MPI} tasks is split into those that
                  have fine-grid duties and those that have
                  coarse-grid duties, and the different computations
                  and communications in the algorithm are then
                  rescheduled and mapped in such a way that the
                  maximum degree of overlapping is achieved while
                  preserving data dependencies among them. In many
                  ranges of interest, the extra cost associated to the
                  coarse-grid problem can be fully masked by fine-grid
                  related computations (which are embarrassingly
                  parallel). Apart from discussing code implementation
                  details, the paper also presents a comprehensive set
                  of numerical experiments that includes weak
                  scalability analyses with structured and
                  unstructured meshes for the three-dimensional
                  Poisson and linear elasticity problems on a pair of
                  state-of-the-art multicore-based distributed-memory
                  machines. This experimental study reveals remarkable
                  weak scalability in the solution of problems with
                  thousands of millions of unknowns on several tens of
                  thousands of computational cores., In this work we
                  propose a novel parallelization approach of
                  two-level balancing domain decomposition by
                  constraints preconditioning based on overlapping of
                  fine-grid and coarse-grid duties in time. The global
                  set of {MPI} tasks is split into those that have
                  fine-grid duties and those that have coarse-grid
                  duties, and the different computations and
                  communications in the algorithm are then rescheduled
                  and mapped in such a way that the maximum degree of
                  overlapping is achieved while preserving data
                  dependencies among them. In many ranges of interest,
                  the extra cost associated to the coarse-grid problem
                  can be fully masked by fine-grid related
                  computations (which are embarrassingly
                  parallel). Apart from discussing code implementation
                  details, the paper also presents a comprehensive set
                  of numerical experiments that includes weak
                  scalability analyses with structured and
                  unstructured meshes for the three-dimensional
                  Poisson and linear elasticity problems on a pair of
                  state-of-the-art multicore-based distributed-memory
                  machines. This experimental study reveals remarkable
                  weak scalability in the solution of problems with
                  thousands of millions of unknowns on several tens of
                  thousands of computational cores.},
  urldate =	 {2014-04-25},
  journal =	 {{SIAM} Journal on Scientific Computing},
  author =	 {Badia, S. and Mart\'in, A. F. and Principe, J.},
  year =	 2014,
  pages =	 {C190--C218},
  file =
                  {Snapshot:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/QTTI4J8V/130931989.html:text/html}
}

@Article{BadiaMartinPrincipe2013,
  Title =	 {Implementation and Scalability Analysis of Balancing
                  Domain Decomposition Methods},
  Author =	 {Badia, S. and Mart{\'i}n, A. F. and Principe, J.},
  Journal =	 {Arch. Comput. Method. E.},
  Year =	 2013,
  Number =	 3,
  Pages =	 {239--262},
  Volume =	 20,
  Doi =		 {10.1007/s11831-013-9086-4},
  File =
                  {BadiaMartinPrincipe2013.pdf:BadiaMartinPrincipe2013.pdf:PDF}
}

@Article{BadiaMartinPrincipe2014,
  Title =	 {A Highly Scalable Parallel Implementation of
                  Balancing Domain Decomposition by Constraints},
  Author =	 {Badia, S. and Mart{\'i}n, A. F. and Principe, J.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2014,
  Number =	 2,
  Pages =	 {C190--C218},
  Volume =	 36,
  Doi =		 {10.1137/130931989},
  File =
                  {BadiaMartinPrincipe2014.pdf:BadiaMartinPrincipe2014.pdf:PDF}
}

@Article{BankHolst2003,
  Title =	 {A New Paradigm for Parallel Adaptive Meshing
                  Algorithms},
  Author =	 {Bank, R. E. and Holst, M.},
  Journal =	 {SIAM Review},
  Year =	 2003,
  Number =	 2,
  Pages =	 {291--323},
  Volume =	 45,
  File =	 {BankHolst2003.pdf:BankHolst2003.pdf:PDF}
}

@Article{BankLu2004,
  Title =	 {A Domain Decomposition Solver for a Parallel
                  Adaptive Meshing Paradigm},
  Author =	 {Bank, R. E. and Lu, S.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2004,
  Number =	 1,
  Pages =	 {105--127},
  Volume =	 26,
  Doi =		 {10.1137/S1064827503428096},
  File =	 {BankLu2004.pdf:BankLu2004.pdf:PDF}
}

@InCollection{BankNguyen2013,
  Title =	 {A Parallel $hp$-Adaptive Finite Element Method},
  Author =	 {Bank, R. E. and Nguyen, H.},
  Booktitle =	 {Recent Advances in Scientific Computing and
                  Applications},
  Publisher =	 {American Mathematical Society},
  Year =	 2013,
  Editor =	 {Li, J. and Yang, H. and Machorro, E. A.},
  File =	 {BankNguyen2013.pdf:BankNguyen2013.pdf:PDF}
}

@Article{BankOvall2007,
  Title =	 {Dual Functions for a Parallel Adaptive Method},
  Author =	 {Bank, R. E. and Ovall, J. S.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2007,
  Number =	 4,
  Pages =	 {1511--1524},
  Volume =	 29,
  Doi =		 {10.1137/060668304},
  File =	 {BankOvall2007.pdf:BankOvall2007.pdf:PDF}
}

@inproceedings{Bauer:2012:LEL:2388996.2389086,
 author = {Bauer, Michael and Treichler, Sean and Slaughter, Elliott and Aiken, Alex},
 title = {Legion: Expressing Locality and Independence with Logical Regions},
 booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
 series = {SC '12},
 year = {2012},
 isbn = {978-1-4673-0804-5},
 location = {Salt Lake City, Utah},
 pages = {66:1--66:11},
 articleno = {66},
 numpages = {11},
 url = {http://dl.acm.org.recursos.biblioteca.upc.edu/citation.cfm?id=2388996.2389086},
 acmid = {2389086},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
}

@BOOK{Bellan2008,
  title =	 {{Fundamentals of plasma physics}},
  publisher =	 {Cambridge Univ. Press},
  year =	 2008,
  author =	 {Bellan, Paul M},
  address =	 {Cambridge}
}

@BOOK{Bellan2008,
  title =	 {{Fundamentals of plasma physics}},
  publisher =	 {Cambridge Univ. Press},
  year =	 2008,
  author =	 {Bellan, Paul M},
  address =	 {Cambridge}
}
@ARTICLE{Bender2011,
  author =	 {Bender, Michael A. and Kuszmaul, Bradley C. and
                  Teng, Shang-Hua and Wang, Kebin},
  title =	 {Optimal Cache-Oblivious Mesh Layouts},
  journal =	 {Theor. Comp. Sys.},
  year =	 2011,
  volume =	 48,
  pages =	 {269--296},
  number =	 2,
  acmid =	 1936830,
  address =	 {Secaucus, NJ, USA},
  doi =		 {10.1007/s00224-009-9242-2},
  issn =	 {1432-4350},
  issue_date =	 {February 2011},
  keywords =	 {Decomposition tree, Fully-balanced decomposition
                  tree, Geometric separator, Mesh layout,
                  Relax-balanced decomposition tree},
  numpages =	 28,
  publisher =	 {Springer-Verlag New York, Inc.},
  url =		 {http://dx.doi.org/10.1007/s00224-009-9242-2}
}

@ARTICLE{Bender2011,
  author =	 {Bender, Michael A. and Kuszmaul, Bradley C. and
                  Teng, Shang-Hua and Wang, Kebin},
  title =	 {Optimal Cache-Oblivious Mesh Layouts},
  journal =	 {Theor. Comp. Sys.},
  year =	 2011,
  volume =	 48,
  pages =	 {269--296},
  number =	 2,
  acmid =	 1936830,
  address =	 {Secaucus, NJ, USA},
  doi =		 {10.1007/s00224-009-9242-2},
  issn =	 {1432-4350},
  issue_date =	 {February 2011},
  keywords =	 {Decomposition tree, Fully-balanced decomposition
                  tree, Geometric separator, Mesh layout,
                  Relax-balanced decomposition tree},
  numpages =	 28,
  publisher =	 {Springer-Verlag New York, Inc.},
  url =		 {http://dx.doi.org/10.1007/s00224-009-9242-2}
}

@Article{BermejoCarpio2009,
  Title =	 {A space-time adaptive finite element algorithm based
                  on dual weighted residual methodology for parabolic
                  equations},
  Author =	 {Bermejo, R. and Carpio, J.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2009,
  Number =	 5,
  Pages =	 {3324--3355},
  Volume =	 31,
  Doi =		 {10.1137/070698853}
}

@Article{Berrone2009,
  Title =	 {A local-in-space-timestep approach to a finite
                  element discretization of the heat equation with a
                  posteriori estimates},
  Author =	 {Berrone, S.},
  Journal =	 {SIAM J. Numer. Anal.},
  Year =	 2009,
  Number =	 4,
  Pages =	 {3109--3138},
  Volume =	 47,
  Doi =		 {10.1137/080737058},
  File =	 {Berrone2009.pdf:Berrone2009.pdf:PDF}
}

@Article{BerroneMarro2010,
  Title =	 {Numerical investigation of effectivity indices of
                  space-time error indicators for Navier-Stokes
                  equations},
  Author =	 {Berrone, S. and Marro, M.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2010,
  Number =	 {25-28},
  Pages =	 {1764--1782},
  Volume =	 199,
  Doi =		 {10.1016/j.cma.2010.02.004},
  File =	 {BerroneMarro2010.pdf:BerroneMarro2010.pdf:PDF}
}

@Article{BesierRannacher2012,
  Title =	 {Goal-oriented space-time adaptivity in the finite
                  element {G}alerkin method for the computation of
                  nonstationary incompressible flow},
  Author =	 {Besier, M. and Rannacher, R.},
  Journal =	 {Int. J. Numer. Meth. Fluids.},
  Year =	 2012,
  Number =	 9,
  Pages =	 {1139--1166},
  Volume =	 70,
  Doi =		 {10.1002/fld.2735},
  File =	 {BesierRannacher2012.pdf:BesierRannacher2012.pdf:PDF}
}

@Article{BraackBurmanTaschenberger2011,
  Title =	 {Duality Based A Posteriori Error Estimation for
                  Quasi-Periodic Solutions Using Time Averages},
  Author =	 {Braack, M. and Burman, E. and Taschenberger, N.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2011,
  Number =	 5,
  Pages =	 {2199--2216},
  Volume =	 33,
  Doi =		 {10.1137/100809519},
  File =
                  {BraackBurmanTaschenberger2011.pdf:BraackBurmanTaschenberger2011.pdf:PDF}
}

@BOOK{Brenner2010,
  title =	 {The Mathematical Theory of Finite Element Methods},
  publisher =	 {Springer},
  year =	 2010,
  author =	 {Brenner, Susanne C. and Scott, Ridgway},
  edition =	 {3rd edition},
  isbn =	 1441926119,
  timestamp =	 {2012.10.19}
}

@TECHREPORT{BroMKMLSBBGV10,
  author =	 {D. L. Brown and P. Messina and D. Keyes and
                  J. Morrison and R. Lucas and J. Shalf and P. Beckman
                  and R. Brightwell and A. Geist and J.  Vetter},
  title =	 {{S}cientific {G}rand {C}hallenges: {C}rosscutting
                  {T}echnologies for {C}omputing at the {E}xascale},
  institution =	 {Sponsored by {U}.{S}. {D}epartment {O}f {E}nergy},
  year =	 2010,
  type =	 {{W}orkshop's {R}eport},
  address =	 {Washington, D.C., U.S.},
  owner =	 {principe},
  timestamp =	 {2014.03.21}
}

@TECHREPORT{BroMKMLSBBGV10,
  author =	 {D. L. Brown and P. Messina and D. Keyes and
                  J. Morrison and R. Lucas and J. Shalf and P. Beckman
                  and R. Brightwell and A. Geist and J.  Vetter},
  title =	 {{S}cientific {G}rand {C}hallenges: {C}rosscutting
                  {T}echnologies for {C}omputing at the {E}xascale},
  institution =	 {Sponsored by {U}.{S}. {D}epartment {O}f {E}nergy},
  year =	 2010,
  type =	 {{W}orkshop's {R}eport},
  address =	 {Washington, D.C., U.S.},
  owner =	 {principe},
  timestamp =	 {2014.03.21}
}

@Article{BuergNazarov2015,
  Title =	 {Goal-oriented adaptive finite element methods for
                  elliptic problems revisited},
  Author =	 {B{\"u}rg, M. and Nazarov, M.},
  Journal =	 {J. Comput. Appl. Math.},
  Year =	 2015,
  Pages =	 {125--147},
  Volume =	 287,
  Doi =		 {10.1016/j.cam.2015.03.031},
  File =	 {BuergNazarov2015.pdf:BuergNazarov2015.pdf:PDF}
}

@Article{BursteddeBurtscherGhattasStadlerTuWilcox2009,
  Title =	 {{ALPS}: {A} framework for parallel adaptive {PDE}
                  solution},
  Author =	 {Burstedde, C. and Burtscher, M. and Ghattas, O. and
                  Stadler, G. and Tu, T. and Wilcox, L. C.},
  Journal =	 {J. Phys.: Conf. Ser.},
  Year =	 2009,
  Number =	 1,
  Pages =	 012009,
  Volume =	 180,
  File =
                  {BursteddeBurtscherGhattasStadlerTuWilcox2009.pdf:BursteddeBurtscherGhattasStadlerTuWilcox2009.pdf:PDF}
}

@Article{BursteddeGhattasStadlerTuWilcox2009,
  Title =	 {Parallel scalable adjoint-based adaptive solution of
                  variable-viscosity Stokes flow problems},
  Author =	 {Burstedde, C. and Ghattas, O. and Stadler, G. and
                  Tu, T. and Wilcox, L. C.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2009,
  Number =	 {21-26},
  Pages =	 {1691--1700},
  Volume =	 198,
  Doi =		 {10.1016/j.cma.2008.12.015},
  File =
                  {BursteddeGhattasStadlerTuWilcox2009.pdf:BursteddeGhattasStadlerTuWilcox2009.pdf:PDF}
}

@Article{BursteddeStadlerAlisicWilcoxTanGurnisGhattas2013,
  Title =	 {Large-scale adaptive mantle convection simulation},
  Author =	 {Burstedde, C. and Stadler, G. and Alisic, L. and
                  Wilcox, L. C. and Tan, E. and Gurnis, M. and
                  Ghattas, O.},
  Journal =	 {Geophysical Journal International},
  Year =	 2013,
  Number =	 3,
  Pages =	 {889--906},
  Volume =	 192,
  Doi =		 {10.1093/gji/ggs070}
}

@Article{BursteddeWilcoxGhattas2011,
  Title =	 {p4est: Scalable Algorithms for Parallel Adaptive
                  Mesh Refinement on Forests of Octrees},
  Author =	 {Burstedde, C. and Wilcox, L. C. and Ghattas, O.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2011,
  Number =	 3,
  Pages =	 {1103--1133},
  Volume =	 33,
  Doi =		 {10.1137/100791634},
  File =
                  {BursteddeWilcoxGhattas2011.pdf:BursteddeWilcoxGhattas2011.pdf:PDF}
}

@misc{CAxMAN,
  title =	 {Computer Aided Technologies for Additive
                  Manufacturing ({CAxMan})},
  howpublished = {\url{http://www.caxman.eu}},
  url =		 {https://www.whitehouse.gov/mgi}
}

@misc{CAxMANdeliverable,
  title = {CAxMan Deliverable 4.3 Validation strategy for thermal analysis of MD process},
  howpublished = {\url{http://www.caxman.eu/mma/-/2016122715595771/}},
  urldate = {2017-01-13}
}

@Article{CareyEstepJohanssonLarsonTavener2010,
  Title =	 {Blockwise adaptivity for time dependent problems
                  based on coarse scale adjoint solutions},
  Author =	 {Carey, V. and Estep, D. and Johansson, A. and
                  Larson, M. and Tavener, S.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2010,
  Number =	 4,
  Pages =	 {2121--2145},
  Volume =	 32,
  Doi =		 {10.1137/090753826},
  File =
                  {CareyEstepJohanssonLarsonTavener2010.pdf:CareyEstepJohanssonLarsonTavener2010.pdf:PDF}
}

@Article{CareyEstepTavener2009,
  Title =	 {A Posteriori Analysis and Adaptive Error Control for
                  Multiscale Operator Decomposition Solution of
                  Elliptic Systems {I}: {T}riangular Systems},
  Author =	 {Carey, V. and Estep, D. and Tavener, S.},
  Journal =	 {SIAM J. Numer. Anal.},
  Year =	 2009,
  Number =	 1,
  Pages =	 {740--761},
  Volume =	 47,
  Doi =		 {10.1137/070689917},
  File =
                  {CareyEstepTavener2009.pdf:CareyEstepTavener2009.pdf:PDF}
}

@Article{CasadeiDiezVerdugo2013,
  Title =	 {An algorithm for mesh refinement and un-refinement
                  in fast transient dynamics},
  Author =	 {Casadei, F. and D{\'i}ez, P. and Verdugo, F.},
  Journal =	 {Int. J. Comput. Methods},
  Year =	 2013,
  Number =	 04,
  Pages =	 {31 Pages},
  Volume =	 10,
  Doi =		 {10.1142/S0219876213500187}
}

@Article{Charpentier2001,
  Title =	 {Checkpointing Schemes for Adjoint Codes:
                  {A}pplication to the Meteorological Model Meso-{NH}},
  Author =	 {Charpentier, I.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2001,
  Number =	 6,
  Pages =	 {2135--2151},
  Volume =	 22,
  Doi =		 {10.1137/S1064827598343735},
  File =	 {Charpentier2001.pdf:Charpentier2001.pdf:PDF}
}

@Article{ClemensLangTeleagaWimmer2009,
  Title =	 {Adaptivity in space and time for
                  magnetoquasistatics},
  Author =	 {Clemens, M. and Lang, J. and Teleaga, D. and Wimmer,
                  G.},
  Journal =	 {J. Comput. Math.},
  Year =	 2009,
  Number =	 5,
  Pages =	 {642--656},
  Volume =	 27,
  Doi =		 {10.4208/jcm.2009.27.5.015}
}

@Article{CyrShadidWildey2014,
  Title =	 {Approaches for Adjoint-Based A Posteriori Analysis
                  of Stabilized Finite Element Methods},
  Author =	 {Cyr, E. C. and Shadid, J. and Wildey, T.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2014,
  Number =	 2,
  Pages =	 {A766--A791},
  Volume =	 36,
  Doi =		 {10.1137/120895822},
  File =	 {CyrShadidWildey2014.pdf:CyrShadidWildey2014.pdf:PDF}
}

@Article{CyrShadidWildey2015,
  Title =	 {Towards efficient backward-in-time adjoint
                  computations using data compression techniques},
  Author =	 {Cyr, E. C. and Shadid, J. N. and Wildey, T.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2015,
  Pages =	 {24--44},
  Volume =	 288,
  Doi =		 {10.1016/j.cma.2014.12.001},
  File =	 {CyrShadidWildey2015.pdf:CyrShadidWildey2015.pdf:PDF}
}

@TECHREPORT{DOEreport2012,
  title =	 {Report on the Workshop on Extreme-Scale Solvers:
                  Transition to Future Architectures},
  institution =	 {U.S. Department of Energy},
  year =	 2012,
  owner =	 {principe},
  timestamp =	 {2014.03.21}
}

@TECHREPORT{DOEreport2012,
  title =	 {Report on the Workshop on Extreme-Scale Solvers:
                  Transition to Future Architectures},
  institution =	 {U.S. Department of Energy},
  year =	 2012,
  owner =	 {principe},
  timestamp =	 {2014.03.21}
}

@Article{DemkowiczRachowiczDevloo2002,
  Title =	 {A Fully Automatic hp-Adaptivity},
  Author =	 {Demkowicz, L. and Rachowicz, W. and Devloo, Ph.},
  Journal =	 {J. Sci. Comput.},
  Year =	 2002,
  Number =	 {1-4},
  Pages =	 {117--142},
  Volume =	 17,
  Doi =		 {10.1023/A:1015192312705},
  File =
                  {DemkowiczRachowiczDevloo2002.pdf:DemkowiczRachowiczDevloo2002.pdf:PDF}
}

@Article{DiFlauraudVohralikYousef2014,
  Title =	 {A posteriori error estimates, stopping criteria, and
                  adaptivity for multiphase compositional Darcy flows
                  in porous media},
  Author =	 {Di Pietro, D. A. and Flauraud, E. and Vohral{\'i}k,
                  M. and Yousef, S.},
  Journal =	 {J. Comput. Phys.},
  Year =	 2014,
  Pages =	 {163--187},
  Volume =	 276,
  Doi =		 {10.1016/j.jcp.2014.06.061},
  File =
                  {DiFlauraudVohralikYousef2014.pdf:DiFlauraudVohralikYousef2014.pdf:PDF}
}

@Article{DiVohralikYousef2014,
  Title =	 {An a posteriori-based, fully adaptive algorithm with
                  adaptive stopping criteria and mesh refinement for
                  thermal multiphase compositional flows in porous
                  media},
  Author =	 {Di Pietro, D. A. and Vohral{\'i}k, M. and Yousef,
                  S.},
  Journal =	 {Comput. Math. Appl.},
  Year =	 2014,
  Number =	 12,
  Pages =	 {2331--2347},
  Volume =	 68,
  Doi =		 {10.1016/j.camwa.2014.08.008}
}

@Article{DiezCalderon2007,
  Title =	 {Goal-oriented error estimation for transient
                  parabolic problems},
  Author =	 {D{\'i}ez, P. and Calder{\'o}n, G.},
  Journal =	 {Comput. Mech.},
  Year =	 2007,
  Number =	 5,
  Pages =	 {631--646},
  Volume =	 39,
  Doi =		 {10.1007/s00466-006-0106-1},
  File =	 {DiezCalderon2007.pdf:DiezCalderon2007.pdf:PDF}
}

@ARTICLE{Dohrmann2003,
  author =	 {C. R. Dohrmann},
  title =	 {A Preconditioner for Substructuring Based on
                  Constrained Energy Minimization},
  journal =	 {SIAM Journal on Scientific Computing},
  year =	 2003,
  volume =	 25,
  pages =	 {246-258},
  number =	 1,
  doi =		 {10.1137/S1064827502412887},
  file =	 {:Papers/Domdec/Dohrman_2003.pdf:PDF},
  keywords =	 {Domdec; substructuring; domain decomposition;
                  iterative methods; FETI; balancing domain
                  decomposition},
  owner =	 {principe},
  publisher =	 {SIAM},
  timestamp =	 {2012.07.17},
  url =		 {http://link.aip.org/link/?SCE/25/246/1}
}

@Article{DominguesGomesRousselSchneider2009,
  Title =	 {Space-time adaptive multiresolution methods for
                  hyperbolic conservation laws: Applications to
                  compressible {Euler} equations},
  Author =	 {Domingues, M. O. and Gomes, S. M. and Roussel,
                  O. and Schneider, K.},
  Journal =	 {Appl. Numer. Math.},
  Year =	 2009,
  Number =	 9,
  Pages =	 {2303--2321},
  Volume =	 59,
  Doi =		 {10.1016/j.apnum.2008.12.018}
}

@Article{DuarteBonaventuraMassotBourdonDescombesDumont2012,
  Title =	 {A new numerical strategy with space-time adaptivity
                  and error control for multi-scale streamer discharge
                  simulations},
  Author =	 {Duarte, M. and Bonaventura, Z. and Massot, M. and
                  Bourdon, A. and Descombes, S. and Dumont, T.},
  Journal =	 {J. Comput. Phys.},
  Year =	 2012,
  Number =	 {3, {SI}},
  Pages =	 {1002--1019},
  Volume =	 231,
  Doi =		 {10.1016/j.jcp.2011.07.002}
}

@BOOK{Efendiev2009,
  title =	 {Multiscale Finite Element Methods},
  publisher =	 {Springer},
  year =	 2009,
  author =	 {Y. Efendiev and T. Y. Hou},
  series =	 {Surveys and Tutorials in the Applied Mathematical
                  Sciences, Vol.  4},
  owner =	 {principe},
  timestamp =	 {2014.03.22}
}

@BOOK{Efendiev2009,
  title =	 {Multiscale Finite Element Methods},
  publisher =	 {Springer},
  year =	 2009,
  author =	 {Y. Efendiev and T. Y. Hou},
  series =	 {Surveys and Tutorials in the Applied Mathematical
                  Sciences, Vol.  4},
  owner =	 {principe},
  timestamp =	 {2014.03.22}
}

@Article{ElErnVohralik2011,
  Title =	 {Guaranteed and robust a posteriori error estimates
                  and balancing discretization and linearization
                  errors for monotone nonlinear problems},
  Author =	 {El Alaoui, L. and Ern, A. and Vohral{\'i}k, M.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2011,
  Number =	 {37-40},
  Pages =	 {2782 --2795},
  Volume =	 200,
  Doi =		 {10.1016/j.cma.2010.03.024},
  File =	 {ElErnVohralik2011.pdf:ElErnVohralik2011.pdf:PDF}
}

@Article{ErnVohralik2013,
  Title =	 {Adaptive Inexact Newton Methods with A Posteriori
                  Stopping Criteria for Nonlinear Diffusion PDEs},
  Author =	 {Ern, A. and Vohral{\'i}k, M.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2013,
  Number =	 4,
  Pages =	 {A1761--A1791},
  Volume =	 35,
  Doi =		 {10.1137/120896918},
  File =	 {ErnVohralik2013.pdf:ErnVohralik2013.pdf:PDF}
}

@Article{EstepCareyGintingTavenerWildey2008,
  Title =	 {A posteriori error analysis of multiscale operator
                  decomposition methods for multiphysics models},
  Author =	 {Estep, D. and Carey, V. and Ginting, V. and Tavener,
                  S. and Wildey, T.},
  Journal =	 {J. Phys.: Conf. Ser.},
  Year =	 2008,
  Number =	 1,
  Pages =	 012075,
  Volume =	 125,
  Doi =		 {10.1088/1742-6596/125/1/012075},
  File =
                  {EstepCareyGintingTavenerWildey2008.pdf:EstepCareyGintingTavenerWildey2008.pdf:PDF}
}

@Article{EstepGintingRoppShadidTavener2008,
  Title =	 {An A Posteriori-A Priori Analysis of Multiscale
                  Operator Splitting},
  Author =	 {Estep, D. and Ginting, V. and Ropp, D. and Shadid,
                  J. and Tavener, S.},
  Journal =	 {SIAM J. Numer. Anal.},
  Year =	 2008,
  Number =	 3,
  Pages =	 {1116--1146},
  Volume =	 46,
  Doi =		 {10.1137/07068237X},
  File =
                  {EstepGintingRoppShadidTavener2008.pdf:EstepGintingRoppShadidTavener2008.pdf:PDF}
}

@Article{EstepTavenerWildey2008,
  Title =	 {A Posteriori Analysis and Improved Accuracy for an
                  Operator Decomposition Solution of a Conjugate Heat
                  Transfer Problem},
  Author =	 {Estep, D. and Tavener, S. and Wildey, T.},
  Journal =	 {SIAM J. Numer. Anal.},
  Year =	 2008,
  Number =	 4,
  Pages =	 {2068--2089},
  Volume =	 46,
  Doi =		 {10.1137/060678737},
  File =
                  {EstepTavenerWildey2008.pdf:EstepTavenerWildey2008.pdf:PDF}
}

@Article{EstepTavenerWildey2010,
  Title =	 {A posteriori error estimation and adaptive mesh
                  refinement for a multiscale operator decomposition
                  approach to fluid-solid heat transfer},
  Author =	 {Estep, D. and Tavener, S. and Wildey, T.},
  Journal =	 {J. Comput. Phys.},
  Year =	 2010,
  Number =	 11,
  Pages =	 {4143--4158},
  Volume =	 229,
  Doi =		 {10.1016/j.jcp.2010.02.003},
  File =
                  {EstepTavenerWildey2010.pdf:EstepTavenerWildey2010.pdf:PDF}
}

@Article{FickBrummelenZee2010,
  Title =	 {On the adjoint-consistent formulation of interface
                  conditions in goal-oriented error estimation and
                  adaptivity for fluid-structure interaction},
  Author =	 {Fick, P. W. and van Brummelen, E. H. and van der
                  Zee, K. G.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2010,
  Number =	 {49-52},
  Pages =	 {3369--3385},
  Volume =	 199,
  Doi =		 {10.1016/j.cma.2010.07.009},
  File =
                  {FickBrummelenZee2010.pdf:FickBrummelenZee2010.pdf:PDF}
}

@Article{FlahertyLoyShephardSzymanskiTerescoZiantz1997,
  Title =	 {Adaptive Local Refinement with Octree Load Balancing
                  for the Parallel Solution of Three-Dimensional
                  Conservation Laws},
  Author =	 {Flaherty, J. E. and Loy, R. M. and Shephard,
                  M. S. and Szymanski, B. K. and Teresco, J. D. and
                  Ziantz, L. H.},
  Journal =	 {Journal of Parallel and Distributed Computing},
  Year =	 1997,
  Number =	 2,
  Pages =	 {139--152},
  Volume =	 47,
  Doi =		 {10.1006/jpdc.1997.1412}
}

@incollection{Gander_50years,
  author =	 {Gander, Martin J.},
  booktitle =	 {Multiple shooting and time domain decomposition},
  editor =	 {Carraro, T. and Geiger, M. and K\"orkel, S. and
                  Rannacher, R.},
  publisher =	 {Springer},
  title =	 {50 years of Time Parallel Time Integration},
  year =	 2015,
  pages =	 {69--113},
  volume =	 9,
  series =	 {Contributions in Mathematical and Computational
                  Sciences},
}

@Article{GassnerStaudenmaierHindenlangAtakMunz2015,
  Title =	 {A space-time adaptive discontinuous {G}alerkin
                  scheme},
  Author =	 {Gassner, G. and Staudenmaier, M. and Hindenlang,
                  F. and Atak, M. and Munz, C.-D.},
  Journal =	 {Comput. Fluids},
  Year =	 2015,
  Pages =	 {247--261},
  Volume =	 117,
  Doi =		 {10.1016/j.compfluid.2015.05.002}
}

@Article{GraetschBathe2006,
  Title =	 {Goal-oriented error estimation in the analysis of
                  fluid flows with structural interactions},
  Author =	 {Gr{\"a}tsch, T. and Bathe, K.-J.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2006,
  Number =	 {41-43},
  Pages =	 {5673--5684},
  Volume =	 195,
  Doi =		 {10.1016/j.cma.2005.10.020},
  File =	 {GraetschBathe2006.pdf:GraetschBathe2006.pdf:PDF}
}

@Article{Griewank1992,
  Title =	 {Achieving logarithmic growth of temporal and spatial
                  complexity in reverse automatic differentiation},
  Author =	 {Griewank, A.},
  Journal =	 {Optim. Method. Softw.},
  Year =	 1992,
  Number =	 1,
  Pages =	 {35--54},
  Volume =	 1,
  Doi =		 {10.1080/10556789208805505},
  File =	 {Griewank1992.pdf:Griewank1992.pdf:PDF}
}

@Article{GriewankWalther2000,
  Title =	 {Algorithm 799: Revolve: {A}n Implementation of
                  Checkpointing for the Reverse or Adjoint Mode of
                  Computational Differentiation},
  Author =	 {Griewank, A. and Walther, A.},
  Journal =	 {ACM Trans. Math. Softw.},
  Year =	 2000,
  Number =	 1,
  Pages =	 {19--45},
  Volume =	 26,
  Doi =		 {10.1145/347837.347846},
  File =	 {GriewankWalther2000.pdf:GriewankWalther2000.pdf:PDF}
}

@Article{Hartmann2007,
  Title =	 {Adjoint Consistency Analysis of Discontinuous
                  {G}alerkin Discretizations},
  Author =	 {Hartmann, R.},
  Journal =	 {SIAM J. Numer. Anal.},
  Year =	 2007,
  Number =	 6,
  Pages =	 {2671--2696},
  Volume =	 45,
  Doi =		 {10.1137/060665117},
  File =	 {Hartmann2007.pdf:Hartmann2007.pdf:PDF}
}

@Article{Hartmann2008,
  Title =	 {Multitarget Error Estimation and Adaptivity in
                  Aerodynamic Flow Simulations},
  Author =	 {Hartmann, R.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2008,
  Number =	 1,
  Pages =	 {708--731},
  Volume =	 31,
  Doi =		 {10.1137/070710962},
  File =	 {Hartmann2008.pdf:Hartmann2008.pdf:PDF}
}

@InCollection{HartmannHouston2003,
  Title =	 {Goal-Oriented A Posteriori Error Estimation for
                  Multiple Target Functionals},
  Author =	 {Hartmann, R. and Houston, P.},
  Booktitle =	 {Hyperbolic Problems: Theory, Numerics, Applications},
  Publisher =	 {Springer Berlin Heidelberg},
  Year =	 2003,
  Editor =	 {Hou, T. Y. and Tadmor, E.},
  Pages =	 {579--588},
  Doi =		 {10.1007/978-3-642-55711-8_54},
  File =	 {HartmannHouston2003.pdf:HartmannHouston2003.pdf:PDF},
  ISBN =	 {978-3-642-62929-7},
  Language =	 {English}
}

@TECHREPORT{Heroux2013,
  author =	 {M. Heroux and J. Dongarra},
  title =	 {Toward a New Metric for Ranking High Performance
                  Computing Systems},
  institution =	 {UTK EECS Tech Report and Sandia National Labs Report
                  SAND2013-4744},
  year =	 2013,
  owner =	 {principe},
  timestamp =	 {2014.03.20}
}

@TECHREPORT{Heroux2013,
  author =	 {M. Heroux and J. Dongarra},
  title =	 {Toward a New Metric for Ranking High Performance
                  Computing Systems},
  institution =	 {UTK EECS Tech Report and Sandia National Labs Report
                  SAND2013-4744},
  year =	 2013,
  owner =	 {principe},
  timestamp =	 {2014.03.20}
}

@ARTICLE{Hou_1997,
  author =	 {Thomas Y. Hou and Xiao-Hui Wu},
  title =	 {A Multiscale Finite Element Method for Elliptic
                  Problems in Composite Materials and Porous Media},
  journal =	 {Journal of Computational Physics},
  year =	 1997,
  volume =	 134,
  pages =	 {169--189},
  number =	 1,
  file =	 {Hou_1997.pdf:Papers/Stabilization/Hou_1997.pdf:PDF},
  keywords =	 {Stabilization Multiscale},
  owner =	 {principe},
  timestamp =	 {2014.03.22},
  url =		 {
                  http://www.sciencedirect.com/science/article/B6WHY-45S92GJ-4V/2/acc7ccf29efbf67af654e59594d36737}
}

@ARTICLE{Hou_1997,
  author =	 {Thomas Y. Hou and Xiao-Hui Wu},
  title =	 {A Multiscale Finite Element Method for Elliptic
                  Problems in Composite Materials and Porous Media},
  journal =	 {Journal of Computational Physics},
  year =	 1997,
  volume =	 134,
  pages =	 {169--189},
  number =	 1,
  file =	 {Hou_1997.pdf:Papers/Stabilization/Hou_1997.pdf:PDF},
  keywords =	 {Stabilization Multiscale},
  owner =	 {principe},
  timestamp =	 {2014.03.22},
  url =		 {
                  http://www.sciencedirect.com/science/article/B6WHY-45S92GJ-4V/2/acc7ccf29efbf67af654e59594d36737}
}

@Article{JiranekStrakosVohralik2010,
  Title =	 {A Posteriori Error Estimates Including Algebraic
                  Error and Stopping Criteria for Iterative Solvers},
  Author =	 {Jir{\'a}nek, P. and Strako{\v{s}}, Z. and
                  Vohral{\'i}k, M.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2010,
  Number =	 3,
  Pages =	 {1567--1590},
  Volume =	 32,
  Doi =		 {10.1137/08073706X},
  File =
                  {JiranekStrakosVohralik2010.pdf:JiranekStrakosVohralik2010.pdf:PDF}
}

@ARTICLE{Keyes2011,
  author =	 {David E. Keyes},
  title =	 {Exaflop/s: The why and the how},
  journal =	 {Comptes Rendus M{\'e}canique},
  year =	 2011,
  volume =	 339,
  pages =	 {70--77},
  number =	 {2--3},
  abstract =	 {The best paths to the exascale summit are debatable,
                  but all are narrow and treacherous, constrained by
                  fundamental laws of physics, capital cost, operating
                  cost, power requirements, programmability, and
                  reliability.  Many scientific and engineering
                  applications force the modeling community to attempt
                  to scale this summit. Drawing on vendor projections
                  and experiences with scientific codes on
                  contemporary platforms, we outline the challenges
                  and propose roles and essential adaptations for
                  mathematical modelers in one of the great global
                  scientific quests the next decade.},
  doi =		 {10.1016/j.crme.2010.11.002},
  file =	 {Published version:Keyes2011.pdf:PDF},
  issn =	 {1631-0721},
  keywords =	 {Computer science},
  owner =	 {principe},
  timestamp =	 {2014.03.20},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S1631072110002032}
}

@ARTICLE{Keyes2011,
  author =	 {David E. Keyes},
  title =	 {Exaflop/s: The why and the how},
  journal =	 {Comptes Rendus M{\'e}canique},
  year =	 2011,
  volume =	 339,
  pages =	 {70--77},
  number =	 {2--3},
  abstract =	 {The best paths to the exascale summit are debatable,
                  but all are narrow and treacherous, constrained by
                  fundamental laws of physics, capital cost, operating
                  cost, power requirements, programmability, and
                  reliability.  Many scientific and engineering
                  applications force the modeling community to attempt
                  to scale this summit. Drawing on vendor projections
                  and experiences with scientific codes on
                  contemporary platforms, we outline the challenges
                  and propose roles and essential adaptations for
                  mathematical modelers in one of the great global
                  scientific quests the next decade.},
  doi =		 {10.1016/j.crme.2010.11.002},
  file =	 {Published version:Keyes2011.pdf:PDF},
  issn =	 {1631-0721},
  keywords =	 {Computer science},
  owner =	 {principe},
  timestamp =	 {2014.03.20},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S1631072110002032}
}

@Article{KrauseDickopfPotseKrause2015,
  Title =	 {Towards a large-scale scalable adaptive heart model
                  using shallow tree meshes},
  Author =	 {Krause, D. and Dickopf, T. and Potse, M. and Krause,
                  R.},
  Journal =	 {J. Comput. Phys.},
  Year =	 2015,
  Pages =	 {79--94},
  Volume =	 298,
  Doi =		 {10.1016/j.jcp.2015.05.005},
  File =
                  {KrauseDickopfPotseKrause2015.pdf:KrauseDickopfPotseKrause2015.pdf:PDF}
}

@Article{KronbichlerHeisterBangerth2012,
  Title =	 {High accuracy mantle convection simulation through
                  modern numerical methods},
  Author =	 {Kronbichler, M. and Heister, T. and Bangerth, W.},
  Journal =	 {Geophysical Journal International},
  Year =	 2012,
  Number =	 1,
  Pages =	 {12--29},
  Volume =	 191,
  Doi =		 {10.1111/j.1365-246X.2012.05609.x},
  File =
                  {KronbichlerHeisterBangerth2012.pdf:KronbichlerHeisterBangerth2012.pdf:PDF}
}

@Article{LarsonBengzon2008,
  Title =	 {Adaptive finite element approximation of
                  multiphysics problems},
  Author =	 {Larson, M. G. and Bengzon, F.},
  Journal =	 {Commun. Numer. Meth. En.},
  Year =	 2008,
  Number =	 6,
  Pages =	 {505--521},
  Volume =	 24,
  Doi =		 {10.1002/cnm.1087},
  File =	 {LarsonBengzon2008.pdf:LarsonBengzon2008.pdf:PDF}
}

@Article{LarsonMalqvist2007,
  Title =	 {Goal oriented adaptivity for coupled flow and
                  transport problems with applications in oil
                  reservoir simulations},
  Author =	 {Larson, M. G. and M{\aa}lqvist, A.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2007,
  Number =	 {37-40},
  Pages =	 {3546--3561},
  Volume =	 196,
  Doi =		 {10.1016/j.cma.2006.10.038},
  File =	 {LarsonMalqvist2007.pdf:LarsonMalqvist2007.pdf:PDF}
}

@Article{LarsonSoederlundBengzon2008,
  Title =	 {Adaptive finite element approximation of coupled
                  flow and transport problems with applications in
                  heat transfer},
  Author =	 {Larson, M. G. and S{\"o}derlund, R. and Bengzon, F.},
  Journal =	 {Int. J. Numer. Meth. Fluids.},
  Year =	 2008,
  Number =	 9,
  Pages =	 {1397--1420},
  Volume =	 57,
  Doi =		 {10.1002/fld.1818},
  File =
                  {LarsonSoederlundBengzon2008.pdf:LarsonSoederlundBengzon2008.pdf:PDF}
}

@Article{LarssonRunesson2015,
  Title =	 {A sequential-adaptive strategy in space-time with
                  application to consolidation of porous media},
  Author =	 {Larsson, F. and Runesson, K.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2015,
  Pages =	 {146--171},
  Volume =	 288,
  Doi =		 {10.1016/j.cma.2014.12.002},
  File =	 {LarssonRunesson2015.pdf:LarssonRunesson2015.pdf:PDF}
}

@Article{LiWiberg1998,
  Title =	 {Implementation and adaptivity of a space-time finite
                  element method for structural dynamics},
  Author =	 {Li, XD and Wiberg, NE},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 1998,
  Number =	 {1--4},
  Pages =	 {211--229},
  Volume =	 156,
  Doi =		 {10.1016/S0045-7825(97)00207-7}
}

@Article{LilienthalSchneppWeiland2014,
  Title =	 {Non-dissipative space-time hp-discontinuous {G}alerkin
                  method for the time-dependent {Maxwell} equations},
  Author =	 {Lilienthal, M. and Schnepp, S. M. and Weiland, T.},
  Journal =	 {J. Comput. Phys.},
  Year =	 2014,
  Pages =	 {589--607},
  Volume =	 275,
  Doi =		 {10.1016/j.jcp.2014.07.015},
  File =
                  {LilienthalSchneppWeiland2014.pdf:LilienthalSchneppWeiland2014.pdf:PDF}
}

@ARTICLE{Lin2009,
  author =	 {Paul T. Lin and John N. Shadid and Marzio Sala and
                  Raymond S. Tuminaro and Gary L. Hennigan and Robert
                  J. Hoekstra},
  title =	 {Performance of a parallel algebraic multilevel
                  preconditioner for stabilized finite element
                  semiconductor device modeling},
  journal =	 {Journal on Computational Physics},
  year =	 2009,
  volume =	 228,
  pages =	 {6250-6267},
  number =	 17,
  bibsource =	 {DBLP, http://dblp.uni-trier.de},
  doi =		 {http://dx.doi.org/10.1016/j.jcp.2009.05.024},
  owner =	 {principe},
  timestamp =	 {2014.03.21}
}

@ARTICLE{Lin2009,
  author =	 {Paul T. Lin and John N. Shadid and Marzio Sala and
                  Raymond S. Tuminaro and Gary L. Hennigan and Robert
                  J. Hoekstra},
  title =	 {Performance of a parallel algebraic multilevel
                  preconditioner for stabilized finite element
                  semiconductor device modeling},
  journal =	 {Journal on Computational Physics},
  year =	 2009,
  volume =	 228,
  pages =	 {6250-6267},
  number =	 17,
  bibsource =	 {DBLP, http://dblp.uni-trier.de},
  doi =		 {http://dx.doi.org/10.1016/j.jcp.2009.05.024},
  owner =	 {principe},
  timestamp =	 {2014.03.21}
}

@Article{LotstedtSoderbergRamageHemmingsson-Franden2002,
  Title =	 {Implicit solution of hyperbolic equations with
                  space-time adaptivity},
  Author =	 {Lotstedt, P and Soderberg, S and Ramage, A and
                  Hemmingsson-Franden, L},
  Journal =	 {BIT},
  Year =	 2002,
  Number =	 1,
  Pages =	 {134--158},
  Volume =	 42,
  Doi =		 {10.1023/A:1021978304268}
}

@Article{LuShephardTendulkarBeall2014,
  Title =	 {Parallel mesh adaptation for high-order finite
                  element methods with curved element geometry},
  Author =	 {Lu, Q. and Shephard, M. S. and Tendulkar, S. and
                  Beall, M.},
  Journal =	 {Eng. Comput.},
  Year =	 2014,
  Number =	 2,
  Pages =	 {271--286},
  Volume =	 30,
  Doi =		 {10.1007/s00366-013-0329-7},
  File =
                  {LuShephardTendulkarBeall2014.pdf:LuShephardTendulkarBeall2014.pdf:PDF}
}

@Article{MaderMartinsAlonsoWeide2008,
  Title =	 {{ADjoint}: {A}n Approach for the Rapid Development
                  of Discrete Adjoint Solvers},
  Author =	 {Mader, C. A. and Martins, R. R. A. and Alonso,
                  J. J. and van der Weide, E.},
  Journal =	 {AIAA J.},
  Year =	 2008,
  Number =	 4,
  Pages =	 {863--873},
  Volume =	 46,
  Doi =		 {10.2514/1.29123},
  File =
                  {MaderMartinsAlonsoWeide2008.pdf:MaderMartinsAlonsoWeide2008.pdf:PDF}
}

@ARTICLE{Mandel1993,
  author =	 {J. Mandel},
  title =	 {Balancing domain decomposition},
  journal =	 {Communications in Numerical Methods in Engineering},
  year =	 1993,
  volume =	 9,
  pages =	 {233--241},
  number =	 3,
  abstract =	 {Abstract 10.1002/cnm.1640090307.abs The
                  Neumann-Neumann algorithm is known to be an
                  efficient domain decomposition preconditioner with
                  unstructured subdomains for iterative solution of
                  finite-element discretizations of difficult problems
                  with strongly discontinuous coefficients (De Roeck
                  and Le Tallec, 1991). However, this algorithm
                  suffers from the need to solve in each iteration an
                  inconsistent singular problem for every subdomain,
                  and its convergence deteriorates with increasing
                  number of subdomains due to the lack of a coarse
                  problem to propagate the error globally. We show
                  that the equilibrium conditions for the singular
                  problems on subdomains lead to a simple and natural
                  construction of a coarse problem. The construction
                  is purely algebraic and applies also to systems such
                  as those that arise in elasticity. A convergence
                  bound independent of the number of subdomains is
                  proved and results of computational tests are
                  reported.},
  file =	 {:Papers/Domdec/bdd.ps:PostScript},
  issn =	 {1099-0887},
  keywords =	 {Domdec},
  owner =	 {principe},
  publisher =	 {John Wiley \& Sons, Ltd},
  timestamp =	 {2012.07.17},
  url =		 {http://dx.doi.org/10.1002/cnm.1640090307}
}

@ARTICLE{Mandel1993,
  author =	 {J. Mandel},
  title =	 {Balancing domain decomposition},
  journal =	 {Communications in Numerical Methods in Engineering},
  year =	 1993,
  volume =	 9,
  pages =	 {233--241},
  number =	 3,
  abstract =	 {Abstract 10.1002/cnm.1640090307.abs The
                  Neumann-Neumann algorithm is known to be an
                  efficient domain decomposition preconditioner with
                  unstructured subdomains for iterative solution of
                  finite-element discretizations of difficult problems
                  with strongly discontinuous coefficients (De Roeck
                  and Le Tallec, 1991). However, this algorithm
                  suffers from the need to solve in each iteration an
                  inconsistent singular problem for every subdomain,
                  and its convergence deteriorates with increasing
                  number of subdomains due to the lack of a coarse
                  problem to propagate the error globally. We show
                  that the equilibrium conditions for the singular
                  problems on subdomains lead to a simple and natural
                  construction of a coarse problem. The construction
                  is purely algebraic and applies also to systems such
                  as those that arise in elasticity. A convergence
                  bound independent of the number of subdomains is
                  proved and results of computational tests are
                  reported.},
  file =	 {:Papers/Domdec/bdd.ps:PostScript},
  issn =	 {1099-0887},
  keywords =	 {Domdec},
  owner =	 {principe},
  publisher =	 {John Wiley \& Sons, Ltd},
  timestamp =	 {2012.07.17},
  url =		 {http://dx.doi.org/10.1002/cnm.1640090307}
}

@ARTICLE{Mandel2003,
  author =	 {Mandel, Jan and Dohrmann, Clark R.},
  title =	 {Convergence of a balancing domain decomposition by
                  constraints and energy minimization},
  journal =	 {Numerical Linear Algebra with Applications},
  year =	 2003,
  volume =	 10,
  pages =	 {639--659},
  number =	 7,
  abstract =	 {A convergence theory is presented for a
                  substructuring preconditioner based on constrained
                  energy minimization concepts. The substructure
                  spaces consist of local functions with zero values
                  of the constraints, while the coarse space consists
                  of minimal energy functions with the constraint
                  values continuous across substructure interfaces.
                  In applications, the constraints include values at
                  corners and optionally averages on edges and
                  faces. The preconditioner is reformulated as an
                  additive Schwarz method and analysed by building on
                  existing results for balancing domain
                  decomposition. The main result is a bound on the
                  condition number based on inequalities involving the
                  matrices of the preconditioner. Estimates of the
                  form {C(1+log2(H/h))} are obtained under the
                  standard assumptions of substructuring theory.
                  Computational results demonstrating the performance
                  of method are included. Published in 2003 by John
                  Wiley \& Sons, Ltd.},
  doi =		 {10.1002/nla.341},
  file =
                  {341_ftp.pdf:/Users/sbadia/Work/zotero/storage/BC2PG2N7/341_ftp.pdf:PDF},
  issn =	 {1099-1506},
  keywords =	 {additive Schwarz, balancing domain decomposition,
                  {FETI}, iterative substructuring,
                  {Neumann--Neumann}, non-overlapping domain
                  decomposition},
  timestamp =	 {2012.10.19},
  url =		 {http://dx.doi.org/10.1002/nla.341}
}

@ARTICLE{Mandel2008,
  author =	 {Mandel, Jan and Soused\'ik, Bed\v{r}ich and
                  Dohrmann, Clark},
  title =	 {Multispace and multilevel {BDDC}},
  journal =	 {Computing},
  year =	 2008,
  volume =	 83,
  pages =	 {55-85},
  abstract =	 {The Balancing Domain Decomposition by Constraints
                  (BDDC) method is the most advanced method from the
                  Balancing family of iterative substructuring methods
                  for the solution of large systems of linear
                  algebraic equations arising from discretization of
                  elliptic boundary value problems.  In the case of
                  many substructures, solving the coarse problem
                  exactly becomes a bottleneck. Since the coarse
                  problem in BDDC has the same structure as the
                  original problem, it is straightforward to apply the
                  BDDC method recursively to solve the coarse problem
                  only approximately.  In this paper, we formulate a
                  new family of abstract Multispace BDDC methods and
                  give condition number bounds from the abstract
                  additive Schwarz preconditioning theory. The
                  Multilevel BDDC is then treated as a special case of
                  the Multispace BDDC and abstract multilevel
                  condition number bounds are given. The abstract
                  bounds yield polylogarithmic condition number bounds
                  for an arbitrary fixed number of levels and scalar
                  elliptic problems discretized by finite elements in
                  two and three spatial dimensions. Numerical
                  experiments confirm the theory.},
  affiliation =	 {University of Colorado Denver Department of
                  Mathematical and Statistical Sciences P.O. Box
                  173364 Campus Box 170 Denver CO 80217 USA},
  file =
                  {:/home/principe/Javier/References/Papers/Domdec/Mandel2008.pdf:PDF},
  issn =	 {0010-485X},
  issue =	 2,
  keyword =	 {Computer Science},
  owner =	 {principe},
  publisher =	 {Springer Wien},
  timestamp =	 {2012.10.22},
  url =		 {http://dx.doi.org/10.1007/s00607-008-0014-7}
}

@Article{MeidnerRannacherVihharev2009,
  Title =	 {Goal-oriented error control of the iterative
                  solution of finite element equations},
  Author =	 {Meidner, D. and Rannacher, R. and Vihharev, J.},
  Journal =	 {J. Numer. Math.},
  Year =	 2009,
  Number =	 2,
  Pages =	 {143--172},
  Volume =	 17,
  Doi =		 {10.1515/JNUM.2009.009},
  File =
                  {MeidnerRannacherVihharev2009.pdf:MeidnerRannacherVihharev2009.pdf:PDF}
}

@INBOOK{Meier-Yang2006,
  chapter =	 {Parallel Algebraic Multigrid Methods - High
                  Performance Preconditioners},
  pages =	 {209-236},
  title =	 {Numerical Solution of Partial Differential Equations
                  on Parallel Computers},
  publisher =	 {Springer-Verlag},
  year =	 2006,
  editor =	 {Bruaset A.M. and Tveito A.},
  author =	 {U. Meier-Yang},
  owner =	 {principe},
  timestamp =	 {2014.03.19}
}

@INBOOK{Meier-Yang2006,
  chapter =	 {Parallel Algebraic Multigrid Methods - High
                  Performance Preconditioners},
  pages =	 {209-236},
  title =	 {Numerical Solution of Partial Differential Equations
                  on Parallel Computers},
  publisher =	 {Springer-Verlag},
  year =	 2006,
  editor =	 {Bruaset A.M. and Tveito A.},
  author =	 {U. Meier-Yang},
  owner =	 {principe},
  timestamp =	 {2014.03.19}
}

@Article{MelenkWohlmuth2001,
  Title =	 {On residual-based a posteriori error estimation in
                  hp-FEM},
  Author =	 {Melenk, J. M. and Wohlmuth, B. I.},
  Journal =	 {Advances in Computational Mathematics},
  Year =	 2001,
  Number =	 {1-4},
  Pages =	 {311--331},
  Volume =	 15,
  Doi =		 {10.1023/A:1014268310921},
  File =	 {MelenkWohlmuth2001.pdf:MelenkWohlmuth2001.pdf:PDF}
}

@Article{MichelettiPerotto2008,
  Title =	 {Anisotropic mesh adaption for time-dependent
                  problems},
  Author =	 {Micheletti, S. and Perotto, S.},
  Journal =	 {Int. J. Numer. Meth. Fluids.},
  Year =	 2008,
  Number =	 9,
  Pages =	 {1009--1015},
  Volume =	 58,
  Doi =		 {10.1002/fld.1597}
}

@Article{OdenVemaganti2000,
  Title =	 {Estimation of Local Modeling Error and Goal-Oriented
                  Adaptive Modeling of Heterogeneous Materials:
                  {I}. {E}rror Estimates and Adaptive Algorithms},
  Author =	 {Oden, J. T. and Vemaganti, K. S.},
  Journal =	 {J. Comput. Phys.},
  Year =	 2000,
  Number =	 1,
  Pages =	 {22--47},
  Volume =	 164,
  Doi =		 {10.1006/jcph.2000.6585},
  File =	 {OdenVemaganti2000.pdf:OdenVemaganti2000.pdf:PDF}
}

@Article{Pardo2010,
  Title =	 {Multigoal-oriented adaptivity for hp-discontinuous
                  finite element methods},
  Author =	 {Pardo, D.},
  Journal =	 {Procedia Comput. Sci.},
  Year =	 2010,
  Number =	 1,
  Pages =	 {1953--1961},
  Volume =	 1,
  Doi =		 {10.1016/j.procs.2010.04.219},
  File =	 {Pardo2010.pdf:Pardo2010.pdf:PDF}
}

@Article{PortaPerottoBallio2012,
  Title =	 {A space-time adaptation scheme for unsteady shallow
                  water problems},
  Author =	 {Porta, G. M. and Perotto, S. and Ballio, F.},
  Journal =	 {Math. Comput. Simulat.},
  Year =	 2012,
  Number =	 {12, SI},
  Pages =	 {2929--2950},
  Volume =	 82,
  Doi =		 {10.1016/j.matcom.2011.06.004},
  File =
                  {PortaPerottoBallio2012.pdf:PortaPerottoBallio2012.pdf:PDF}
}

@Article{RannacherVihharev2013,
  Title =	 {Adaptive finite element analysis of nonlinear
                  problems: balancing of discretization and iteration
                  errors},
  Author =	 {Rannacher, R. and Vihharev, J.},
  Journal =	 {J. Numer. Math.},
  Year =	 2013,
  Number =	 1,
  Pages =	 {23--61},
  Volume =	 21,
  Doi =		 {10.1515/jnum-2013-0002},
  File =
                  {RannacherVihharev2013.pdf:RannacherVihharev2013.pdf:PDF}
}

@Article{RannacherWestenbergerWollner2010,
  Title =	 {Adaptive finite element solution of eigenvalue
                  problems: Balancing of discretization and iteration
                  error},
  Author =	 {Rannacher, R. and Westenberger, A. and Wollner, W.},
  Journal =	 {J. Numer. Math.},
  Year =	 2010,
  Number =	 4,
  Pages =	 {303--327},
  Volume =	 18,
  Doi =		 {10.1515/JNUM.2010.015}
}

@Article{RemacleFrazaoLiShephard2006,
  Title =	 {An adaptive discretization of shallow-water
                  equations based on discontinuous {G}alerkin methods},
  Author =	 {Remacle, J.-F. and Fraz{\~a}o, S. S. and Li, X. and
                  Shephard, M. S.},
  Journal =	 {Int. J. Numer. Meth. Fluids.},
  Year =	 2006,
  Number =	 8,
  Pages =	 {903--923},
  Volume =	 52,
  Doi =		 {10.1002/fld.1204},
  File =
                  {RemacleFrazaoLiShephard2006.pdf:RemacleFrazaoLiShephard2006.pdf:PDF}
}

@Article{ReyGosseletRey2015,
  Title =	 {Strict bounding of quantities of interest in
                  computations based on domain decomposition},
  Author =	 {Rey, V. and Gosselet, P. and Rey, C.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2015,
  Pages =	 {212--228},
  Volume =	 287,
  Doi =		 {10.1016/j.cma.2015.01.009},
  File =	 {ReyGosseletRey2015.pdf:ReyGosseletRey2015.pdf:PDF}
}

@Article{ReyReyGosselet2014,
  Title =	 {A strict error bound with separated contributions of
                  the discretization and of the iterative solver in
                  non-overlapping domain decomposition methods},
  Author =	 {Rey, V. and Rey, C. and Gosselet, P.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2014,
  Pages =	 {293--303},
  Volume =	 270,
  Doi =		 {0.1016/j.cma.2013.12.001},
  File =	 {ReyReyGosselet2014.pdf:ReyReyGosselet2014.pdf:PDF}
}

@Article{RodriguezSahniLaheyJansen2013,
  Title =	 {A parallel adaptive mesh method for the numerical
                  simulation of multiphase flows},
  Author =	 {Rodriguez, J. M. and Sahni, O. and Lahey Jr,
                  R. T. and Jansen, K. E.},
  Journal =	 {Comput. Fluids},
  Year =	 2013,
  Pages =	 {115--131},
  Volume =	 87,
  Doi =		 {10.1016/j.compfluid.2013.04.004},
  File =
                  {RodriguezSahniLaheyJansen2013.pdf:RodriguezSahniLaheyJansen2013.pdf:PDF}
}

@Article{RossaCoutinho2013,
  Title =	 {Parallel adaptive simulation of gravity currents on
                  the lock-exchange problem},
  Author =	 {Rossa, A. L. and Coutinho, A. L.},
  Journal =	 {Comput. Fluids},
  Year =	 2013,
  Pages =	 {782--794},
  Volume =	 88,
  Doi =		 {10.1016/j.compfluid.2013.06.008},
  File =	 {RossaCoutinho2013.pdf:RossaCoutinho2013.pdf:PDF}
}

@Article{RossiCotelaLafontaineDadvandIdelsohn2013,
  Title =	 {Parallel adaptive mesh refinement for incompressible
                  flow problems},
  Author =	 {Rossi, R. and Cotela, J. and Lafontaine, N. M. and
                  Dadvand, P. and Idelsohn, S. R.},
  Journal =	 {Comput. Fluids},
  Year =	 2013,
  Pages =	 {342--355},
  Volume =	 80,
  Doi =		 {10.1016/j.compfluid.2012.01.023},
  File =
                  {RossiCotelaLafontaineDadvandIdelsohn2013.pdf:RossiCotelaLafontaineDadvandIdelsohn2013.pdf:PDF}
}

@Article{SchmichVexler2008,
  Title =	 {Adaptivity with dynamic meshes for space-time finite
                  element discretizations of parabolic equations},
  Author =	 {Schmich, Michael and Vexler, Boris},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2008,
  Number =	 1,
  Pages =	 {369--393},
  Volume =	 30,
  Doi =		 {10.1137/060670468}
}

@Article{SimsekWuZeeBrummelen2015,
  Title =	 {Duality-based two-level error estimation for
                  time-dependent {PDE}s: Application to linear and
                  nonlinear parabolic equations},
  Author =	 {Simsek, G. and Wu, X. and van der Zee, K. G. and van
                  Brummelen, E. H.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2015,
  Number =	 {SI},
  Pages =	 {83--109},
  Volume =	 288,
  Doi =		 {10.1016/j.cma.2014.11.019},
  File =
                  {SimsekWuZeeBrummelen2015.pdf:SimsekWuZeeBrummelen2015.pdf:PDF}
}

@Article{SolinDubcovaKruis2010,
  Title =	 {Adaptive hp-{FEM} with dynamical meshes for
                  transient heat and moisture transfer problems},
  Author =	 {Solin, Pavel and Dubcova, Lenka and Kruis, Jaroslav},
  Journal =	 {J. Comput. Appl. Math.},
  Year =	 2010,
  Note =	 {2nd International Conference on Finite Element
                  Methods in Engineering and Science, Tahoe City, CA,
                  JAN 05-09, 2009},
  Number =	 {12, {SI}},
  Pages =	 {3103--3112},
  Volume =	 233,
  Doi =		 {10.1016/j.cam.2009.07.025}
}

@Article{SolinKorous2013,
  Title =	 {Space-time adaptive hp-{FEM} for problems with
                  traveling sharp fronts},
  Author =	 {Solin, P. and Korous, L.},
  Journal =	 {Computing},
  Year =	 2013,
  Number =	 {1, S, SI},
  Pages =	 {S709--S722},
  Volume =	 95,
  Doi =		 {10.1007/s00607-012-0243-7},
  File =	 {SolinKorous2013.pdf:SolinKorous2013.pdf:PDF}
}

@ARTICLE{Stuben2001,
  author =	 {K. St\"{u}ben},
  title =	 {A review of algebraic multigrid},
  journal =	 {Journal of Computational and Applied Mathematics},
  year =	 2001,
  volume =	 128,
  pages =	 {281-309},
  number =	 {1-2},
  abstract =	 {Since the early 1990s, there has been a strongly
                  increasing demand for more efficient methods to
                  solve large sparse, unstructured linear systems of
                  equations. For practically relevant problem sizes,
                  classical one-level methods had already reached
                  their limits and new hierarchical algorithms had to
                  be developed in order to allow an efficient solution
                  of even larger problems. This paper gives a review
                  of the first hierarchical and purely matrix-based
                  approach, algebraic multigrid (AMG). AMG can
                  directly be applied, for instance, to efficiently
                  solve various types of elliptic partial differential
                  equations discretized on unstructured meshes, both
                  in 2D and 3D. Since AMG does not make use of any
                  geometric information, it is a “plug-in” solver
                  which can even be applied to problems without any
                  geometric background, provided that the underlying
                  matrix has certain properties.},
  doi =		 {10.1016/S0377-0427(00)00516-1},
  issn =	 {0377-0427},
  keywords =	 {Algebraic multigrid},
  owner =	 {principe},
  timestamp =	 {2013.07.02},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0377042700005161}
}

@BOOK{Toselli2005,
  title =	 {Domain Decomposition Methods - Algorithms and
                  Theory},
  publisher =	 {Springer-Verlag},
  year =	 2005,
  author =	 {A. Toselli and O. Widlund},
  file =	 {:Books&Courses/Domain Decomposition Methods
                  Algorithms and Theory - Andrea Toselli.pdf:PDF},
  keywords =	 {Domdec},
  owner =	 {principe},
  timestamp =	 {2012.07.17}
}

@techreport{Trilinos-Overview,
  title =	 "{An Overview of Trilinos}",
  author =	 "Michael Heroux and Roscoe Bartlett and Vicki Howle
                  Robert Hoekstra and Jonathan Hu and Tamara Kolda and
                  Richard Lehoucq and Kevin Long and Roger Pawlowski
                  and Eric Phipps and Andrew Salinger and Heidi
                  Thornquist and Ray Tuminaro and James Willenbring
                  and Alan Williams ",
  institution =	 "Sandia National Laboratories",
  number =	 "SAND2003-2927",
  year =	 2003
}

@techreport{Trilinos-Overview,
  title =	 "{An Overview of Trilinos}",
  author =	 "Michael Heroux and Roscoe Bartlett and Vicki Howle
                  Robert Hoekstra and Jonathan Hu and Tamara Kolda and
                  Richard Lehoucq and Kevin Long and Roger Pawlowski
                  and Eric Phipps and Andrew Salinger and Heidi
                  Thornquist and Ray Tuminaro and James Willenbring
                  and Alan Williams ",
  institution =	 "Sandia National Laboratories",
  number =	 "SAND2003-2927",
  year =	 2003
}

@Article{VerdugoDiez2012,
  Title =	 {Computable bounds of functional outputs in linear
                  visco-elastodynamics},
  Author =	 {Verdugo, F. and D{\'i}ez, P.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2012,
  Pages =	 {313--330},
  Volume =	 {245--246},
  Doi =		 {10.1016/j.cma.2012.06.016},
  File =	 {VerdugoDiez2012.pdf:VerdugoDiez2012.pdf:PDF}
}

@Article{VerdugoParesDiez2013,
  Title =	 {Modal-based goal-oriented error assessment for
                  timeline-dependent quantities in transient dynamics},
  Author =	 {Verdugo, F. and Par{\'e}s, N. and D{\'i}ez, P.},
  Journal =	 {Int. J. Numer. Meth. Engng.},
  Year =	 2013,
  Number =	 8,
  Pages =	 {685--720},
  Volume =	 95,
  Doi =		 {10.1002/nme.4538},
  File =
                  {VerdugoParesDiez2013.pdf:VerdugoParesDiez2013.pdf:PDF}
}

@Article{VerdugoParesDiez2014,
  Title =	 {Error Assessment in Structural Transient Dynamics},
  Author =	 {Verdugo, F. and Par{\'e}s, N. and D{\'i}ez, P.},
  Journal =	 {Arch. Comput. Method. E.},
  Year =	 2014,
  Number =	 1,
  Pages =	 {59--90},
  Volume =	 21,
  Doi =		 {10.1007/s11831-014-9096-x},
  File =
                  {VerdugoParesDiez2014.pdf:VerdugoParesDiez2014.pdf:PDF}
}

@Article{VerdugoParesDiez2014a,
  Title =	 {Goal-oriented space-time adaptivity for transient
                  dynamics using a modal description of the adjoint
                  solution},
  Author =	 {Verdugo, F. and Par{\'e}s, N. and D{\'i}ez, P.},
  Journal =	 {Comput. Mech.},
  Year =	 2014,
  Number =	 2,
  Pages =	 {331--352},
  Volume =	 54,
  Doi =		 {10.1007/s00466-014-0988-2},
  File =
                  {VerdugoParesDiez2014a.pdf:VerdugoParesDiez2014a.pdf:PDF}
}

@Article{VeyVoigt2007,
  Title =	 {Adaptive full domain covering meshes for parallel
                  finite element computations},
  Author =	 {Vey, S. and Voigt, A.},
  Journal =	 {Computing},
  Year =	 2007,
  Number =	 1,
  Pages =	 {53--75},
  Volume =	 81,
  Doi =		 {10.1007/s00607-007-0243-1},
  File =	 {VeyVoigt2007.pdf:VeyVoigt2007.pdf:PDF}
}

@Article{WangMoinIaccarino2009,
  Title =	 {Minimal Repetition Dynamic Checkpointing Algorithm
                  for Unsteady Adjoint Calculation},
  Author =	 {Wang, Q. and Moin, P. and Iaccarino, G.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2009,
  Number =	 4,
  Pages =	 {2549--2567},
  Volume =	 31,
  Doi =		 {10.1137/080727890},
  File =
                  {WangMoinIaccarino2009.pdf:WangMoinIaccarino2009.pdf:PDF}
}

@Article{ZanderBogKollmannsbergerSchillingerRank2015,
  Title =	 {Multi-level $hp$-adaptivity: high-order mesh
                  adaptivity without the difficulties of constraining
                  hanging nodes},
  Author =	 {Zander, N. and Bog, T. and Kollmannsberger, S. and
                  Schillinger, D. and Rank, E.},
  Journal =	 {Comput. Mech.},
  Year =	 2015,
  Number =	 3,
  Pages =	 {499--517},
  Volume =	 55,
  Doi =		 {10.1007/s00466-014-1118-x},
  File =
                  {ZanderBogKollmannsbergerSchillingerRank2015.pdf:ZanderBogKollmannsbergerSchillingerRank2015.pdf:PDF}
}

@Article{ZeeBrummelenAkkermanBorst2011,
  Title =	 {Goal-oriented error estimation and adaptivity for
                  fluid-structure interaction using exact linearized
                  adjoints},
  Author =	 {van der Zee, K. G.G. and van Brummelen, E. H. and
                  Akkerman, I. and de Borst, R.},
  Journal =	 {Comput. Methods Appl. Mech. Engrg.},
  Year =	 2011,
  Number =	 {37-40},
  Pages =	 {2738--2757},
  Volume =	 200,
  Doi =		 {10.1016/j.cma.2010.12.010},
  File =
                  {ZeeBrummelenAkkermanBorst2011.pdf:ZeeBrummelenAkkermanBorst2011.pdf:PDF},
  Review =	 {- + Goal oriented mesh adaptivity for FSI + Very
                  difficult derivation of the linearized adjoint for
                  fluid-structure interaction.  + They find the exact
                  equation for the adjoint, and solve it using
                  suitable numerical methods, independent of the
                  method used for the primal problem.  + The FSI
                  problem is resolved with fixed point methods.  +
                  Only stationary fluid structure interaction. A very
                  simple fluid model and a very simple structure
                  model.  -}
}

@Article{ZeeBrummelenBorst2010,
  Title =	 {Goal-Oriented Error Estimation and Adaptivity for
                  Free-Boundary Problems: The Domain-Map Linearization
                  Approach},
  Author =	 {van der Zee, K. G. and van Brummelen, E. H. and de
                  Borst, R.},
  Journal =	 {SIAM J. Sci. Comput.},
  Year =	 2010,
  Number =	 2,
  Pages =	 {1064--1092},
  Volume =	 32,
  Doi =		 {10.1137/080741227},
  File =
                  {ZeeBrummelenBorst2010.pdf:ZeeBrummelenBorst2010.pdf:PDF}
}

@misc{_additive_????,
  title =	 {Additive {Manufacturing} {Workshop} {Presentations}
                  and {Videos}},
  url =
                  {http://sites.nationalacademies.org/PGA/biso/IUTAM/PGA_168737},
  urldate =	 {2015-12-29},
  file =	 {Additive Manufacturing Workshop Presentations and
                  Videos:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/HVJEQ446/PGA_168737.html:text/html}
}

@article{achdou_domain_2000,
  title =	 {A domain decomposition preconditioner for an
                  advection--diffusion problem},
  volume =	 184,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782599002273},
  doi =		 {10.1016/S0045-7825(99)00227-3},
  abstract =	 {We propose a generalization of the Neumann-Neumann
                  algorithm for advection-diffusion problems: the
                  Neumann conditions are replaced by suitable Robin
                  conditions. The method is first introduced and
                  analysed in the case where the domain is partitioned
                  into vertical strips, then generalized to an
                  arbitrary decomposition. A coarse space procedure is
                  proposed. Finally, numerical results for
                  bidimensional and tridimensional tests are given.},
  number =	 {2-4},
  urldate =	 {2013-07-15},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Achdou, Yves and Le Tallec, Patrick and Nataf,
                  Frédéric and Vidrascu, Marina},
  year =	 2000,
  pages =	 {145--170},
  file =	 {ScienceDirect Full Text
                  PDF:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/QG2BSK5B/Achdou
                  et al. - 2000 - A domain decomposition
                  preconditioner for an
                  advec.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/BF26VHXP/S0045782599002273.html:text/html}
}

@ARTICLE{achdou_domain_2000,
  author =	 {Achdou, Yves and Le Tallec, Patrick and Nataf,
                  Fr\'ed\'eric and Vidrascu, Marina},
  title =	 {A domain decomposition preconditioner for an
                  advection--diffusion problem},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  year =	 2000,
  volume =	 184,
  pages =	 {145--170},
  number =	 {2--4},
  abstract =	 {We propose a generalization of the
                  {NeumannâNeumann} algorithm for
                  advectionâdiffusion problems: the Neumann
                  conditions are replaced by suitable Robin
                  conditions. The method is first introduced and
                  analysed in the case where the domain is partitioned
                  into vertical strips, then generalized to an
                  arbitrary decomposition. A coarse space procedure is
                  proposed. Finally, numerical results for
                  bidimensional and tridimensional tests are given.},
  doi =		 {10.1016/S0045-7825(99)00227-3},
  file =	 {ScienceDirect Full Text
                  PDF:/home/principe/Javier/System/Config/firefox/zotero/storage/BNKIRCT6/Achdou
                  et al. - 2000 - A domain decomposition
                  preconditioner for an
                  advec.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/DBBSCNZ8/S0045782599002273.html:text/html},
  issn =	 {0045-7825},
  owner =	 {principe},
  timestamp =	 {2014.03.23},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782599002273},
  urldate =	 {2013-07-15}
}

@ARTICLE{achdou_domain_2000,
  author =	 {Achdou, Yves and Le Tallec, Patrick and Nataf,
                  Fr\'{e}d\'{e}ric and Vidrascu, Marina},
  title =	 {A domain decomposition preconditioner for an
                  advection--diffusion problem},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  year =	 2000,
  volume =	 184,
  pages =	 {145--170},
  number =	 {2--4},
  abstract =	 {We propose a generalization of the
                  {NeumannâNeumann} algorithm for
                  advectionâdiffusion problems: the Neumann
                  conditions are replaced by suitable Robin
                  conditions. The method is first introduced and
                  analysed in the case where the domain is partitioned
                  into vertical strips, then generalized to an
                  arbitrary decomposition. A coarse space procedure is
                  proposed. Finally, numerical results for
                  bidimensional and tridimensional tests are given.},
  doi =		 {10.1016/S0045-7825(99)00227-3},
  file =	 {ScienceDirect Full Text
                  PDF:/home/principe/Javier/System/Config/firefox/zotero/storage/BNKIRCT6/Achdou
                  et al. - 2000 - A domain decomposition
                  preconditioner for an
                  advec.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/DBBSCNZ8/S0045782599002273.html:text/html},
  issn =	 {0045-7825},
  owner =	 {principe},
  timestamp =	 {2014.03.23},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782599002273},
  urldate =	 {2013-07-15}
}

@inproceedings{aluru_parallel_1997,
  title =	 {Parallel domain decomposition and load balancing
                  using space-filling curves},
  doi =		 {10.1109/HIPC.1997.634498},
  abstract =	 {Partitioning techniques based on space filling
                  curves have received much recent attention due to
                  their low running time and good load balance
                  characteristics. The basic idea underlying these
                  methods is to order the multidimensional data
                  according to a space filling curve and partition the
                  resulting one dimensional order. However, space
                  filling curves are defined for points that lie on a
                  uniform grid of a particular resolution. It is
                  typically assumed that the coordinates of the points
                  are representable using a fixed number of bits, and
                  the run times of the algorithms depend upon the
                  number of bits used. We present a simple and
                  efficient technique for ordering arbitrary and
                  dynamic multidimensional data using space filling
                  curves and its application to parallel domain
                  decomposition and load balancing. Our technique is
                  based on a comparison routine that determines the
                  relative position of two points in the order induced
                  by a space filling curve. The comparison routine
                  could then be used in conjunction with any parallel
                  sorting algorithm to effect parallel domain
                  decomposition},
  booktitle =	 {Fourth International Conference on High-Performance
                  Computing, 1997. Proceedings},
  author =	 {Aluru, S. and Sevilgen, F.},
  year =	 1997,
  keywords =	 {balance characteristics, comparison routine,
                  computational geometry, Concurrent computing, Costs,
                  curve fitting, dynamic multidimensional data,
                  Filling, Iterative algorithms, Iterative methods,
                  load balancing, Load management, Mathematics,
                  mathematics computing, multidimensional data,
                  Multidimensional systems, one dimensional order,
                  Parallel algorithms, parallel domain decomposition,
                  parallel programming, parallel sorting algorithm,
                  Partitioning algorithms, partitioning techniques,
                  relative position, resource allocation, Runtime,
                  Sorting, space filling curves, uniform grid},
  pages =	 {230--235},
  file =	 {IEEE Xplore Abstract
                  Record:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/48KMF56V/abs_all.html:text/html}
}

@inproceedings{aluru_parallel_1997,
  title =	 {Parallel domain decomposition and load balancing
                  using space-filling curves},
  doi =		 {10.1109/HIPC.1997.634498},
  abstract =	 {Partitioning techniques based on space filling
                  curves have received much recent attention due to
                  their low running time and good load balance
                  characteristics. The basic idea underlying these
                  methods is to order the multidimensional data
                  according to a space filling curve and partition the
                  resulting one dimensional order. However, space
                  filling curves are defined for points that lie on a
                  uniform grid of a particular resolution. It is
                  typically assumed that the coordinates of the points
                  are representable using a fixed number of bits, and
                  the run times of the algorithms depend upon the
                  number of bits used. We present a simple and
                  efficient technique for ordering arbitrary and
                  dynamic multidimensional data using space filling
                  curves and its application to parallel domain
                  decomposition and load balancing. Our technique is
                  based on a comparison routine that determines the
                  relative position of two points in the order induced
                  by a space filling curve. The comparison routine
                  could then be used in conjunction with any parallel
                  sorting algorithm to effect parallel domain
                  decomposition},
  booktitle =	 {Fourth International Conference on High-Performance
                  Computing, 1997. Proceedings},
  author =	 {Aluru, S. and Sevilgen, F.},
  year =	 1997,
  keywords =	 {balance characteristics, comparison routine,
                  computational geometry, Concurrent computing, Costs,
                  curve fitting, dynamic multidimensional data,
                  Filling, Iterative algorithms, Iterative methods,
                  load balancing, Load management, Mathematics,
                  mathematics computing, multidimensional data,
                  Multidimensional systems, one dimensional order,
                  Parallel algorithms, parallel domain decomposition,
                  parallel programming, parallel sorting algorithm,
                  Partitioning algorithms, partitioning techniques,
                  relative position, resource allocation, Runtime,
                  Sorting, space filling curves, uniform grid},
  pages =	 {230--235},
  file =	 {IEEE Xplore Abstract
                  Record:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/48KMF56V/abs_all.html:text/html}
}

@inproceedings{ang_abstract_2014,
	title = {Abstract {Machine} {Models} and {Proxy} {Architectures} for {Exascale} {Computing}},
	isbn = {978-1-4799-7564-8},
	url = {http://ieeexplore.ieee.org/document/7017960/},
	doi = {10.1109/Co-HPC.2014.4},
	urldate = {2017-01-13},
	publisher = {IEEE},
	author = {Ang, J.A. and Barrett, R.F. and Benner, R.E. and Burke, D. and Chan, C. and Cook, J. and Donofrio, D. and Hammond, S.D. and Hemmert, K.S. and Kelly, S.M. and Le, H. and Leung, V.J. and Resnick, D.R. and Rodrigues, A.F. and Shalf, J. and Stark, D. and Unat, D. and Wright, N.J.},
	year = {2014},
	pages = {25--32},
        booktitle = {Co-HPC'14 Proceedings of the 1st International Workshop on Hardware-Software Co-Design for High Performance Computing}
}

@book{ao_2012_2012,
  address =	 {Hong Kong},
  series =	 {World {Congress} on {Engineering}, {WCE} 2012},
  title =	 {The 2012 {International} {Conference} of
                  {Manufacturing} {Engineering} and {Engineering}
                  {Management}, the 2012 {International} {Conference}
                  of {Mechanical} {Engineering}},
  isbn =	 {978-988-19252-2-0},
  language =	 {eng},
  number =	 {4 - 6 July, 2012, Imperial College London, London,
                  U.K / organized by Association, IAENG, International
                  Association of Engineers. S. I. Ao ... (eds.) ;
                  Vol. 3},
  publisher =	 {IAENG},
  editor =	 {Ao, S. I. and {International Association of
                  Engineers}},
  year =	 2012,
  file =
                  {WCE2012_pp1518-1523.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/FF8QAHQQ/WCE2012_pp1518-1523.pdf:application/pdf}
}

@article{araya_multiscale_2013,
  title =	 {Multiscale Hybrid-Mixed Method},
  volume =	 51,
  issn =	 {0036-1429},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/120888223},
  doi =		 {10.1137/120888223},
  abstract =	 {This work presents a priori and a posteriori error
                  analyses of a new multiscale hybrid-mixed method
                  ({MHM)} for an elliptic model. Specially designed to
                  incorporate multiple scales into the construction of
                  basis functions, this finite element method relaxes
                  the continuity of the primal variable through the
                  action of Lagrange multipliers, while assuring the
                  strong continuity of the normal component of the
                  flux (dual variable). As a result, the dual
                  variable, which stems from a simple postprocessing
                  of the primal variable, preserves local
                  conservation. We prove existence and uniqueness of a
                  solution for the {MHM} method as well as optimal
                  convergence estimates of any order in the natural
                  norms.  Also, we propose a face-residual a
                  posteriori error estimator, and prove that it
                  controls the error of both variables in the natural
                  norms. Several numerical tests assess the
                  theoretical results., This work presents a priori
                  and a posteriori error analyses of a new multiscale
                  hybrid-mixed method ({MHM)} for an elliptic
                  model. Specially designed to incorporate multiple
                  scales into the construction of basis functions,
                  this finite element method relaxes the continuity of
                  the primal variable through the action of Lagrange
                  multipliers, while assuring the strong continuity of
                  the normal component of the flux (dual variable). As
                  a result, the dual variable, which stems from a
                  simple postprocessing of the primal variable,
                  preserves local conservation. We prove existence and
                  uniqueness of a solution for the {MHM} method as
                  well as optimal convergence estimates of any order
                  in the natural norms.  Also, we propose a
                  face-residual a posteriori error estimator, and
                  prove that it controls the error of both variables
                  in the natural norms. Several numerical tests assess
                  the theoretical results.},
  number =	 6,
  urldate =	 {2014-05-15},
  journal =	 {{SIAM} Journal on Numerical Analysis},
  author =	 {Araya, R. and Harder, C. and Paredes, D. and
                  Valentin, F.},
  year =	 2013,
  pages =	 {3505--3531},
  file =
                  {Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/HN6A6C22/120888223.html:text/html}
}

@article{arnold_unified_2002,
  title =	 {Unified analysis of discontinuous {Galerkin} methods
                  for elliptic problems},
  volume =	 39,
  copyright =	 {Copyright © 2002 Society for Industrial and Applied
                  Mathematics},
  issn =	 {0036-1429},
  url =		 {http://www.jstor.org/stable/4101034},
  abstract =	 {We provide a framework for the analysis of a large
                  class of discontinuous methods for second-order
                  elliptic problems. It allows for the understanding
                  and comparison of most of the discontinuous Galerkin
                  methods that have been proposed over the past three
                  decades for the numerical treatment of elliptic
                  problems.},
  number =	 5,
  urldate =	 {2015-01-28},
  journal =	 {SIAM Journal on Numerical Analysis},
  author =	 {Arnold, Douglas N. and Brezzi, Franco and Cockburn,
                  Bernardo and Marini, L. Donatella},
  year =	 2002,
  pages =	 {1749--1779},
  file =	 {JSTOR Full Text
                  PDF:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/ZFACPU33/Arnold
                  et al. - 2002 - Unified Analysis of Discontinuous
                  Galerkin Methods.pdf:application/pdf}
}

@article{art-mlbddc,
  title =	 {Multilevel Balancing Domain Decomposition at Extreme
                  Scales},
  issn =	 {1064-8275},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/15M1013511},
  doi =		 {10.1137/15M1013511},
  abstract =	 {In this paper we present a fully distributed,
                  communicator-aware, recursive, and
                  interlevel-overlapped message-passing implementation
                  of the multilevel balancing domain decomposition by
                  constraints (MLBDDC) preconditioner. The
                  implementation highly relies on subcommunicators in
                  order to achieve the desired effect of coarse-grain
                  overlapping of computation and communication, and
                  communication and communication among levels in the
                  hierarchy (namely, interlevel
                  overlapping). Essentially, the main communicator is
                  split into as many nonoverlapping subsets of
                  message-passing interface (MPI) tasks (i.e., MPI
                  subcommunicators) as levels in the
                  hierarchy. Provided that specialized resources
                  (cores and memory) are devoted to each level, a
                  careful rescheduling and mapping of all the
                  computations and communications in the algorithm
                  lets a high degree of overlapping be exploited among
                  levels. All subroutines and associated data
                  structures are expressed recursively, and therefore
                  MLBDDC preconditioners with an arbitrary number of
                  levels can be built while re-using significant and
                  recurrent parts of the codes. This approach leads to
                  excellent weak scalability results as soon as
                  level-1 tasks can fully overlap coarser-levels
                  duties. We provide a model to indicate how to choose
                  the number of levels and coarsening ratios between
                  consecutive levels and determine qualitatively the
                  scalability limits for a given choice. We have
                  carried out a comprehensive weak scalability
                  analysis of the proposed implementation for the
                  three-dimensional Laplacian and linear elasticity
                  problems on structured and unstructured
                  meshes. Excellent weak scalability results have been
                  obtained up to 458,752 IBM BG/Q cores and 1.8
                  million MPI being, being the first time that exact
                  domain decomposition preconditioners (only based on
                  sparse direct solvers) reach these scales.},
  urldate =	 {2016-01-28},
  journal =	 {SIAM Journal on Scientific Computing},
  volume =	 38,
  number =	 1,
  author =	 {Badia, Santiago and Mart\'in, Alberto F. and
                  Principe, Javier},
  year =	 2016,
  pages =	 {C22--C52},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/RA2WP2XF/Badia
                  et al. - 2016 - Multilevel Balancing Domain
                  Decomposition at
                  Extre.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/U54ZVX27/15M1013511.html:text/html}
}

@article{art-st,
	title = {Space-Time Balancing Domain Decomposition},
	volume = {39},
	issn = {1064-8275},
	url = {http://epubs.siam.org/doi/10.1137/16M1074266},
	doi = {10.1137/16M1074266},
	abstract = {In this work, we propose two-level space-time domain decomposition preconditioners for parabolic problems discretized using finite elements. They are motivated as an extension to space-time of balancing domain decomposition by constraints preconditioners. The key ingredients to be defined are the subassembled space and operator, the coarse degrees of freedom (DOFs) in which we want to enforce continuity among subdomains at the preconditioner level, and the transfer operator from the subassembled to the original finite element space. With regard to the subassembled operator, a perturbation of the time derivative is needed to end up with a well-posed preconditioner. The set of coarse DOFs includes the time average (at the space-time subdomain) of classical space constraints plus new constraints between consecutive subdomains in time. Numerical experiments show that the proposed schemes are weakly scalable in time, i.e., we can efficiently exploit increasing computational resources to solve more time steps in the same total elapsed time. Further, the scheme is also weakly space-time scalable, since it leads to asymptotically constant iterations when solving larger problems both in space and time. Excellent wall clock time weak scalability is achieved for space-time parallel solvers on some thousands of cores.},
	number = {2},
	urldate = {2018-02-05},
	journal = {SIAM Journal on Scientific Computing},
	author = {Badia, S. and Olm, M.},
	year = {2017},
	pages = {C194--C213},
	file = {Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/MTZUUQ7E/Badia and Olm - 2017 - Space-Time Balancing Domain Decomposition.pdf:application/pdf;Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/L75865AP/16M1074266.html:text/html}
}

@ARTICLE{artstokes,
  author =	 {Badia, S. and Mart\'in, A. F.},
  title =	 {Balancing domain decomposition preconditioning for
                  the discrete {S}tokes problem with continuous
                  pressures},
  journal =	 {In preparation},
  year =	 2014
}

@ARTICLE{artstokes,
  author =	 {Badia, S. and Mart\'in, A. F.},
  title =	 {Scalable preconditioners for the Stokes problem with
                  continuous pressures},
  journal =	 {In preparation},
  year =	 2014
}

@article{ascher_implicit-explicit_1997,
  title =	 {Implicit-explicit Runge-Kutta methods for
                  time-dependent partial differential equations},
  volume =	 25,
  issn =	 {0168-9274},
  shorttitle =	 {Special Issue on Time Integration},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0168927497000561},
  doi =		 {10.1016/S0168-9274(97)00056-1},
  abstract =	 {Implicit-explicit ({IMEX)} linear multistep
                  time-discretization schemes for partial differential
                  equations have proved useful in many
                  applications. However, they tend to have undesirable
                  time-step restrictions when applied to
                  convection-diffusion problems, unless diffusion
                  strongly dominates and an appropriate {BDF-based}
                  scheme is selected (Ascher et al., 1995). In this
                  paper, we develop Runge-Kutta-based {IMEX} schemes
                  that have better stability regions than the best
                  known {IMEX} multistep schemes over a wide parameter
                  range.},
  number =	 {2-3},
  urldate =	 {2013-03-06},
  journal =	 {Applied Numerical Mathematics},
  author =	 {Ascher, Uri M. and Ruuth, Steven J. and Spiteri,
                  Raymond J.},
  year =	 1997,
  pages =	 {151--167},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/UBWQRZER/Ascher
                  et al. - 1997 - Implicit-explicit Runge-Kutta
                  methods for
                  time-dep.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/I9T5ABN4/S0168927497000561.html:text/html}
}

@article{ascher_implicit-explicit_1997,
  title =	 {Implicit-explicit Runge-Kutta methods for
                  time-dependent partial differential equations},
  volume =	 25,
  issn =	 {0168-9274},
  shorttitle =	 {Special Issue on Time Integration},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0168927497000561},
  doi =		 {10.1016/S0168-9274(97)00056-1},
  abstract =	 {Implicit-explicit ({IMEX)} linear multistep
                  time-discretization schemes for partial differential
                  equations have proved useful in many
                  applications. However, they tend to have undesirable
                  time-step restrictions when applied to
                  convection-diffusion problems, unless diffusion
                  strongly dominates and an appropriate {BDF-based}
                  scheme is selected (Ascher et al., 1995). In this
                  paper, we develop Runge-Kutta-based {IMEX} schemes
                  that have better stability regions than the best
                  known {IMEX} multistep schemes over a wide parameter
                  range.},
  number =	 {2--3},
  urldate =	 {2013-03-06},
  journal =	 {Applied Numerical Mathematics},
  author =	 {Ascher, Uri M. and Ruuth, Steven J. and Spiteri,
                  Raymond J.},
  year =	 1997,
  pages =	 {151--167},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/UBWQRZER/Ascher
                  et al. - 1997 - Implicit-explicit Runge-Kutta
                  methods for
                  time-dep.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/I9T5ABN4/S0168927497000561.html:text/html}
}

@article{bachorski1999finite,
  title={Finite-element prediction of distortion during gas metal arc welding using the shrinkage volume approach},
  author={Bachorski, A and Painter, MJ and Smailes, AJ and Wahab, Muhammad Abdul},
  journal={Journal of Materials Processing Technology},
  volume={92},
  pages={405--409},
  year={1999},
  publisher={Elsevier}
}

@book{bader_space-filling_2012,
	title = {Space-Filling Curves: An Introduction With Applications in Scientific Computing},
	isbn = {978-3-642-31045-4},
	shorttitle = {Space-Filling Curves},
	abstract = {The present book provides an introduction to using space-filling curves ({SFC}) as tools in scientific computing. Special focus is laid on the representation of {SFC} and on resulting algorithms. For example, grammar-based techniques are introduced for traversals of Cartesian and octree-type meshes, and arithmetisation of {SFC} is explained to compute {SFC} mappings and indexings.The locality properties of {SFC} are discussed in detail, together with their importance for algorithms. Templates for parallelisation and cache-efficient algorithms are presented to reflect the most important applications of {SFC} in scientific computing. Special attention is also given to the interplay of adaptive mesh refinement and {SFC}, including the structured refinement of triangular and tetrahedral grids. For each topic, a short overview is given on the most important publications and recent research activities.},
	pagetotal = {286},
	publisher = {Springer Science \& Business Media},
	author = {Bader, Michael},
        year = {2012},
	date = {2012-10-14},
	langid = {english},
	note = {Google-Books-{ID}: {eIe}\_OdFP0WkC},
	keywords = {Computers / Computer Science, Computers / Data Processing, Computers / General, Computers / Programming / Algorithms, Mathematics / Applied, Mathematics / Discrete Mathematics, Mathematics / Numerical Analysis}
}

@unpublished{badia-heterogeneous,
	title = {Physics-based balancing domain decomposition by constraints for multi-material problems},
	url = {https://hal.archives-ouvertes.fr/hal-01337968},
	abstract = {In this work, we present a novel balancing domain decomposition by constraints preconditioner that is robust for multi-material and heterogeneous problems. We start with a well-balanced subdomain partition and, based on an aggregation of elements according to their physical coefficients, we end up with a finer physics-based ({PB}) subdomain partition. Next, we define geometrical objects (corners, edges, and faces) for this {PB} partition, and select some of them to enforce subdomain continuity (primal objects). When the physical coefficient in each {PB} subdomain is constant and the primal objects satisfy a mild condition on the existence of acceptable paths, we can show both theoretically and numerically that the condition number does not depend on the contrast of the coefficient. The constant coefficient condition is computationally feasible for multi-material problems. However, for highly heterogeneous problems, such restriction might result into a large coarse problem. In this case, we propose a relaxed version of the method where we only require that the maximal contrast of the physical coefficient in each {PB} subdomain is smaller than a predefined threshold. The threshold can be chosen so that the condition number is reasonably small while the size of the coarse problem is not too large. An extensive set of numerical experiments is provided to support our findings. In particular, we show a robustness and a weak scalability analysis up to 8000 cores of the new preconditioner when applied to a 3D heterogeneous problem with more than 260 million degrees of freedom. For the scalability analysis, we have exploited a highly scalable advanced inter-level overlapped implementation of the preconditioner that deals very efficiently with the coarse problem computation.},
	author = {Badia, Santiago and Martín, Alberto F. and Nguyen, Hieu},
        year = {2016},
	urldate = {2017-01-10},
	date = {2016-11},
	note = {Submitted},
	keywords = {adaptive coarse space, {BDDC}, heterogeneous problem, parallel, parallel solver, preconditioner},
	file = {HAL PDF Full Text:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/QCT6UEC4/Badia et al. - 2016 - PHYSICS-BASED BALANCING DOMAIN DECOMPOSITION BY CO.pdf:application/pdf}
}

@article{badia_adaptive_2013,
  title =	 {Adaptive Finite Element Simulation of Incompressible
                  Flows by Hybrid Continuous-Discontinuous {Galerkin}
                  Formulations},
  volume =	 35,
  issn =	 {1064-8275},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/120880732},
  doi =		 {10.1137/120880732},
  abstract =	 {In this work we design hybrid
                  continuous-discontinuous finite element spaces that
                  permit discontinuities on nonmatching element
                  interfaces of nonconforming meshes. Then we develop
                  an equal-order stabilized finite element formulation
                  for incompressible flows over these hybrid spaces,
                  which combines the element interior stabilization of
                  SUPG-type continuous Galerkin formulations and the
                  jump stabilization of discontinuous Galerkin
                  formulations. Optimal stability and convergence
                  results are obtained. For the adaptive setting, we
                  use a standard error estimator and marking
                  strategy. Numerical experiments show the optimal
                  accuracy of the hybrid algorithm for both uniformly
                  and adaptively refined nonconforming meshes. The
                  outcome of this work is a finite element formulation
                  that can naturally be used on nonconforming meshes,
                  as discontinuous Galerkin formulations, while
                  keeping the much lower CPU cost of continuous
                  Galerkin formulations., In this work we design
                  hybrid continuous-discontinuous finite element
                  spaces that permit discontinuities on nonmatching
                  element interfaces of nonconforming meshes. Then we
                  develop an equal-order stabilized finite element
                  formulation for incompressible flows over these
                  hybrid spaces, which combines the element interior
                  stabilization of SUPG-type continuous Galerkin
                  formulations and the jump stabilization of
                  discontinuous Galerkin formulations. Optimal
                  stability and convergence results are obtained. For
                  the adaptive setting, we use a standard error
                  estimator and marking strategy. Numerical
                  experiments show the optimal accuracy of the hybrid
                  algorithm for both uniformly and adaptively refined
                  nonconforming meshes. The outcome of this work is a
                  finite element formulation that can naturally be
                  used on nonconforming meshes, as discontinuous
                  Galerkin formulations, while keeping the much lower
                  CPU cost of continuous Galerkin formulations.},
  number =	 1,
  urldate =	 {2014-11-13},
  journal =	 {SIAM Journal on Scientific Computing},
  author =	 {Badia, S. and Baiges, J.},
  year =	 2013,
  pages =	 {A491--A516},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/ERWEHSBM/Badia
                  and Baiges - 2013 - Adaptive Finite Element
                  Simulation of
                  Incompressib.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/E5TK32I5/120880732.html:text/html}
}

@article{badia_algebraic_2008,
  title =	 {Algebraic Pressure Segregation Methods for the
                  Incompressible {Navier}-{Stokes} Equations},
  volume =	 15,
  issn =	 {1134-3060, 1886-1784},
  url =
                  {http://link.springer.com/article/10.1007/s11831-008-9020-3},
  doi =		 {10.1007/s11831-008-9020-3},
  abstract =	 {This work is an overview of algebraic pressure
                  segregation methods for the incompressible
                  Navier-Stokes equations. These methods can be
                  understood as an inexact LU block factorization of
                  the original system matrix. We have considered a
                  wide set of methods: algebraic pressure correction
                  methods, algebraic velocity correction methods and
                  the Yosida method. Higher order schemes, based on
                  improved factorizations, are also introduced. We
                  have also explained the relationship between these
                  pressure segregation methods and some widely used
                  preconditioners, and we have introduced
                  predictor-corrector methods, one-loop algorithms
                  where nonlinearity and iterations towards the
                  monolithic system are coupled.},
  language =	 {en},
  number =	 3,
  urldate =	 {2014-06-12},
  journal =	 {Archives of Computational Methods in Engineering},
  author =	 {Badia, S. and Codina, R.},
  year =	 2008,
  keywords =	 {Appl.Mathematics/Computational Methods of
                  Engineering},
  pages =	 {343--369}
}

@article{badia_analysis_2006,
  title =	 {Analysis of a Stabilized Finite Element
                  Approximation of the Transient Convection-Diffusion
                  Equation Using an {ALE} Framework},
  volume =	 44,
  issn =	 00361429,
  url =
                  {http://link.aip.org/link/SJNAAM/v44/i5/p2159/s1&Agg=doi},
  doi =		 {10.1137/050643532},
  number =	 5,
  urldate =	 {2012-02-10},
  journal =	 {SIAM Journal on Numerical Analysis},
  author =	 {Badia, Santiago and Codina, Ramon},
  year =	 2006,
  pages =	 2159
}

@article{badia_analysis_2015,
  title =	 {Analysis of an Unconditionally Convergent Stabilized
                  Finite Element Formulation for Incompressible
                  Magnetohydrodynamics},
  volume =	 22,
  issn =	 {1134-3060, 1886-1784},
  url =		 {http://link.springer.com/10.1007/s11831-014-9129-5},
  doi =		 {10.1007/s11831-014-9129-5},
  language =	 {en},
  number =	 4,
  urldate =	 {2015-10-22},
  journal =	 {Archives of Computational Methods in Engineering},
  author =	 {Badia, Santiago and Codina, Ramon and Planas, Ramon},
  year =	 2015,
  pages =	 {621--636},
  file =
                  {art%3A10.1007%2Fs11831-014-9129-5.pdf:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/V56PVIH3/art%3A10.1007%2Fs11831-014-9129-5.pdf:application/pdf}
}

@article{badia_atomistic--continuum_2008,
  title =	 {On Atomistic-to-Continuum Coupling by Blending},
  volume =	 7,
  issn =	 15403459,
  url =
                  {http://epubs.siam.org/mms/resource/1/mmsubt/v7/i1/p381_s1},
  doi =		 {10.1137/07069969X},
  number =	 1,
  urldate =	 {2012-02-10},
  journal =	 {Multiscale Modeling \& Simulation},
  author =	 {Badia, Santiago and Parks, Michael and Bochev, Pavel
                  and Gunzburger, Max and Lehoucq, Richard},
  year =	 2008,
  pages =	 381
}

@article{badia_block_2014,
  title =	 {Block recursive {LU} preconditioners for the
                  thermally coupled incompressible inductionless {MHD}
                  problem},
  volume =	 274,
  issn =	 00219991,
  url =
                  {http://linkinghub.elsevier.com/retrieve/pii/S0021999114004355},
  doi =		 {10.1016/j.jcp.2014.06.028},
  language =	 {en},
  urldate =	 {2015-05-21},
  journal =	 {Journal of Computational Physics},
  author =	 {Badia, Santiago and Mart\'in, Alberto F. and Planas,
                  Ramon},
  year =	 2014,
  pages =	 {562--591},
  file =
                  {1-s2.0-S0021999114004355-main.pdf:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/5QQ9XCTF/1-s2.0-S0021999114004355-main.pdf:application/pdf}
}

@article{badia_combined_2011,
  title =	 {A combined nodal continuous-discontinuous finite
                  element formulation for the {Maxwell} problem},
  volume =	 218,
  issn =	 {0096-3003},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0096300311012306},
  doi =		 {10.1016/j.amc.2011.09.058},
  abstract =	 {Continuous Galerkin formulations are appealing due
                  to their low computational cost, whereas
                  discontinuous Galerkin formulation facilitate
                  adaptative mesh refinement and are more accurate in
                  regions with jumps of physical parameters. Since
                  many electromagnetic problems involve materials with
                  different physical properties, this last point is
                  very important. For this reason, in this article we
                  have developed a combined cG–dG formulation for
                  Maxwell’s problem that allows arbitrary finite
                  element spaces with functions continuous in patches
                  of finite elements and discontinuous on the
                  interfaces of these patches. In particular, the
                  second formulation we propose comes from a novel
                  continuous Galerkin formulation that reduces the
                  amount of stabilization introduced in the numerical
                  system. In all cases, we have performed stability
                  and convergence analyses of the methods. The outcome
                  of this work is a new approach that keeps the low
                  CPU cost of recent nodal continuous formulations
                  with the ability to deal with coefficient jumps and
                  adaptivity of discontinuous ones. All these methods
                  have been tested using a problem with singular
                  solution and another one with different materials,
                  in order to prove that in fact the resulting
                  formulations can properly deal with these problems.},
  number =	 8,
  urldate =	 {2012-02-10},
  journal =	 {Applied Mathematics and Computation},
  author =	 {Badia, Santiago and Codina, Ramon},
  year =	 2011,
  keywords =	 {Discontinuous Galerkin methods, Electromagnetism,
                  Finite elements, Maxwell’s problem, Singular
                  solutions, Stabilization techniques},
  pages =	 {4276--4294}
}

@article{badia_convergence_2007,
  title =	 {Convergence analysis of the {FEM} approximation of
                  the first order projection method for incompressible
                  flows with and without the inf-sup condition},
  volume =	 107,
  issn =	 {0029-599X, 0945-3245},
  url =
                  {http://www.springerlink.com/content/a0m714k4j4576285/},
  doi =		 {10.1007/s00211-007-0099-5},
  number =	 4,
  urldate =	 {2012-02-10},
  journal =	 {Numerische Mathematik},
  author =	 {Badia, Santiago and Codina, Ramon},
  year =	 2007,
  pages =	 {533--557}
}

@article{badia_convergence_2014,
  title =	 {Convergence towards weak solutions of the
                  {Navier}-{Stokes} equations for a finite element
                  approximation with numerical subgrid-scale
                  modelling},
  volume =	 34,
  issn =	 {0272-4979, 1464-3642},
  url =		 {http://imajna.oxfordjournals.org/content/34/3/1193},
  doi =		 {10.1093/imanum/drt023},
  abstract =	 {Residual-based stabilized finite element (FE)
                  techniques for the Navier–Stokes equations lead to
                  numerical discretizations that provide convection
                  stabilization as well as pressure stability without
                  the need to satisfy an inf–sup condition. They can
                  be motivated by using a variational multiscale (VMS)
                  framework, based on the decomposition of the fluid
                  velocity into a resolvable FE component plus a
                  modelled subgrid-scale component. The subgrid
                  closure acts as a large eddy simulation turbulence
                  model, leading to accurate under-resolved
                  simulations. However, even though VMS formulations
                  are increasingly used in the applied FE community,
                  their numerical analysis has been restricted to a
                  priori estimates and convergence to smooth solutions
                  only, via a priori error estimates. In this work, we
                  prove that some versions of these methods (based on
                  dynamic and orthogonal closures) also converge to
                  weak (turbulent) solutions of the Navier–Stokes
                  equations. These results are obtained by using
                  compactness results in Bochner–Lebesgue spaces.},
  language =	 {en},
  number =	 3,
  urldate =	 {2014-08-20},
  journal =	 {IMA Journal of Numerical Analysis},
  author =	 {Badia, Santiago and Guti\'errez-Santacreu, Juan
                  Vicente},
  year =	 2014,
  keywords =	 {Convergence, Navier–Stokes equations, Stability,
                  Stabilized finite element methods, subgrid scales,
                  Variational multiscale methods},
  pages =	 {1193--1221}
}

@article{badia_convergence_2016,
	title = {Convergence to Suitable Weak Solutions for a Finite Element Approximation of the {Navier}–{Stokes} Equations with Numerical Subgrid Scale Modeling},
	volume = {71},
	issn = {0885-7474, 1573-7691},
	url = {https://link.springer.com/article/10.1007/s10915-016-0304-8},
	doi = {10.1007/s10915-016-0304-8},
	abstract = {In this work we prove that weak solutions constructed by a variational multiscale method are suitable in the sense of Scheffer. In order to prove this result, we consider a subgrid model that enforces orthogonality between subgrid and finite element components. Further, the subgrid component must be tracked in time. Since this type of schemes introduce pressure stabilization, we have proved the result for equal-order velocity and pressure finite element spaces that do not satisfy a discrete inf-sup condition.},
	language = {en},
	number = {1},
	urldate = {2018-02-05},
	journal = {Journal of Scientific Computing},
	author = {Badia, S. and Gutiérrez-Santacreu, J. V.},
	year = {2017},
	pages = {386--413},
	file = {Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/B7W87YJG/s10915-016-0304-8.html:text/html}
}

@article{badia_coupling_2009,
  title =	 {Coupling {Biot} and {Navier}-{Stokes} equations for
                  modelling fluid-poroelastic media interaction},
  volume =	 228,
  issn =	 {0021-9991},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0021999109004069},
  doi =		 {10.1016/j.jcp.2009.07.019},
  abstract =	 {The interaction between a fluid and a poroelastic
                  structure is a complex problem that couples the
                  Navier–Stokes equations with the Biot system. The
                  finite element approximation of this problem is
                  involved due to the fact that both subproblems are
                  indefinite. In this work, we first design
                  residual-based stabilization techniques for the Biot
                  system, motivated by the variational multiscale
                  approach. Then, we state the monolithic
                  Navier–Stokes/Biot system with the appropriate
                  transmission conditions at the interface. For the
                  solution of the coupled system, we adopt both
                  monolithic solvers and heterogeneous domain
                  decomposition strategies. Different domain
                  decomposition methods are considered and their
                  convergence is analyzed for a simplified problem. We
                  compare the efficiency of all the methods on a test
                  problem that exhibits a large added-mass effect, as
                  it happens in hemodynamics applications.},
  number =	 21,
  urldate =	 {2012-02-10},
  journal =	 {Journal of Computational Physics},
  author =	 {Badia, Santiago and Quaini, Annalisa and Quarteroni,
                  Alfio},
  year =	 2009,
  keywords =	 {Biot system, Darcy’s problem, Fluid–structure
                  interaction, Hemodynamics, Poromechanics, Stabilized
                  finite elements},
  pages =	 {7986--8014}
}

@article{badia_differentiable_2016,
	title = {Differentiable monotonicity-preserving schemes for discontinuous {Galerkin} methods on arbitrary meshes},
	volume = {320},
	issn = {0045-7825},
	url = {http://www.sciencedirect.com/science/article/pii/S0045782516319090},
	doi = {10.1016/j.cma.2017.03.032},
	abstract = {This work is devoted to the design of interior penalty discontinuous Galerkin (dG) schemes that preserve maximum principles at the discrete level for the steady transport and convection–diffusion problems and the respective transient problems with implicit time integration. Monotonic schemes that combine explicit time stepping with dG space discretization are very common, but the design of such schemes for implicit time stepping is rare, and it had only been attained so far for 1D problems. The proposed scheme is based on a piecewise linear dG discretization supplemented with an artificial diffusion that linearly depends on a shock detector that identifies the troublesome areas. In order to define the new shock detector, we have introduced the concept of discrete local extrema. The diffusion operator is a graph-Laplacian, instead of the more common finite element discretization of the Laplacian operator, which is essential to keep monotonicity on general meshes and in multi-dimension. The resulting nonlinear stabilization is non-smooth and nonlinear solvers can fail to converge. As a result, we propose a smoothed (twice differentiable) version of the nonlinear stabilization, which allows us to use Newton with line search nonlinear solvers and dramatically improve nonlinear convergence. A theoretical numerical analysis of the proposed schemes shows that they satisfy the desired monotonicity properties. Further, the resulting operator is Lipschitz continuous and there exists at least one solution of the discrete problem, even in the non-smooth version. We provide a set of numerical results to support our findings.},
	urldate = {2018-02-05},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Badia, S. and Bonilla, J. and Hierro, A.},
	year = {2017},
	keywords = {Finite elements, Discontinuous Galerkin, Discrete maximum principle, Monotonicity, Local extrema diminishing, Shock capturing},
	pages = {582--605},
	file = {ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/EIR7733F/S0045782516319090.html:text/html}
}


@article{badia_discrete_2015,
  title =	 {On discrete maximum principles for discontinuous
                  {Galerkin} methods},
  volume =	 286,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S004578251400485X},
  doi =		 {10.1016/j.cma.2014.12.006},
  abstract =	 {The aim of this work is to propose a
                  monotonicity-preserving method for discontinuous
                  Galerkin (dG) approximations of convection–diffusion
                  problems. To do so, a novel definition of discrete
                  maximum principle (DMP) is proposed using the
                  discrete variational setting of the problem, and we
                  show that the fulfilment of this DMP implies that
                  the minimum/maximum (depending on the sign of the
                  forcing term) is on the boundary for
                  multidimensional problems. Then, an artificial
                  viscosity (AV) technique is designed for
                  convection-dominant problems that satisfies the
                  above mentioned DMP. The noncomplete stabilized
                  interior penalty dG method is proved to fulfil the
                  DMP property for the one-dimensional linear case
                  when adding such AV with certain parameters. The
                  benchmarks for the constant values to satisfy the
                  DMP are calculated and tested in the numerical
                  experiments section. Finally, the method is applied
                  to different test problems in one and two dimensions
                  to show its performance.},
  urldate =	 {2015-01-12},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Badia, Santiago and Hierro, Alba},
  year =	 2015,
  keywords =	 {Convection–diffusion, Convection-dominated flows,
                  Discontinuous Galerkin, Nonlinear stabilization,
                  shock capturing, Stabilized finite elements},
  pages =	 {107--122},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/X9I4FPQS/Badia
                  and Hierro - 2015 - On discrete maximum principles
                  for discontinuous
                  G.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/BI7P2G73/S004578251400485X.html:text/html}
}

@article{badia_enhanced_2013,
  title =	 {Enhanced balancing {Neumann}-{Neumann}
                  preconditioning in computational fluid and solid
                  mechanics},
  volume =	 96,
  copyright =	 {Copyright © 2013 John Wiley \& Sons, Ltd.},
  issn =	 {1097-0207},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nme.4541/abstract},
  doi =		 {10.1002/nme.4541},
  abstract =	 {In this work, we propose an enhanced implementation
                  of balancing Neumann–Neumann (BNN) preconditioning
                  together with a detailed numerical comparison
                  against the balancing domain decomposition by
                  constraints (BDDC) preconditioner. As model
                  problems, we consider the Poisson and linear
                  elasticity problems. On one hand, we propose a novel
                  way to deal with singular matrices and
                  pseudo-inverses appearing in local solvers. It is
                  based on a kernel identification strategy that
                  allows us to efficiently compute the action of the
                  pseudo-inverse via local indefinite solvers. We
                  further show how, identifying a minimum set of
                  degrees of freedom to be fixed, an equivalent
                  definite system can be solved instead, even in the
                  elastic case. On the other hand, we propose a simple
                  implementation of the algorithm that reduces the
                  number of Dirichlet solvers to only one per
                  iteration, leading to similar computational cost as
                  additive methods. After these improvements of the
                  BNN preconditioned conjugate gradient algorithm, we
                  compare its performance against that of the BDDC
                  preconditioners on a pair of large-scale
                  distributed-memory platforms. The enhanced BNN
                  method is a competitive preconditioner for
                  three-dimensional Poisson and elasticity problems
                  and outperforms the BDDC method in many
                  cases. Copyright © 2013 John Wiley \& Sons, Ltd.},
  language =	 {en},
  number =	 4,
  urldate =	 {2014-06-12},
  journal =	 {International Journal for Numerical Methods in
                  Engineering},
  author =	 {Badia, Santiago and Mart\'in, Alberto F. and
                  Principe, Javier},
  year =	 2013,
  keywords =	 {Balancing domain decomposition, BDDC, BNN,
                  coarse-grid correction, domain decomposition,
                  elasticity, parallelization, scalability},
  pages =	 {203--230},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/MQ38X3GX/Badia
                  et al. - 2013 - Enhanced balancing Neumann–Neumann
                  preconditioning.pdf:application/pdf}
}

@article{badia_error_2014,
  title =	 {Error analysis of discontinuous {Galerkin} methods
                  for the {Stokes} problem under minimal regularity},
  volume =	 34,
  issn =	 {0272-4979, 1464-3642},
  url =		 {http://imajna.oxfordjournals.org/content/34/2/800},
  doi =		 {10.1093/imanum/drt022},
  abstract =	 {In this article, we analyse several discontinuous
                  Galerkin (DG) methods for the Stokes problem under
                  minimal regularity on the solution. We assume that
                  the velocity u belongs to and the pressure . First,
                  we analyse standard DG methods assuming that the
                  right-hand side f belongs to . A DG method that is
                  well defined for f belonging to is then
                  investigated. The methods under study include
                  stabilized DG methods using equal-order spaces and
                  inf–sup stable ones where the pressure space is one
                  polynomial degree less than the velocity space.},
  language =	 {en},
  number =	 2,
  urldate =	 {2014-06-12},
  journal =	 {IMA Journal of Numerical Analysis},
  author =	 {Badia, S. and Codina, R. and Gudi, T. and Guzm\'an,
                  J.},
  year =	 2014,
  keywords =	 {Discontinuous Galerkin, Error estimates, Finite
                  element, Stabilized methods, Stokes problems},
  pages =	 {800--819},
  file =
                  {Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/J66W5TM6/800.html:text/html}
}

@article{badia_finite_2011,
  title =	 {Finite element approximation of nematic liquid
                  crystal flows using a saddle-point structure},
  volume =	 230,
  issn =	 {0021-9991},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0021999110006480},
  doi =		 {10.1016/j.jcp.2010.11.033},
  abstract =	 {In this work, we propose finite element schemes for
                  the numerical approximation of nematic liquid
                  crystal flows, based on a saddle-point formulation
                  of the director vector sub-problem. It introduces a
                  Lagrange multiplier that allows to enforce the
                  sphere condition. In this setting, we can consider
                  the limit problem (without penalty) and the
                  penalized problem (using a Ginzburg–Landau penalty
                  function) in a unified way. Further, the resulting
                  schemes have a stable behavior with respect to the
                  value of the penalty parameter, a key difference
                  with respect to the existing schemes. Two different
                  methods have been considered for the time
                  integration. First, we have considered an implicit
                  algorithm that is unconditionally stable and energy
                  preserving. The linearization of the problem at
                  every time step value can be performed using a
                  quasi-Newton method that allows to decouple fluid
                  velocity and director vector computations for every
                  tangent problem. Then, we have designed a linear
                  semi-implicit algorithm (i.e. it does not involve
                  nonlinear iterations) and proved that it is
                  unconditionally stable, verifying a discrete energy
                  inequality. Finally, some numerical simulations are
                  provided.},
  number =	 4,
  urldate =	 {2012-02-10},
  journal =	 {Journal of Computational Physics},
  author =	 {Badia, Santiago and Guill\'en-Gonz\'alez, Francisco
                  and Guti\'errez-Santacreu, Juan Vicente},
  year =	 2011,
  keywords =	 {Ericksen–Leslie problem, Finite element methods,
                  Ginzburg–Landau problem, Nematic liquid crystals,
                  Saddle-point problems},
  pages =	 {1686--1706},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/WDZH58ZF/Badia
                  et al. - 2011 - Finite element approximation of
                  nematic liquid cry.pdf:application/pdf}
}

@article{badia_fluidstructure_2007,
  title =	 {On some fluid-structure iterative algorithms using
                  pressure segregation methods. {Application} to
                  aeroelasticity},
  volume =	 72,
  issn =	 {1097-0207},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nme.1998/abstract},
  doi =		 {10.1002/nme.1998},
  abstract =	 {In this paper we suggest some algorithms for the
                  fluid–structure interaction problem stated using a
                  domain decomposition framework. These methods
                  involve stabilized pressure segregation methods for
                  the solution of the fluid problem and fixed point
                  iterative algorithms for the fluid–structure
                  coupling. With one single loop the solution of the
                  coupled system tends to one of the strongly coupled
                  monolithic systems. These coupling algorithms are
                  applied to the aeroelastic simulation of suspension
                  bridges. We assess flexural and torsional
                  frequencies for a given inflow velocity. Increasing
                  this velocity we reach the value for which the
                  flutter phenomenon appears. Copyright © 2007 John
                  Wiley \& Sons, Ltd.},
  language =	 {en},
  number =	 1,
  urldate =	 {2012-02-10},
  journal =	 {International Journal for Numerical Methods in
                  Engineering},
  author =	 {Badia, Santiago and Codina, Ramon},
  year =	 2007,
  keywords =	 {aeroelasticity, bridge aerodynamics, Fluid–structure
                  interaction, pressure segregation},
  pages =	 {46--71}
}

@article{badia_fluidstructure_2008,
  title =	 {Fluid-structure partitioned procedures based on
                  {Robin} transmission conditions},
  volume =	 227,
  issn =	 {0021-9991},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0021999108002192},
  doi =		 {10.1016/j.jcp.2008.04.006},
  abstract =	 {In this article we design new partitioned procedures
                  for fluid–structure interaction problems, based on
                  Robin-type transmission conditions. The choice of
                  the coefficient in the Robin conditions is justified
                  via simplified models. The strategy is effective
                  whenever an incompressible fluid interacts with a
                  relatively thin membrane, as in hemodynamics
                  applications. We analyze theoretically the new
                  iterative procedures on a model problem, which
                  represents a simplified blood-vessel system. In
                  particular, the Robin–Neumann scheme exhibits
                  enhanced convergence properties with respect to the
                  existing partitioned procedures. The theoretical
                  results are checked using numerical
                  experimentation.},
  number =	 14,
  urldate =	 {2012-02-10},
  journal =	 {Journal of Computational Physics},
  author =	 {Badia, Santiago and Nobile, Fabio and Vergara,
                  Christian},
  year =	 2008,
  keywords =	 {Added-mass effect, Fluid–structure interaction,
                  Hemodynamics, Partitioned procedures, Robin boundary
                  conditions, Transmission conditions},
  pages =	 {7027--7051}
}

@article{badia_force-based_2007,
  title =	 {A Force-Based Blending Model for
                  Atomistic-to-Continuum Coupling},
  volume =	 5,
  issn =	 {1543-1649},
  url =
                  {http://www.dl.begellhouse.com/journals/61fd1b191cf7e96f,4883346404604d91,15fcdac44934638e.html},
  doi =		 {10.1615/IntJMultCompEng.v5.i5.30},
  number =	 5,
  urldate =	 {2012-02-10},
  journal =	 {International Journal for Multiscale Computational
                  Engineering},
  author =	 {Badia, S. and Bochev, P. and Lehoucq, R. and Parks,
                  M. L. and Fish, Jacob and Nuggehally, Mohan A. and
                  Gunzburger, M.},
  year =	 2007,
  pages =	 {387--406}
}

@article{badia_highly_2014,
  title =	 {A Highly Scalable Parallel Implementation of
                  Balancing Domain Decomposition by Constraints},
  volume =	 36,
  issn =	 {1064-8275},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/130931989},
  doi =		 {10.1137/130931989},
  abstract =	 {In this work we propose a novel parallelization
                  approach of two-level balancing domain decomposition
                  by constraints preconditioning based on overlapping
                  of fine-grid and coarse-grid duties in time. The
                  global set of MPI tasks is split into those that
                  have fine-grid duties and those that have
                  coarse-grid duties, and the different computations
                  and communications in the algorithm are then
                  rescheduled and mapped in such a way that the
                  maximum degree of overlapping is achieved while
                  preserving data dependencies among them. In many
                  ranges of interest, the extra cost associated to the
                  coarse-grid problem can be fully masked by fine-grid
                  related computations (which are embarrassingly
                  parallel). Apart from discussing code implementation
                  details, the paper also presents a comprehensive set
                  of numerical experiments that includes weak
                  scalability analyses with structured and
                  unstructured meshes for the three-dimensional
                  Poisson and linear elasticity problems on a pair of
                  state-of-the-art multicore-based distributed-memory
                  machines. This experimental study reveals remarkable
                  weak scalability in the solution of problems with
                  thousands of millions of unknowns on several tens of
                  thousands of computational cores., In this work we
                  propose a novel parallelization approach of
                  two-level balancing domain decomposition by
                  constraints preconditioning based on overlapping of
                  fine-grid and coarse-grid duties in time. The global
                  set of MPI tasks is split into those that have
                  fine-grid duties and those that have coarse-grid
                  duties, and the different computations and
                  communications in the algorithm are then rescheduled
                  and mapped in such a way that the maximum degree of
                  overlapping is achieved while preserving data
                  dependencies among them. In many ranges of interest,
                  the extra cost associated to the coarse-grid problem
                  can be fully masked by fine-grid related
                  computations (which are embarrassingly
                  parallel). Apart from discussing code implementation
                  details, the paper also presents a comprehensive set
                  of numerical experiments that includes weak
                  scalability analyses with structured and
                  unstructured meshes for the three-dimensional
                  Poisson and linear elasticity problems on a pair of
                  state-of-the-art multicore-based distributed-memory
                  machines. This experimental study reveals remarkable
                  weak scalability in the solution of problems with
                  thousands of millions of unknowns on several tens of
                  thousands of computational cores.},
  number =	 2,
  urldate =	 {2014-06-12},
  journal =	 {SIAM Journal on Scientific Computing},
  author =	 {Badia, S. and Martín, A. and Principe, J.},
  year =	 2014,
  pages =	 {C190--C218},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/FRZKGT2D/Badia
                  et al. - 2014 - A Highly Scalable Parallel
                  Implementation of Balan.pdf:application/pdf}
}

@article{badia_implementation_2013,
  title =	 {Implementation and Scalability Analysis of Balancing
                  Domain Decomposition Methods},
  volume =	 20,
  issn =	 {1134-3060, 1886-1784},
  url =
                  {http://link.springer.com/article/10.1007/s11831-013-9086-4},
  doi =		 {10.1007/s11831-013-9086-4},
  abstract =	 {In this paper we present a detailed description of a
                  high-performance distributed-memory implementation
                  of balancing domain decomposition preconditioning
                  techniques. This coverage provides a pool of
                  implementation hints and considerations that can be
                  very useful for scientists that are willing to
                  tackle large-scale distributed-memory machines using
                  these methods. On the other hand, the paper includes
                  a comprehensive performance and scalability study of
                  the resulting codes when they are applied for the
                  solution of the Poisson problem on a large-scale
                  multicore-based distributed-memory machine with up
                  to 4096 cores. Well-known theoretical results
                  guarantee the optimality (algorithmic scalability)
                  of these preconditioning techniques for weak scaling
                  scenarios, as they are able to keep the condition
                  number of the preconditioned operator bounded by a
                  constant with fixed load per core and increasing
                  number of cores. The experimental study presented in
                  the paper complements this mathematical analysis and
                  answers how far can these methods go in the number
                  of cores and the scale of the problem to still be
                  within reasonable ranges of efficiency on current
                  distributed-memory machines. Besides, for those
                  scenarios where poor scalability is expected, the
                  study precisely identifies, quantifies and justifies
                  which are the main sources of inefficiency.},
  language =	 {en},
  number =	 3,
  urldate =	 {2014-06-12},
  journal =	 {Archives of Computational Methods in Engineering},
  author =	 {Badia, Santiago and Martín, Alberto F. and Principe,
                  Javier},
  year =	 2013,
  keywords =	 {Appl.Mathematics/Computational Methods of
                  Engineering},
  pages =	 {239--262},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/3M93V426/Badia
                  et al. - 2013 - Implementation and Scalability
                  Analysis of
                  Balanci.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/PSEQ95Q2/10.html:text/html}
}


@article{badia_long-term_2010,
  title =	 {Long-Term Stability Estimates and Existence of a
                  Global Attractor in a Finite Element Approximation
                  of the {Navier}-{Stokes} Equations with Numerical
                  Subgrid Scale Modeling},
  volume =	 48,
  issn =	 00361429,
  url =
                  {http://link.aip.org/link/SJNAAM/v48/i3/p1013/s1&Agg=doi},
  doi =		 {10.1137/090766681},
  number =	 3,
  urldate =	 {2012-02-10},
  journal =	 {SIAM Journal on Numerical Analysis},
  author =	 {Badia, Santiago and Codina, Ramon and
                  Guti\'errez-Santacreu, Juan Vicente},
  year =	 2010,
  pages =	 1013
}

@article{badia_modular_2008,
  title =	 {Modular vs. non-modular preconditioners for
                  fluid-structure systems with large added-mass
                  effect},
  volume =	 197,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782508001758},
  doi =		 {10.1016/j.cma.2008.04.018},
  abstract =	 {In this article we address the numerical simulation
                  of fluid–structure interaction (FSI) problems
                  featuring large added-mass effect. We analyze
                  different preconditioners for the coupled system
                  matrix obtained after space–time discretization and
                  linearization of the FSI problem. The classical
                  Dirichlet–Neumann preconditioner has the advantage
                  of “modularity” because it allows to reuse existing
                  fluid and structure codes with minimum effort
                  (simple interface communication). Unfortunately, its
                  performance is very poor in case of large added-mass
                  effects. Alternatively, we consider two non-modular
                  approaches. The first one consists in
                  preconditioning the coupled system with a suitable
                  diagonal scaling combined with an ILUT
                  preconditioner. The system is then solved by a
                  Krylov method. The drawback of this procedure is
                  that the combination of fluid and structure codes to
                  solve the coupled system is not straightforward. The
                  second non-modular approach we consider is a
                  splitting technique based on an inexact block-LU
                  factorization of the linear FSI system. The
                  resulting algorithm computes the fluid velocity
                  separately from the coupled pressure–structure
                  system at each iteration, reducing the computational
                  cost. Independently of the preconditioner, the
                  efficiency of semi-implicit algorithms (i.e., those
                  that treat geometric and fluid nonlinearities in an
                  explicit way) is highlighted and their performance
                  compared to the one of implicit algorithms. All the
                  methods are tested on three-dimensional blood-vessel
                  systems. The algorithm combining the non-modular
                  ILUT preconditioner with Krylov methods proved to be
                  the fastest.},
  number =	 {49-50},
  urldate =	 {2012-02-10},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Badia, Santiago and Quaini, Annalisa and Quarteroni,
                  Alfio},
  year =	 2008,
  keywords =	 {Added-mass effect, Fluid–structure interaction,
                  Hemodynamics, Partitioned procedures,
                  preconditioners, Semi-implicit coupling},
  pages =	 {4216--4232}
}

@article{badia_monotonicity-preserving_2014,
  title =	 {On Monotonicity-Preserving Stabilized Finite Element
                  Approximations of Transport Problems},
  volume =	 36,
  issn =	 {1064-8275, 1095-7197},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/130927206},
  doi =		 {10.1137/130927206},
  language =	 {en},
  number =	 6,
  urldate =	 {2015-05-07},
  journal =	 {SIAM Journal on Scientific Computing},
  author =	 {Badia, Santiago and Hierro, Alba},
  year =	 2014,
  pages =	 {A2673--A2697},
  file =
                  {130927206.pdf:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/K24INI6F/130927206.pdf:application/pdf}
}

@article{badia_monotonicity-preserving_2017,
	title = {Monotonicity-preserving finite element schemes based on differentiable nonlinear stabilization},
	volume = {313},
	issn = {0045-7825},
	url = {http://www.sciencedirect.com/science/article/pii/S0045782516306405},
	doi = {10.1016/j.cma.2016.09.035},
	abstract = {In this work, we propose a nonlinear stabilization technique for scalar conservation laws with implicit time stepping. The method relies on an artificial diffusion method, based on a graph-Laplacian operator. It is nonlinear, since it depends on a shock detector. Further, the resulting method is linearity preserving. The same shock detector is used to gradually lump the mass matrix. The resulting method is {LED}, positivity preserving, and also satisfies a global {DMP}. Lipschitz continuity has also been proved. However, the resulting scheme is highly nonlinear, leading to very poor nonlinear convergence rates. We propose a smooth version of the scheme, which leads to twice differentiable nonlinear stabilization schemes. It allows one to straightforwardly use Newton’s method and obtain quadratic convergence. In the numerical experiments, steady and transient linear transport, and transient Burgers’ equation have been considered in 2D. Using the Newton method with a smooth version of the scheme we can reduce 10 to 20 times the number of iterations of Anderson acceleration with the original non-smooth scheme. In any case, these properties are only true for the converged solution, but not for iterates. In this sense, we have also proposed the concept of projected nonlinear solvers, where a projection step is performed at the end of every nonlinear iterations onto a {FE} space of admissible solutions. The space of admissible solutions is the one that satisfies the desired monotonic properties (maximum principle or positivity).},
	pages = {133--158},
        journal =  {Computer Methods in Applied Mechanics and Engineering},
	journaltitle = {Computer Methods in Applied Mechanics and Engineering},
	shortjournal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Badia, Santiago and Bonilla, Jesús},
        year   = {2017},
	urldate = {2017-01-10},
	date = {2017-01-01},
	keywords = {Discrete maximum principle, Finite elements, Monotonicity, Nonlinear solvers, shock capturing},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/PR9TGUSJ/Badia and Bonilla - 2017 - Monotonicity-preserving finite element schemes bas.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/WF2AVS2Z/S0045782516306405.html:text/html}
}

@article{badia_multiscale_2009,
  title =	 {On a multiscale approach to the transient {Stokes}
                  problem: {Dynamic} subscales and anisotropic
                  space-time discretization},
  volume =	 207,
  issn =	 {0096-3003},
  shorttitle =	 {On a multiscale approach to the transient {Stokes}
                  problem},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0096300308008291},
  doi =		 {10.1016/j.amc.2008.10.059},
  abstract =	 {In this article, we analyze some residual-based
                  stabilization techniques for the transient Stokes
                  problem when considering anisotropic time–space
                  discretizations. We define an anisotropic time–space
                  discretization as a family of time–space partitions
                  that does not satisfy the condition h 2 ⩽ C δ t with
                  C uniform with respect to h and δt. Standard
                  residual-based stabilization techniques are
                  motivated by a multiscale approach, approximating
                  the effect of the subscales onto the large
                  scales. One of the approximations is to consider the
                  subscales quasi-static (neglecting their time
                  derivative). It is well known that these techniques
                  are unstable for anisotropic time–space
                  discretizations. We show that the use of dynamic
                  subscales (where the subscales time derivatives are
                  not neglected) solves the problem, and prove optimal
                  convergence and stability results that are valid for
                  anisotropic time–space discretizations. Also the
                  improvements related to the use of orthogonal
                  subscales are addressed.},
  number =	 2,
  urldate =	 {2012-02-10},
  journal =	 {Applied Mathematics and Computation},
  author =	 {Badia, Santiago and Codina, Ramon},
  year =	 2009,
  keywords =	 {Convergence, Dynamic subscales, Multiscale,
                  Stability, Stabilized finite elements, Stokes},
  pages =	 {415--433}
}

@article{badia_nodal-based_2012,
  title =	 {A Nodal-based Finite Element Approximation of the
                  {Maxwell} Problem Suitable for Singular Solutions},
  volume =	 50,
  copyright =	 {© 2012 Society for Industrial and Applied
                  Mathematics},
  url =		 {http://link.aip.org/link/?SNA/50/398/1},
  doi =		 {10.1137/110835360},
  abstract =	 {A new mixed finite element approximation of
                  Maxwell's problem is proposed, its main features
                  being that it is based on a novel augmented
                  formulation of the continuous problem and the
                  introduction of a mesh dependent stabilizing term,
                  which yields a very weak control on the divergence
                  of the unknown. The method is shown to be stable and
                  convergent in the natural \$H(\{{\textbackslash}rm
                  {\textbackslash}mathbf\{curl\}\}{\textbackslash}, 0;
                  {\textbackslash}Omega)\$ norm for this unknown. In
                  particular, convergence also applies to singular
                  solutions, for which classical nodal-based
                  interpolations are known to suffer from spurious
                  convergence upon mesh refinement.},
  number =	 2,
  urldate =	 {2012-03-26},
  journal =	 {SIAM Journal on Numerical Analysis},
  author =	 {Badia, Santiago and Codina, Ramon},
  year =	 2012,
  keywords =	 {Finite elements, Maxwell equations, nodal elements,
                  Singular solutions, Stabilization techniques},
  pages =	 {398--417}
}

@article{badia_overview_2011,
  title =	 {An Overview on Numerical Analyses of Nematic Liquid
                  Crystal Flows},
  volume =	 18,
  issn =	 {1134-3060},
  url =
                  {http://www.springerlink.com/content/j62j70320w502076/abstract/},
  doi =		 {10.1007/s11831-011-9061-x},
  abstract =	 {The purpose of this work is to provide an overview
                  of the most recent numerical developments in the
                  field of nematic liquid crystals. The
                  Ericksen-Leslie equations govern the motion of a
                  nematic liquid crystal. This system, in its simplest
                  form, consists of the Navier-Stokes equations
                  coupled with an extra anisotropic stress tensor,
                  which represents the effect of the nematic liquid
                  crystal on the fluid, and a convective harmonic map
                  equation. The sphere constraint must be enforced
                  almost everywhere in order to obtain an energy
                  estimate. Since an almost everywhere satisfaction of
                  this restriction is not appropriate at a numerical
                  level, two alternative approaches have been
                  introduced: a penalty method and a saddle-point
                  method. These approaches are suitable for their
                  numerical approximation by finite elements, since a
                  discrete version of the restriction is enough to
                  prove the desired energy estimate. The
                  Ginzburg-Landau penalty function is usually used to
                  enforce the sphere constraint. Finite element
                  methods of mixed type will play an important role
                  when designing numerical approximations for the
                  penalty method in order to preserve the intrinsic
                  energy estimate. The inf-sup condition that makes
                  the saddle-point method well-posed is not clear
                  yet. The only inf-sup condition for the Lagrange
                  multiplier is obtained in the dual space of H 1
                  (Ω). But such an inf-sup condition requires more
                  regularity for the director vector than the one
                  provided by the energy estimate. Herein, we will
                  present an alternative inf-sup condition whose proof
                  for its discrete counterpart with finite elements is
                  still open.},
  number =	 3,
  urldate =	 {2012-04-18},
  journal =	 {Archives of Computational Methods in Engineering},
  author =	 {Badia, S. and Guill\'en-Gonz\'alez, F. and
                  Guti\'errez-Santacreu, J.},
  year =	 2011,
  keywords =	 {Engineering},
  pages =	 {285--313}
}

@article{badia_pressure_2008,
  title =	 {Pressure segregation methods based on a discrete
                  pressure {Poisson} equation. {An} algebraic
                  approach},
  volume =	 56,
  issn =	 {1097-0363},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/fld.1532/abstract},
  doi =		 {10.1002/fld.1532},
  abstract =	 {In this paper, we introduce some pressure
                  segregation methods obtained from a non-standard
                  version of the discrete monolithic system, where the
                  continuity equation has been replaced by a pressure
                  Poisson equation obtained at the discrete level. In
                  these methods it is the velocity instead of the
                  pressure the extrapolated unknown. Moreover,
                  predictor–corrector schemes are suggested, again
                  motivated by the new monolithic system. Key
                  implementation aspects are discussed, and a complete
                  stability analysis is performed. We end with a set
                  of numerical examples in order to compare these
                  methods with classical pressure-correction
                  schemes. Copyright © 2007 John Wiley \& Sons, Ltd.},
  language =	 {en},
  number =	 4,
  urldate =	 {2012-02-14},
  journal =	 {International Journal for Numerical Methods in
                  Fluids},
  author =	 {Badia, Santiago and Codina, Ramon},
  year =	 2008,
  keywords =	 {incompressible Navier–Stokes equations,
                  predictor–corrector, pressure segregation, velocity
                  correction},
  pages =	 {351--382}
}

@article{badia_robinrobin_2009,
  title =	 {Robin-{Robin} preconditioned {Krylov} methods for
                  fluid-structure interaction problems},
  volume =	 198,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782509001546},
  doi =		 {10.1016/j.cma.2009.04.004},
  abstract =	 {In this work, we propose a Robin–Robin
                  preconditioner combined with Krylov iterations for
                  the solution of the interface system arising in
                  fluid–structure interaction (FSI) problems. It can
                  be seen as a partitioned FSI procedure and in this
                  respect it generalizes the ideas introduced in
                  [S. Badia, F. Nobile, C. Vergara,
                  J. Comput. Phys. 227 (2008) 7027–7051]. We analyze
                  the convergence of GMRES iterations with the
                  Robin–Robin preconditioner on a model problem and
                  compare its efficiency with some existing
                  algorithms. The method is shown to be very efficient
                  for many challenging fluid–structure interaction
                  problems, such as those characterized by a large
                  added-mass effect or by enclosed fluids. In
                  particular, the possibility to solve balloon-type
                  problems without any special treatment makes this
                  algorithm very appealing compared to the
                  computationally intensive existing approaches.},
  number =	 {33-36},
  urldate =	 {2012-02-10},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Badia, Santiago and Nobile, Fabio and Vergara,
                  Christian},
  year =	 2009,
  keywords =	 {Added-mass effect, Domain decomposition
                  preconditioners, Enclosed fluid problems,
                  Fluid–structure interaction, Hemodynamics,
                  Partitioned procedures, Robin boundary conditions},
  pages =	 {2768--2784}
}

@article{badia_scalability_2015,
  title =	 {On the scalability of inexact balancing domain
                  decomposition by constraints with overlapped
                  coarse/fine corrections},
  volume =	 50,
  issn =	 {0167-8191},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0167819115001180},
  doi =		 {10.1016/j.parco.2015.09.004},
  abstract =	 {In this work, we analyze the scalability of inexact
                  two-level balancing domain decomposition by
                  constraints (BDDC) preconditioners for Krylov
                  subspace iterative solvers, when using a highly
                  scalable asynchronous parallel implementation where
                  fine and coarse correction computations are
                  overlapped in time. This way, the coarse-grid
                  problem can be fully overlapped by fine-grid
                  computations (which are embarrassingly parallel) in
                  a wide range of cases. Further, we consider inexact
                  solvers to reduce the computational cost/complexity
                  and memory consumption of coarse and local problems
                  and boost the scalability of the solver. Out of our
                  numerical experimentation, we conclude that the BDDC
                  preconditioner is quite insensitive to inexact
                  solvers. In particular, one cycle of algebraic
                  multigrid (AMG) is enough to attain algorithmic
                  scalability. Further, the clear reduction of
                  computing time and memory requirements of inexact
                  solvers compared to sparse direct ones makes
                  possible to scale far beyond state-of-the-art BDDC
                  implementations. Excellent weak scalability results
                  have been obtained with the proposed
                  inexact/overlapped implementation of the two-level
                  BDDC preconditioner, up to 93,312 cores and 20
                  billion unknowns on JUQUEEN. Further, we have also
                  applied the proposed setting to unstructured meshes
                  and partitions for the pressure Poisson solver in
                  the backward-facing step benchmark domain.},
  urldate =	 {2016-01-14},
  journal =	 {Parallel Computing},
  author =	 {Badia, Santiago and Mart\'in, Alberto F. and
                  Principe, Javier},
  year =	 2015,
  keywords =	 {BDDC, domain decomposition, Inexact solvers,
                  Overlapping, parallelization, scalability},
  pages =	 {1--24},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/QHVSCRI7/Badia
                  et al. - 2015 - On the scalability of inexact
                  balancing domain
                  dec.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/8VCGW7DJ/S0167819115001180.html:text/html}
}

@article{badia_splitting_2008,
  title =	 {Splitting Methods Based on Algebraic Factorization
                  for Fluid-Structure Interaction},
  volume =	 30,
  issn =	 10648275,
  url =
                  {http://link.aip.org/link/SJOCE3/v30/i4/p1778/s1&Agg=doi},
  doi =		 {10.1137/070680497},
  number =	 4,
  urldate =	 {2012-02-10},
  journal =	 {SIAM Journal on Scientific Computing},
  author =	 {Badia, Santiago and Quaini, Annalisa and Quarteroni,
                  Alfio},
  year =	 2008,
  pages =	 1778
}

@article{badia_stability_2014,
  title =	 {Stability, Convergence, and Accuracy of Stabilized
                  Finite Element Methods for the Wave Equation in
                  Mixed Form},
  volume =	 52,
  issn =	 {0036-1429},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/130918708},
  doi =		 {10.1137/130918708},
  abstract =	 {In this paper we propose two stabilized finite
                  element methods for different functional frameworks
                  of the wave equation in mixed form. These stabilized
                  finite element methods are stable for any pair of
                  interpolation spaces of the unknowns. The
                  variational forms corresponding to different
                  functional settings are treated in a unified manner
                  through the introduction of length scales related to
                  the unknowns. Stability and convergence analysis is
                  performed together with numerical experiments. It is
                  shown that modifying the length scales allows one to
                  mimic at the discrete level the different functional
                  settings of the continuous problem and influence the
                  stability and accuracy of the resulting methods., In
                  this paper we propose two stabilized finite element
                  methods for different functional frameworks of the
                  wave equation in mixed form. These stabilized finite
                  element methods are stable for any pair of
                  interpolation spaces of the unknowns. The
                  variational forms corresponding to different
                  functional settings are treated in a unified manner
                  through the introduction of length scales related to
                  the unknowns. Stability and convergence analysis is
                  performed together with numerical experiments. It is
                  shown that modifying the length scales allows one to
                  mimic at the discrete level the different functional
                  settings of the continuous problem and influence the
                  stability and accuracy of the resulting methods.},
  number =	 4,
  urldate =	 {2015-05-07},
  journal =	 {SIAM Journal on Numerical Analysis},
  author =	 {Badia, S. and Codina, R. and Espinoza, H.},
  year =	 2014,
  pages =	 {1729--1752},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/FT73MWG5/Badia
                  et al. - 2014 - Stability, Convergence, and Accuracy
                  of
                  Stabilized.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/CEAFK56U/130918708.html:text/html}
}

@article{badia_stabilized_2010,
  title =	 {Stabilized continuous and discontinuous {Galerkin}
                  techniques for {Darcy} flow},
  volume =	 199,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782510000241},
  doi =		 {10.1016/j.cma.2010.01.015},
  abstract =	 {We design stabilized methods based on the
                  variational multiscale decomposition of Darcy's
                  problem. A model for the subscales is designed by
                  using a heuristic Fourier analysis. This model
                  involves a characteristic length scale, that can go
                  from the element size to the diameter of the domain,
                  leading to stabilized methods with different
                  stability and convergence properties. These
                  stabilized methods mimic different possible
                  functional settings of the continuous problem. The
                  optimal method depends on the velocity and pressure
                  approximation order. They also involve a subgrid
                  projector that can be either the identity (when
                  applied to finite element residuals) or can have an
                  image orthogonal to the finite element space. In
                  particular, we have designed a new stabilized method
                  that allows the use of piecewise constant
                  pressures. We consider a general setting in which
                  velocity and pressure can be approximated by either
                  continuous or discontinuous approximations. All
                  these methods have been analyzed, proving stability
                  and convergence results. In some cases, duality
                  arguments have been used to obtain error bounds in
                  the L2-norm.},
  number =	 {25-28},
  urldate =	 {2015-01-12},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Badia, Santiago and Codina, Ramon},
  year =	 2010,
  keywords =	 {Characteristic length scale, Darcy's problem,
                  Orthogonal subgrid scales, Stabilized finite element
                  methods},
  pages =	 {1654--1667},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/RW6MIAVP/Badia
                  and Codina - 2010 - Stabilized continuous and
                  discontinuous Galerkin
                  t.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/ZTT3HI25/S0045782510000241.html:text/html}
}

@article{badia_stabilized_2012,
  title =	 {On stabilized finite element methods based on the
                  {Scott}-{Zhang} projector. {Circumventing} the
                  inf-sup condition for the {Stokes} problem},
  volume =	 {247-248},
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782512002435},
  doi =		 {10.1016/j.cma.2012.07.020},
  abstract =	 {In this work we propose a stabilized finite element
                  method that permits us to circumvent discrete
                  inf–sup conditions, e.g. allowing equal order
                  interpolation. The type of method we propose belongs
                  to the family of symmetric stabilization techniques,
                  which are based on the introduction of additional
                  terms that penalize the difference between some
                  quantities, i.e. the pressure gradient in the Stokes
                  problem, and their finite element projections. The
                  key feature of the formulation we propose is the
                  definition of the projection to be used, a
                  non-standard Scott–Zhang projector that is
                  well-defined for L 1 ( Ω ) functions. The resulting
                  method has some appealing features: the projector is
                  local and nested meshes or enriched spaces are not
                  required.},
  number =	 0,
  urldate =	 {2012-09-20},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Badia, Santiago},
  year =	 2012,
  keywords =	 {Indefinite systems, Stabilized finite elements,
                  Stokes problem},
  pages =	 {65--72}
}

@article{badia_stokes_2012,
  title =	 {Stokes, {Maxwell} and {Darcy}: {A} single finite
                  element approximation for three model problems},
  volume =	 62,
  issn =	 {0168-9274},
  shorttitle =	 {Stokes, {Maxwell} and {Darcy}},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0168927411001097},
  doi =		 {10.1016/j.apnum.2011.07.001},
  abstract =	 {In this work we propose stabilized finite element
                  methods for Stokesʼ, Maxwellʼs and Darcyʼs problems
                  that accommodate any interpolation of velocities and
                  pressures. We briefly review the formulations we
                  have proposed for these three problems independently
                  in a unified manner, stressing the advantages of our
                  approach. In particular, for Darcyʼs problem we are
                  able to design stabilized methods that yield optimal
                  convergence both for the primal and the dual
                  problems. In the case of Maxwellʼs problem, the
                  formulation we propose allows one to use continuous
                  finite element interpolations that converge
                  optimally to the continuous solution even if it is
                  non-smooth. Once the formulation is presented for
                  the three model problems independently, we also show
                  how it can be used for a problem that combines all
                  the operators of the independent problems. Stability
                  and convergence is achieved regardless of the fact
                  that any of these operators dominates the others, a
                  feature not possible for the methods of which we are
                  aware.},
  number =	 4,
  urldate =	 {2012-02-27},
  journal =	 {Applied Numerical Mathematics},
  author =	 {Badia, Santiago and Codina, Ramon},
  year =	 2012,
  keywords =	 {Compatible approximations, Nodal interpolations,
                  Primal and dual problems, Singular solutions,
                  Stabilized finite elements},
  pages =	 {246--263}
}

@article{badia_unconditionally_2013,
  title =	 {On an unconditionally convergent stabilized finite
                  element approximation of resistive
                  magnetohydrodynamics},
  volume =	 234,
  issn =	 {0021-9991},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0021999112005761},
  doi =		 {10.1016/j.jcp.2012.09.031},
  abstract =	 {In this work, we propose a new stabilized finite
                  element formulation for the approximation of the
                  resistive magnetohydrodynamics equations. The
                  novelty of this formulation is the fact that it
                  always converges to the physical solution, even for
                  singular ones. A detailed set of numerical
                  experiments have been performed in order to validate
                  our approach.},
  number =	 0,
  urldate =	 {2012-11-26},
  journal =	 {Journal of Computational Physics},
  author =	 {Badia, Santiago and Codina, Ramon and Planas, Ramon},
  year =	 2013,
  keywords =	 {Finite elements, Magnetohydrodynamics, Singular
                  solutions, Stabilized finite element methods},
  pages =	 {399--416}
}

@article{badia_unconditionally_2013-1,
  title =	 {Unconditionally stable operator splitting algorithms
                  for the incompressible magnetohydrodynamics system
                  discretized by a stabilized finite element
                  formulation based on projections},
  volume =	 93,
  copyright =	 {Copyright © 2012 John Wiley \& Sons, Ltd.},
  issn =	 {1097-0207},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nme.4392/abstract},
  doi =		 {10.1002/nme.4392},
  abstract =	 {In this article, we propose different splitting
                  procedures for the transient incompressible
                  magnetohydrodynamics (MHD) system that are
                  unconditionally stable. We consider two levels of
                  splitting, on one side we perform the segregation of
                  the fluid pressure and magnetic pseudo-pressure from
                  the vectorial fields computation. At the second
                  level, the fluid velocity and induction fields are
                  also decoupled. This way, we transform a fully
                  coupled indefinite multi-physics system into a set
                  of smaller definite ones, clearly reducing the CPU
                  cost. With regard to the finite element
                  approximation, we stick to an unconditionally
                  convergent stabilized finite element formulation
                  because it introduces convection stabilization,
                  allows to circumvent inf-sup conditions (clearly
                  simplifying implementation issues), and is able to
                  capture non-smooth solutions of the magnetic
                  subproblem. However, residual-based finite element
                  formulations are not suitable for segregation,
                  because they lose the skew-symmetry of the
                  off-diagonal blocks. Therefore, in this work, we
                  have proposed a novel term-by-term stabilization of
                  the MHD system based on projections that is still
                  unconditionally convergent. Copyright © 2012 John
                  Wiley \& Sons, Ltd.},
  language =	 {en},
  number =	 3,
  urldate =	 {2013-07-09},
  journal =	 {International Journal for Numerical Methods in
                  Engineering},
  author =	 {Badia, Santiago and Planas, Ramon and
                  Guti\'errez-Santacreu, Juan Vicente},
  year =	 2013,
  keywords =	 {Fractional step methods, incompressible
                  magnetohydrodynamics, operator splitting algorithms,
                  Stabilized finite element methods, symmetric
                  projection stabilization},
  pages =	 {302--328},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/4SDG2S4X/Badia
                  et al. - 2013 - Unconditionally stable operator
                  splitting
                  algorith.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/2HDD4UIE/abstract.html:text/html}
}

@article{badia_unified_2009,
  title =	 {Unified Stabilized Finite Element Formulations for
                  the {Stokes} and the {Darcy} Problems},
  volume =	 47,
  issn =	 00361429,
  url =
                  {http://link.aip.org/link/SJNAAM/v47/i3/p1971/s1&Agg=doi},
  doi =		 {10.1137/08072632X},
  number =	 3,
  urldate =	 {2012-02-10},
  journal =	 {SIAM Journal on Numerical Analysis},
  author =	 {Badia, Santiago and Codina, Ramon},
  year =	 2009,
  pages =	 {1971--2000}
}

@INPROCEEDINGS{baker_challenges_2011,
  author =	 {Baker, Allison H. and Gamblin, Todd and Schulz,
                  Martin and Yang, Ulrike Meier},
  title =	 {Challenges of Scaling Algebraic Multigrid Across
                  Modern Multicore Architectures},
  booktitle =	 {Proceedings of the 2011 {IEEE} International
                  Parallel \& Distributed Processing Symposium},
  year =	 2011,
  series =	 {{IPDPS} '11},
  pages =	 {275--286},
  address =	 {Washington, {DC}, {USA}},
  abstract =	 {Algebraic multigrid ({AMG)} is a popular solver for
                  large-scale scientific computing and an essential
                  component of many simulation codes. {AMG} has shown
                  to be extremely efficient on distributed-memory
                  architectures.  However, when executed on modern
                  multicore architectures, we face new challenges that
                  can significantly deteriorate {AMG's} performance.
                  We examine its performance and scalability on three
                  disparate multicore architectures: a cluster with
                  four {AMD} Opteron Quad-core processors per node
                  (Hera), a Cray {XT5} with two {AMD} Opteron Hex-core
                  processors per node (Jaguar), and an {IBM} Blue
                  {Gene/P} system with a single Quad-core processor
                  (Intrepid). We discuss our experiences on these
                  platforms and present results using both an
                  {MPI-only} and a hybrid {MPI/OpenMP} model. We also
                  discuss a set of techniques that helped to overcome
                  the associated problems, including thread and
                  process pinning and correct memory associations.},
  doi =		 {10.1109/IPDPS.2011.35},
  isbn =	 {978-0-7695-4385-7},
  owner =	 {principe},
  timestamp =	 {2014.03.21},
  url =		 {http://dx.doi.org/10.1109/IPDPS.2011.35},
  urldate =	 {2014-02-25}
}

@article{bangerth_algorithms_2012,
	title = {Algorithms and data structures for massively parallel generic adaptive finite element codes},
	volume = {38},
	issn = {0098-3500},
	url = {http://doi.acm.org/10.1145/2049673.2049678},
	doi = {10.1145/2049673.2049678},
	abstract = {Today's largest supercomputers have 100,000s of processor cores and offer the potential to solve partial differential equations discretized by billions of unknowns. However, the complexity of scaling to such large machines and problem sizes has so far prevented the emergence of generic software libraries that support such computations, although these would lower the threshold of entry and enable many more applications to benefit from large-scale computing. We are concerned with providing this functionality for mesh-adaptive finite element computations. We assume the existence of an “oracle” that implements the generation and modification of an adaptive mesh distributed across many processors, and that responds to queries about its structure. Based on querying the oracle, we develop scalable algorithms and data structures for generic finite element methods. Specifically, we consider the parallel distribution of mesh data, global enumeration of degrees of freedom, constraints, and postprocessing. Our algorithms remove the bottlenecks that typically limit large-scale adaptive finite element analyses. We demonstrate scalability of complete finite element workflows on up to 16,384 processors. An implementation of the proposed algorithms, based on the open source software p4est as mesh oracle, is provided under an open source license through the widely used deal.{II} finite element software library.},
	pages = {14:1--14:28},
	number = {2},
        journal = {{ACM} Trans. Math. Softw.},
	journaltitle = {{ACM} Trans. Math. Softw.},
	author = {Bangerth, Wolfgang and Burstedde, Carsten and Heister, Timo and Kronbichler, Martin},
        year = {2012},
	urldate = {2013-09-30},
	date = {2012-01},
	keywords = {Adaptive mesh refinement, object-orientation, Parallel algorithms, software design},
	file = {2010-distributed.pdf:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/Z43CDWVX/2010-distributed.pdf:application/pdf;ACM Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/FWGVC29V/Bangerth et al. - 2012 - Algorithms and data structures for massively paral.pdf:application/pdf}
}

@article{bangerth_deal.ii&mdash;general-purpose_2007,
  title =	 {{deal.II--A} general-purpose object-oriented finite
                  element library},
  volume =	 33,
  issn =	 {0098-3500},
  url =		 {http://doi.acm.org/10.1145/1268776.1268779},
  doi =		 {10.1145/1268776.1268779},
  abstract =	 {An overview of the software design and data
                  abstraction decisions chosen for {deal.II}, a
                  general purpose finite element library written in
                  C++, is given. The library uses advanced
                  object-oriented and data encapsulation techniques to
                  break finite element implementations into smaller
                  blocks that can be arranged to fit users
                  requirements. Through this approach, {deal.II}
                  supports a large number of different applications
                  covering a wide range of scientific areas,
                  programming methodologies, and application-specific
                  algorithms, without imposing a rigid framework into
                  which they have to fit. A judicious use of
                  programming techniques allows us to avoid the
                  computational costs frequently associated with
                  abstract object-oriented class libraries. The paper
                  presents a detailed description of the abstractions
                  chosen for defining geometric information of meshes
                  and the handling of degrees of freedom associated
                  with finite element spaces, as well as of linear
                  algebra, input/output capabilities and of interfaces
                  to other software, such as visualization
                  tools. Finally, some results obtained with
                  applications built atop {deal.II} are shown to
                  demonstrate the powerful capabilities of this
                  toolbox.},
  number =	 4,
  urldate =	 {2013-10-30},
  journal =	 {{ACM} Trans. Math. Softw.},
  author =	 {Bangerth, W. and Hartmann, R. and Kanschat, G.},
  year =	 2007,
  keywords =	 {object-orientation, software design},
  file =	 {ACM Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/TG5QZTPP/Bangerth
                  et al. - 2007 - deal.II&mdash\;A general-purpose
                  object-oriented fi.pdf:application/pdf}
}

@article{bank_new_2000,
  title =	 {A New Paradigm for Parallel Adaptive Meshing
                  Algorithms},
  volume =	 22,
  issn =	 {1064-8275, 1095-7197},
  url =
                  {http://epubs.siam.org/doi/abs/10.1137/S1064827599353701},
  doi =		 {10.1137/S1064827599353701},
  number =	 4,
  urldate =	 {2013-09-26},
  journal =	 {{SIAM} Journal on Scientific Computing},
  author =	 {Bank, Randolph E. and Holst, Michael},
  year =	 2000,
  pages =	 {1411--1443},
  file =	 {A New Paradigm for Parallel Adaptive Meshing
                  Algorithms \: SIAM Journal on Scientific Computing\:
                  Vol. 22, No. 4 (Society for Industrial and Applied
                  Mathematics):/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/4S3UM94V/S1064827599353701.html:text/html;s1064827599353701.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/BDJNKWUS/s1064827599353701.pdf:application/pdf}
}

@article{bank_new_2000,
  title =	 {A New Paradigm for Parallel Adaptive Meshing
                  Algorithms},
  volume =	 22,
  issn =	 {1064-8275, 1095-7197},
  url =
                  {http://epubs.siam.org/doi/abs/10.1137/S1064827599353701},
  doi =		 {10.1137/S1064827599353701},
  number =	 4,
  urldate =	 {2013-09-26},
  journal =	 {{SIAM} Journal on Scientific Computing},
  author =	 {Bank, Randolph E. and Holst, Michael},
  year =	 2000,
  pages =	 {1411--1443},
  file =	 {A New Paradigm for Parallel Adaptive Meshing
                  Algorithms \: SIAM Journal on Scientific Computing\:
                  Vol. 22, No. 4 (Society for Industrial and Applied
                  Mathematics):/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/4S3UM94V/S1064827599353701.html:text/html;s1064827599353701.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/BDJNKWUS/s1064827599353701.pdf:application/pdf}
}

@article{becker_optimal_2001,
  title =	 {An optimal control approach to a posteriori error
                  estimation in finite element methods},
  volume =	 10,
  issn =	 {1474-0508},
  url =
                  {http://journals.cambridge.org/article_S0962492901000010},
  doi =		 {10.1017/S0962492901000010},
  abstract =	 {This article surveys a general approach to error
                  control and adaptive mesh design in Galerkin finite
                  element methods that is based on duality principles
                  as used in optimal control. Most of the existing
                  work on a posteriori error analysis deals with error
                  estimation in global norms like the ‘energy norm’ or
                  the L2 norm, involving usually unknown ‘stability
                  constants’. However, in most applications, the error
                  in a global norm does not provide useful bounds for
                  the errors in the quantities of real physical
                  interest. Further, their sensitivity to local error
                  sources is not properly represented by global
                  stability constants. These deficiencies are overcome
                  by employing duality techniques, as is common in a
                  priori error analysis of finite element methods, and
                  replacing the global stability constants by
                  computationally obtained local sensitivity
                  factors. Combining this with Galerkin orthogonality,
                  a posteriori estimates can be derived directly for
                  the error in the target quantity. In these estimates
                  local residuals of the computed solution are
                  multiplied by weights which measure the dependence
                  of the error on the local residuals. Those, in turn,
                  can be controlled by locally refining or coarsening
                  the computational mesh. The weights are obtained by
                  approximately solving a linear adjoint problem. The
                  resulting a posteriori error estimates provide the
                  basis of a feedback process for successively
                  constructing economical meshes and corresponding
                  error bounds tailored to the particular goal of the
                  computation. This approach, called the
                  ‘dual-weighted-residual method’, is introduced
                  initially within an abstract functional analytic
                  setting, and is then developed in detail for several
                  model situations featuring the characteristic
                  properties of elliptic, parabolic and hyperbolic
                  problems. After having discussed the basic
                  properties of duality-based adaptivity, we
                  demonstrate the potential of this approach by
                  presenting a selection of results obtained for
                  practical test cases. These include problems from
                  viscous fluid flow, chemically reactive flow,
                  elasto-plasticity, radiative transfer, and optimal
                  control. Throughout the paper, open theoretical and
                  practical problems are stated together with
                  references to the relevant literature.},
  urldate =	 {2016-01-15},
  journal =	 {Acta Numerica},
  author =	 {Becker, Roland and Rannacher, Rolf},
  year =	 2001,
  pages =	 {1--102},
  file =	 {Cambridge Journals
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/DSVRISTS/displayAbstract.html:text/html}
}

@article{berumen_quality_2010,
  series =	 {Laser {Assisted} {Net} {Shape} {Engineering} 6,
                  {Proceedings} of the {LANE} 2010, {Part} 2},
  title =	 {Quality control of laser- and powder bed-based
                  {Additive} {Manufacturing} ({AM}) technologies},
  volume =	 {5, Part B},
  issn =	 {1875-3892},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S1875389210005158},
  doi =		 {10.1016/j.phpro.2010.08.089},
  abstract =	 {The quality of metal components manufactured by
                  laser- and powder bed-based additive manufacturing
                  technologies has continuously been improved over the
                  last years. However, to establish this production
                  technology in industries with very high quality
                  standards the accessibility of prevalent quality
                  management methods to all steps of the process chain
                  needs still to be enhanced. This publication
                  describes which tools are and will be available to
                  fulfil those requirements from the perspective of a
                  laser machine manufacturer. Generally five aspects
                  of the part building process are covered by separate
                  Quality Management (QM) modules: the powder quality,
                  the temperature management, the process gas
                  atmosphere, the melt pool behaviour and the
                  documentation module. This paper sets the focus on
                  melt pool analysis and control.},
  urldate =	 {2015-12-29},
  journal =	 {Physics Procedia},
  author =	 {Berumen, Sebastian and Bechmann, Florian and
                  Lindner, Stefan and Kruth, Jean-Pierre and Craeghs,
                  Tom},
  year =	 2010,
  keywords =	 {additive manufacturing, Laser melting, Melt pool,
                  Quality control, Realtime},
  pages =	 {617--622},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/K6N5HDHJ/Berumen
                  et al. - 2010 - Quality control of laser- and powder
                  bed-based Add.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/D2SUFZSQ/S1875389210005158.html:text/html}
}

@article{bran:77,
  abstract =	 {{The boundary-value problem is discretized on
                  several grids (or finite-element spaces) of widely
                  different mesh sizes. Interactions between these
                  levels enable us (i) to solve the possibly nonlinear
                  system of \$n\$ discrete equations in \$O(n)\$
                  operations (\$40n\$ additions and shifts for Poisson
                  problems); (ii) to conveniently adapt the
                  discretization (the local mesh size, local order of
                  approximation, etc.) to the evolving solution in a
                  nearly optimal way, obtaining "\$∞\$-order"
                  approximations and low \$n\$, even when
                  singularities are present. General theoretical
                  analysis of the numerical process. Numerical
                  experiments with linear and nonlinear, elliptic and
                  mixed-type (transonic flow) problems--confirm
                  theoretical predictions. Similar techniques for
                  initial-value problems are briefly discussed.}},
  author =	 {Brandt, Achi},
  citeulike-article-id =3744561,
  citeulike-linkout-0 ={http://dx.doi.org/10.2307/2006422},
  citeulike-linkout-1 ={http://www.jstor.org/stable/2006422},
  doi =		 {10.2307/2006422},
  issn =	 00255718,
  journal =	 {Mathematics of Computation},
  keywords =	 {amg, math, numerical, precond},
  number =	 138,
  pages =	 {333--390},
  posted-at =	 {2008-12-04 08:16:37},
  priority =	 3,
  publisher =	 {American Mathematical Society},
  title =	 {{Multi-Level Adaptive Solutions to Boundary-Value
                  Problems}},
  url =		 {http://dx.doi.org/10.2307/2006422},
  volume =	 31,
  year =	 1977
}

@article{bran:77,
  abstract =	 {{The boundary-value problem is discretized on
                  several grids (or finite-element spaces) of widely
                  different mesh sizes. Interactions between these
                  levels enable us (i) to solve the possibly nonlinear
                  system of \$n\$ discrete equations in \$O(n)\$
                  operations (\$40n\$ additions and shifts for Poisson
                  problems); (ii) to conveniently adapt the
                  discretization (the local mesh size, local order of
                  approximation, etc.) to the evolving solution in a
                  nearly optimal way, obtaining "\$∞\$-order"
                  approximations and low \$n\$, even when
                  singularities are present. General theoretical
                  analysis of the numerical process. Numerical
                  experiments with linear and nonlinear, elliptic and
                  mixed-type (transonic flow) problems--confirm
                  theoretical predictions. Similar techniques for
                  initial-value problems are briefly discussed.}},
  author =	 {Brandt, Achi},
  citeulike-article-id =3744561,
  citeulike-linkout-0 ={http://dx.doi.org/10.2307/2006422},
  citeulike-linkout-1 ={http://www.jstor.org/stable/2006422},
  doi =		 {10.2307/2006422},
  issn =	 00255718,
  journal =	 {Mathematics of Computation},
  keywords =	 {amg, math, numerical, precond},
  number =	 138,
  pages =	 {333--390},
  posted-at =	 {2008-12-04 08:16:37},
  priority =	 3,
  publisher =	 {American Mathematical Society},
  title =	 {{Multi-Level Adaptive Solutions to Boundary-Value
                  Problems}},
  url =		 {http://dx.doi.org/10.2307/2006422},
  volume =	 31,
  year =	 1977
}

@book{branner_modellierung_2011,
  address =	 {München},
  series =	 {Forschungsberichte {IWB}},
  title =	 {Modellierung transienter {Effekte} in der
                  {Struktursimulation} von {Schichtbauverfahren}},
  isbn =	 {978-3-8316-4071-3},
  number =	 {Band 246},
  publisher =	 {Herbert Utz Verlag},
  author =	 {Branner, Gregor},
  year =	 2011,
  keywords =	 {Powder metallurgy, Rapid tooling, Sintering},
  file =
                  {2012-53-Krol.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/S8NZJEGW/2012-53-Krol.pdf:application/pdf}
}

@incollection{brezina_smoothed_2011,
  series =	 {Lecture {Notes} in {Computer} {Science}},
  title =	 {Smoothed {Aggregation} {Spectral} {Element}
                  {Agglomeration} {AMG}: {SA}-$\rho${AMGe}},
  copyright =	 {©2012 Springer-Verlag Berlin Heidelberg},
  isbn =	 {978-3-642-29842-4 978-3-642-29843-1},
  shorttitle =	 {Smoothed {Aggregation} {Spectral} {Element}
                  {Agglomeration} {AMG}},
  url =
                  {http://link.springer.com/chapter/10.1007/978-3-642-29843-1_1},
  abstract =	 {A two-level smoothed aggregation (or SA) scheme with
                  tentative coarse space constructed by spectral
                  element agglomeration method is shown to provide
                  weak-approximation property in a weighted L
                  2-norm. The resulting method utilizing efficient
                  (e.g., polynomial) smoothers is shown to have
                  convergence factor independent of both the coarse
                  and fine-grid mesh-sizes, as well as, to be
                  independent of the contrast (i.e., possible large
                  jumps in the PDE coefficient) for second order
                  elliptic problems discretized on general
                  unstructured meshes. The method allows for
                  multilevel extensions. Presented numerical
                  experiments exhibit behavior in agreement with the
                  developed theory.},
  language =	 {en},
  number =	 7116,
  urldate =	 {2016-01-14},
  booktitle =	 {Large-{Scale} {Scientific} {Computing}},
  publisher =	 {Springer},
  author =	 {Brezina, Marian and Vassilevski, Panayot S.},
  editor =	 {Lirkov, Ivan and Margenov, Svetozar and
                  Wa\'sniewski, Jerzy},
  year =	 2011,
  keywords =	 {Algorithm Analysis and Problem Complexity,
                  Computation by Abstract Devices, Computer-Aided
                  Engineering (CAD, CAE) and Design, Numeric
                  Computing, Programming Techniques, Simulation and
                  Modeling},
  pages =	 {3--15},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/6AGNQZZ2/Brezina
                  and Vassilevski - 2011 - Smoothed Aggregation
                  Spectral Element
                  Agglomeratio.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/6J7UBQ9R/10.html:text/html}
}

@article{brezzi_discontinuous_2004,
  title =	 {Discontinuous {Galerkin} methods for first-order
                  hyperbolic problems},
  volume =	 14,
  doi =		 {10.1142/S0218202504003866},
  number =	 12,
  journal =	 {Mathematical Models and Methods in Applied Sciences
                  (M3AS)},
  author =	 {Brezzi, F. and Marini, L. D. and Suli, E.},
  year =	 2004,
  pages =	 {1893--1903},
  file =	 {Full Text
                  PDF:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/Q3ICMNTV/Brezzi
                  et al. - 2004 - Discontinuous galerkin methods for
                  first-order
                  hyp.pdf:application/pdf;Snapshot:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/V4W8V6M7/S0218202504003866.html:text/html}
}

@techreport{brommel_juqueen_2015,
  title =	 {{JUQUEEN} {Extreme} {Scaling} {Workshop} 2015},
  url =		 {http://juser.fz-juelich.de/record/188191},
  abstract =	 {In conjunction with this year's JUQUEEN Porting and
                  Tuning Workshop, which is part of the PRACE Advanced
                  Training Centres curriculum, JSC continued its
                  series of BlueGene Extreme Scaling Workshops. Seven
                  application teams were invited to stay for two days
                  and work on the scalability of their codes, with
                  dedicated access to the entire JUQUEEN system for a
                  period of 30 hours. Most of the teams' codes had
                  thematic overlap with JSC Simulation Laboratories or
                  were part of an ongoing collaboration with one of
                  the SimLabs. The code teams came from the fields of
                  climate science (ICON from DKRZ, and MPAS-A from KIT
                  and NCAR), engineering (FEMPAR from UPC, and
                  ex\_nl/FE{\textasciicircum}2 from Uni Cologne and TU
                  Freiberg), fluid dynamics (psOpen and SHOCK both
                  from RWTH Aachen), and neuroscience (CoreNeuron from
                  the EPFL Blue Brain Project) and were supported by
                  JSC SimLabs and Cross-sectional teams, with IBM and
                  JUQUEEN technical support. Within the first 24 hours
                  of dedicated access to the entire 28 racks, all
                  seven teams had adapted their codes and datasets to
                  exploit the massive parallelism and restricted node
                  memory for successful executions using all 458,752
                  cores. Most of them also demonstrated excellent
                  strong or weak scalability, qualifying all but one
                  for the High-Q Club. A total of 370 'large' jobs
                  were executed using 12 of the 15 million core-hours
                  of compute time allocated for the workshop. Detailed
                  results for each code, provided by the application
                  teams themselves, is introduced by analysis
                  comparing them to the other 16 High-Q Club
                  codes. Brömmel, Dirk; Frings, Wolfgang; Wylie, Brian
                  J. N.},
  number =	 {FZJ-2015-01645},
  urldate =	 {2016-01-14},
  institution =	 {J\"ulich Supercomputing Center},
  author =	 {Br\"ommel, Dirk and Wylie, Brian J. N. and Frings,
                  Wolfgang},
  year =	 2015,
  file =
                  {Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/DRF7TK39/188191.html:text/html}
}

@article{brown_validation_2012,
  title =	 {Validation of a model for static and dynamic
                  recrystallization in metals},
  volume =	 {32-33},
  issn =	 {0749-6419},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0749641911001963},
  doi =		 {10.1016/j.ijplas.2011.12.006},
  abstract =	 {In this paper, modifications are proposed to a
                  phenomenological plasticity model to account for the
                  evolution of recrystallization and the resultant
                  softening behavior. The novel model includes
                  internal state variables representing dislocation
                  density and the spacing between geometrically
                  necessary subgrain boundaries. In order to capture
                  both single and multiple peak recrystallization, the
                  model tracks the evolution of recrystallized volume
                  fractions for multiple cycles of recrystallization,
                  and has a set of state variables for each volume
                  fraction. A rule of mixtures is used to determine
                  the average stress. The model is capable of
                  capturing static recrystallization as well as both
                  single and multiple peak dynamic recrystallization.
                  Material parameters are fit to data from monotonic
                  compression tests on copper for a wide range of
                  temperatures and strain rates. The model is then
                  validated by using the same parameter set to predict
                  multiple-stage response in which samples are
                  compressed, held at temperature for various lengths
                  of time, and then compressed further. The model
                  predicts both the static recrystallization that
                  occurs between loading stages as well as the dynamic
                  recrystallization occurring during the second
                  loading stage.},
  urldate =	 {2016-01-15},
  journal =	 {International Journal of Plasticity},
  author =	 {Brown, Arthur A. and Bammann, Douglas J.},
  year =	 2012,
  keywords =	 {A. Dislocations, A. Microstructures, B. Constitutive
                  behavior, B. Elastic-viscoplastic material,
                  Misorientation - nominated},
  pages =	 {17--35},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/8MUFZE3N/Brown
                  and Bammann - 2012 - Validation of a model for
                  static and dynamic
                  recry.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/E5SVBSE3/S0749641911001963.html:text/html}
}

@article{brune_composing_2015,
	title = {Composing Scalable Nonlinear Algebraic Solvers},
	volume = {57},
	issn = {0036-1445},
	url = {http://epubs.siam.org.recursos.biblioteca.upc.edu/doi/abs/10.1137/130936725},
	doi = {10.1137/130936725},
	abstract = {Most efficient linear solvers use composable algorithmic components, with the most common model being the combination of a Krylov accelerator and one or more preconditioners. A similar set of concepts may be used for nonlinear algebraic systems, where nonlinear composition of different nonlinear solvers may significantly improve the time to solution. We describe the basic concepts of nonlinear o̧mposition and preconditioning and present a number of solvers applicable to nonlinear partial differential equations. We have developed a software framework in order to easily explore the possible combinations of solvers.  We show that the performance gains from using composed solvers can be substantial compared with gains from standard Newton--Krylov methods.},
	pages = {535--565},
	number = {4},
        journal = {{SIAM} Review},
	journaltitle = {{SIAM} Review},
	shortjournal = {{SIAM} Rev.},
	author = {Brune, P. and Knepley, M. and Smith, B. and Tu, X.},
        year = {2015},
	urldate = {2017-01-11},
	date = {2015-01-01},
	file = {Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/V9ICT3JH/Brune et al. - 2015 - Composing Scalable Nonlinear Algebraic Solvers.pdf:application/pdf;Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/7RB8EJC4/130936725.html:text/html}
}

@article{burstedde_p4est_2011,
  title =	 {{\tt p4est}: Scalable Algorithms for Parallel
                  Adaptive Mesh Refinement on Forests of Octrees},
  volume =	 33,
  issn =	 {1064-8275, 1095-7197},
  shorttitle =	 {p4est},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/100791634},
  doi =		 {10.1137/100791634},
  number =	 3,
  urldate =	 {2013-09-27},
  journal =	 {{SIAM} Journal on Scientific Computing},
  author =	 {Burstedde, Carsten and Wilcox, Lucas C. and Ghattas,
                  Omar},
  year =	 2011,
  pages =	 {1103--1133},
  file =
                  {100791634.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/SHIVQ75P/100791634.pdf:application/pdf;p4est\:
                  Scalable Algorithms for Parallel Adaptive Mesh
                  Refinement on Forests of Octrees \: SIAM Journal on
                  Scientific Computing\: Vol. 33, No. 3 (Society for
                  Industrial and Applied
                  Mathematics):/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/VHCC7787/100791634.html:text/html}
}

@article{buttari_mixed_2007,
  title =	 {Mixed Precision Iterative Refinement Techniques for
                  the Solution of Dense Linear Systems},
  volume =	 21,
  issn =	 {1094-3420},
  url =		 {http://dx.doi.org/10.1177/1094342007084026},
  doi =		 {10.1177/1094342007084026},
  abstract =	 {By using a combination of 32-bit and 64-bit floating
                  point arithmetic, the performance of many dense and
                  sparse linear algebra algorithms can be
                  significantly enhanced while maintaining the 64-bit
                  accuracy of the resulting solution. The approach
                  presented here can apply not only to conventional
                  processors but also to exotic technologies such as
                  Field Programmable Gate Arrays ({FPGA)}, Graphical
                  Processing Units ({GPU)}, and the Cell {BE}
                  processor. Results on modern processor architectures
                  and the Cell {BE} are presented.},
  number =	 4,
  urldate =	 {2014-05-15},
  journal =	 {Int. J. High Perform. Comput. Appl.},
  author =	 {Buttari, Alfredo and Dongarra, Jack and Langou,
                  Julie and Langou, Julien and Luszczek, Piotr and
                  Kurzak, Jakub},
  year =	 2007,
  keywords =	 {Cholesky factorization, iterative, {LU},
                  mixed-precision, refinement},
  pages =	 {457--466}
}

@article{buttari_mixed_2007,
  title =	 {Mixed Precision Iterative Refinement Techniques for
                  the Solution of Dense Linear Systems},
  volume =	 21,
  issn =	 {1094-3420},
  url =		 {http://dx.doi.org/10.1177/1094342007084026},
  doi =		 {10.1177/1094342007084026},
  abstract =	 {By using a combination of 32-bit and 64-bit floating
                  point arithmetic, the performance of many dense and
                  sparse linear algebra algorithms can be
                  significantly enhanced while maintaining the 64-bit
                  accuracy of the resulting solution. The approach
                  presented here can apply not only to conventional
                  processors but also to exotic technologies such as
                  Field Programmable Gate Arrays ({FPGA)}, Graphical
                  Processing Units ({GPU)}, and the Cell {BE}
                  processor. Results on modern processor architectures
                  and the Cell {BE} are presented.},
  number =	 4,
  urldate =	 {2014-05-15},
  journal =	 {Int. J. High Perform. Comput. Appl.},
  author =	 {Buttari, Alfredo and Dongarra, Jack and Langou,
                  Julie and Langou, Julien and Luszczek, Piotr and
                  Kurzak, Jakub},
  year =	 2007,
  keywords =	 {Cholesky factorization, iterative, {LU},
                  mixed-precision, refinement},
  pages =	 {457--466}
}

@article{buttari_using_2008,
  title =	 {Using Mixed Precision for Sparse Matrix Computations
                  to Enhance the Performance While Achieving 64-bit
                  Accuracy},
  volume =	 34,
  issn =	 {0098-3500},
  url =		 {http://doi.acm.org/10.1145/1377596.1377597},
  doi =		 {10.1145/1377596.1377597},
  abstract =	 {By using a combination of 32-bit and 64-bit floating
                  point arithmetic, the performance of many sparse
                  linear algebra algorithms can be significantly
                  enhanced while maintaining the 64-bit accuracy of
                  the resulting solution. These ideas can be applied
                  to sparse multifrontal and supernodal direct
                  techniques and sparse iterative techniques such as
                  Krylov subspace methods. The approach presented here
                  can apply not only to conventional processors but
                  also to exotic technologies such as Field
                  Programmable Gate Arrays ({FPGA)}, Graphical
                  Processing Units ({GPU)}, and the Cell {BE}
                  processor.},
  number =	 4,
  urldate =	 {2014-05-15},
  journal =	 {{ACM} Trans. Math. Softw.},
  author =	 {Buttari, Alfredo and Dongarra, Jack and Kurzak,
                  Jakub and Luszczek, Piotr and Tomov, Stanimir},
  year =	 2008,
  keywords =	 {floating point, iterative refinement, Linear
                  systems, precision},
  pages =	 {17:1--17:22},
  file =	 {ACM Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/T4HHFQ3T/Buttari
                  et al. - 2008 - Using Mixed Precision for Sparse
                  Matrix Computatio.pdf:application/pdf}
}

@article{buttari_using_2008,
  title =	 {Using Mixed Precision for Sparse Matrix Computations
                  to Enhance the Performance While Achieving 64-bit
                  Accuracy},
  volume =	 34,
  issn =	 {0098-3500},
  url =		 {http://doi.acm.org/10.1145/1377596.1377597},
  doi =		 {10.1145/1377596.1377597},
  abstract =	 {By using a combination of 32-bit and 64-bit floating
                  point arithmetic, the performance of many sparse
                  linear algebra algorithms can be significantly
                  enhanced while maintaining the 64-bit accuracy of
                  the resulting solution. These ideas can be applied
                  to sparse multifrontal and supernodal direct
                  techniques and sparse iterative techniques such as
                  Krylov subspace methods. The approach presented here
                  can apply not only to conventional processors but
                  also to exotic technologies such as Field
                  Programmable Gate Arrays ({FPGA)}, Graphical
                  Processing Units ({GPU)}, and the Cell {BE}
                  processor.},
  number =	 4,
  urldate =	 {2014-05-15},
  journal =	 {{ACM} Trans. Math. Softw.},
  author =	 {Buttari, Alfredo and Dongarra, Jack and Kurzak,
                  Jakub and Luszczek, Piotr and Tomov, Stanimir},
  year =	 2008,
  keywords =	 {floating point, iterative refinement, Linear
                  systems, precision},
  pages =	 {17:1--17:22},
  file =	 {ACM Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/T4HHFQ3T/Buttari
                  et al. - 2008 - Using Mixed Precision for Sparse
                  Matrix Computatio.pdf:application/pdf}
}

@article{cai_nonlinearly_2002,
  title =	 {Nonlinearly Preconditioned Inexact Newton
                  Algorithms},
  volume =	 24,
  issn =	 {1064-8275},
  url =
                  {http://epubs.siam.org/doi/abs/10.1137/S106482750037620X},
  doi =		 {10.1137/S106482750037620X},
  abstract =	 {Inexact Newton algorithms are commonly used for
                  solving large sparse nonlinear system of equations
                  {\$F(u{\textasciicircum}\{{\textbackslash}ast\})=0\$}
                  arising, for example, from the discretization of
                  partial differential equations. Even with global
                  strategies such as linesearch or trust region, the
                  methods often stagnate at local minima of
                  \${\textbackslash}{\textbar}F{\textbackslash}{\textbar}\$,
                  especially for problems with unbalanced
                  nonlinearities, because the methods do not have
                  built-in machinery to deal with the unbalanced
                  nonlinearities. To find the same solution
                  \$u{\textasciicircum}\{{\textbackslash}ast\}\$, one
                  may want to solve instead an equivalent nonlinearly
                  preconditioned system \$\{{\textbackslash}cal
                  F\}(u{\textasciicircum}\{{\textbackslash}ast\})=0\$
                  whose nonlinearities are more balanced. In this
                  paper, we propose and study a nonlinear additive
                  Schwarz-based parallel nonlinear preconditioner and
                  show numerically that the new method converges well
                  even for some difficult problems, such as high
                  Reynolds number flows, where a traditional inexact
                  Newton method fails., Inexact Newton algorithms are
                  commonly used for solving large sparse nonlinear
                  system of equations
                  {\$F(u{\textasciicircum}\{{\textbackslash}ast\})=0\$}
                  arising, for example, from the discretization of
                  partial differential equations. Even with global
                  strategies such as linesearch or trust region, the
                  methods often stagnate at local minima of
                  \${\textbackslash}{\textbar}F{\textbackslash}{\textbar}\$,
                  especially for problems with unbalanced
                  nonlinearities, because the methods do not have
                  built-in machinery to deal with the unbalanced
                  nonlinearities. To find the same solution
                  \$u{\textasciicircum}\{{\textbackslash}ast\}\$, one
                  may want to solve instead an equivalent nonlinearly
                  preconditioned system \$\{{\textbackslash}cal
                  F\}(u{\textasciicircum}\{{\textbackslash}ast\})=0\$
                  whose nonlinearities are more balanced. In this
                  paper, we propose and study a nonlinear additive
                  Schwarz-based parallel nonlinear preconditioner and
                  show numerically that the new method converges well
                  even for some difficult problems, such as high
                  Reynolds number flows, where a traditional inexact
                  Newton method fails.},
  number =	 1,
  urldate =	 {2014-05-15},
  journal =	 {{SIAM} Journal on Scientific Computing},
  author =	 {Cai, X. and Keyes, D.},
  year =	 2002,
  pages =	 {183--200},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/S73GK9CH/Cai
                  and Keyes - 2002 - Nonlinearly Preconditioned
                  Inexact Newton
                  Algorith.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/8IEINSCR/S106482750037620X.html:text/html}
}

@article{canuto_bddc_2013,
  title =	 {{BDDC} preconditioners for continuous and
                  discontinuous {G}alerkin methods using spectral/hp
                  elements with variable local polynomial degree},
  issn =	 {0272-4979, 1464-3642},
  url =
                  {http://imajna.oxfordjournals.org/content/early/2013/10/03/imanum.drt037},
  doi =		 {10.1093/imanum/drt037},
  abstract =	 {Locally adapted meshes and polynomial degrees can
                  greatly improve spectral element accuracy and
                  applicability. A balancing domain decomposition by
                  constraints ({BDDC)} preconditioner is constructed
                  and analysed for both continuous ({CG)} and
                  discontinuous ({DG)} Galerkin discretizations of
                  scalar elliptic problems, built by nodal spectral
                  elements with variable polynomial degrees. The {DG}
                  case is reduced to the {CG} case via the auxiliary
                  space method. The proposed {BDDC} preconditioner is
                  proved to be scalable in the number of subdomains
                  and quasi-optimal in both the ratio of local
                  polynomial degrees and element sizes and the ratio
                  of subdomain and element sizes. Several numerical
                  experiments in the plane confirm the obtained
                  theoretical convergence rate estimates, and
                  illustrate the preconditioner performance for both
                  {CG} and {DG} discretizations. Different
                  configurations with locally adapted polynomial
                  degrees are studied, as well as the preconditioner
                  robustness with respect to discontinuities of the
                  elliptic coefficients across subdomain
                  boundaries. These results apply also to other
                  dual-primal preconditioners defined by the same set
                  of primal constraints, such as {FETI-DP}
                  preconditioners.},
  language =	 {en},
  urldate =	 {2014-05-15},
  journal =	 {{IMA} Journal of Numerical Analysis},
  author =	 {Canuto, Claudio and Pavarino, Luca F. and Pieri,
                  Alexandre B.},
  year =	 2014,
  keywords =	 {auxiliary space method, {BDDC}, {FETI-DP},
                  Discontinuous Galerkin method, dual-primal
                  preconditioner, Elliptic problems},
  volume =	 {In press},
  file =
                  {Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/CMA9ZUJN/imanum.drt037.html:text/html}
}

@article{carlberg_gnat_2013,
  title =	 {The {GNAT} method for nonlinear model reduction:
                  {Effective} implementation and application to
                  computational fluid dynamics and turbulent flows},
  volume =	 242,
  issn =	 {0021-9991},
  shorttitle =	 {The {GNAT} method for nonlinear model reduction},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0021999113001472},
  doi =		 {10.1016/j.jcp.2013.02.028},
  abstract =	 {The Gauss-Newton with approximated tensors (GNAT)
                  method is a nonlinear model-reduction method that
                  operates on fully discretized computational
                  models. It achieves dimension reduction by a
                  Petrov-Galerkin projection associated with residual
                  minimization; it delivers computational efficiency
                  by a hyper-reduction procedure based on the ‘gappy
                  POD’ technique. Originally presented in Ref. [1],
                  where it was applied to implicit nonlinear
                  structural-dynamics models, this method is further
                  developed here and applied to the solution of a
                  benchmark turbulent viscous flow problem. To begin,
                  this paper develops global state-space error bounds
                  that justify the method’s design and highlight its
                  advantages in terms of minimizing components of
                  these error bounds. Next, the paper introduces a
                  ‘sample mesh’ concept that enables a distributed,
                  computationally efficient implementation of the GNAT
                  method in finite-volume-based
                  computational-fluid-dynamics (CFD) codes. The
                  suitability of GNAT for parameterized problems is
                  highlighted with the solution of an academic problem
                  featuring moving discontinuities. Finally, the
                  capability of this method to reduce by orders of
                  magnitude the core-hours required for large-scale
                  CFD computations, while preserving accuracy, is
                  demonstrated with the simulation of turbulent flow
                  over the Ahmed body. For an instance of this
                  benchmark problem with over 17 million degrees of
                  freedom, GNAT outperforms several other nonlinear
                  model-reduction methods, reduces the required
                  computational resources by more than two orders of
                  magnitude, and delivers a solution that differs by
                  less than 1\% from its high-dimensional
                  counterpart.},
  urldate =	 {2015-12-31},
  journal =	 {Journal of Computational Physics},
  author =	 {Carlberg, Kevin and Farhat, Charbel and Cortial,
                  Julien and Amsallem, David},
  year =	 2013,
  keywords =	 {CFD, Gappy POD, GNAT, Mesh sampling, Nonlinear model
                  reduction, Petrov-Galerkin},
  pages =	 {623--647},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/AJRZIVTZ/Carlberg
                  et al. - 2013 - The GNAT method for nonlinear model
                  reduction Eff.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/HTEUST3E/S0021999113001472.html:text/html}
}

@article{carlberg_krylov-subspace_2015,
  title =	 {Krylov-subspace recycling via the {POD}-augmented
                  conjugate-gradient algorithm},
  url =		 {http://arxiv.org/abs/1512.05820},
  abstract =	 {This work presents a new Krylov-subspace-recycling
                  method for efficiently solving sequences of linear
                  systems of equations characterized by a
                  non-invariant symmetric-positive-definite matrix. As
                  opposed to typical truncation strategies used in
                  recycling such as deflation, we propose a truncation
                  method inspired by goal-oriented proper orthogonal
                  decomposition (POD) from model reduction. This idea
                  is based on the observation that model reduction
                  aims to compute a low-dimensional subspace that
                  contains an {\textbackslash}textit\{accurate\}
                  solution; as such, we expect the proposed method to
                  generate a low-dimensional subspace that is well
                  suited for computing
                  {\textbackslash}textit\{inexact\} solutions. In
                  particular, we propose specific goal-oriented POD
                  `ingredients' that align the optimality properties
                  of POD with the objective of Krylov-subspace
                  recycling. To compute solutions in the resulting
                  `augmented' POD subspace, we propose a hybrid
                  direct/iterative three-stage iterative method that
                  leverages (1) the optimal ordering of POD basis
                  vectors, and (2) well-conditioned reduced
                  matrices. Numerical experiments performed on
                  real-world solid-mechanics problems highlight the
                  benefits of the proposed method over standard
                  approaches for Krylov-subspace recycling.},
  urldate =	 {2016-01-05},
  journal =	 {arXiv:1512.05820 [math]},
  author =	 {Carlberg, Kevin and Forstall, Virginia and Tuminaro,
                  Ray},
  year =	 2015,
  note =	 {arXiv: 1512.05820},
  keywords =	 {Mathematics - Numerical Analysis},
  file =	 {arXiv\:1512.05820
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/XQEFIBWA/Carlberg
                  et al. - 2015 - Krylov-subspace recycling via the
                  POD-augmented co.pdf:application/pdf;arXiv.org
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/C52CAM64/1512.html:text/html}
}

@article{cermak_tfeti_2014,
  title =	 {A {TFETI} domain decomposition solver for
                  elastoplastic problems},
  volume =	 231,
  issn =	 {0096-3003},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0096300314000253},
  doi =		 {10.1016/j.amc.2013.12.186},
  abstract =	 {We propose an algorithm for the efficient parallel
                  implementation of elastoplastic problems with
                  hardening based on the so-called TFETI (Total Finite
                  Element Tearing and Interconnecting) domain
                  decomposition method. We consider an associated
                  elastoplastic model with the von Mises plastic
                  criterion and the linear isotropic hardening
                  law. Such a model is discretized by the implicit
                  Euler method in time and the consequent one time
                  step elastoplastic problem by the finite element
                  method in space. The latter results in a system of
                  nonlinear equations with a strongly semismooth and
                  strongly monotone operator. The semismooth Newton
                  method is applied to solve this nonlinear
                  system. Corresponding linearized problems arising in
                  the Newton iterations are solved in parallel by the
                  above mentioned TFETI domain decomposition
                  method. The proposed TFETI based algorithm was
                  implemented in Matlab parallel environment and its
                  performance was illustrated on a 3D elastoplastic
                  benchmark. Numerical results for different time
                  discretizations and mesh levels are presented and
                  discussed and a local quadratic convergence of the
                  semismooth Newton method is observed.},
  urldate =	 {2016-01-14},
  journal =	 {Applied Mathematics and Computation},
  author =	 {\v{C}erm\'ak, M. and Kozubek, T. and Sysala, S. and
                  Valdman, J.},
  year =	 2014,
  keywords =	 {Elastoplasticity, finite element method, Semismooth
                  Newton method, Total FETI domain decomposition
                  method},
  pages =	 {634--653},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/DFSNCSHW/Čermák
                  et al. - 2014 - A TFETI domain decomposition solver
                  for elastoplas.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/CX4JB9UH/S0096300314000253.html:text/html}
}

@ARTICLE{chen_ibm_2012,
  author =	 {Chen, Dong and Eisley, {N.A.} and Heidelberger,
                  P. and Senger, {R.M.}  and Sugawara, Y. and Kumar,
                  S. and Salapura, V. and Satterfield, D. and
                  Steinmacher-Burow, B. and Parker, J.},
  title =	 {The {IBM} Blue {Gene/Q} Interconnection Fabric},
  journal =	 {{IEEE} Micro},
  year =	 2012,
  volume =	 32,
  pages =	 {32--43},
  number =	 1,
  abstract =	 {This article describes the {IBM} Blue {Gene/Q}
                  interconnection network and message unit. Blue
                  {Gene/Q} is the third generation in the {IBM} Blue
                  Gene line of massively parallel supercomputers and
                  can be scaled to 20 petaflops and beyond. For better
                  application scalability and performance, Blue
                  {Gene/Q} has new routing algorithms and techniques
                  to parallelize the injection and reception of
                  packets in the network interface.},
  doi =		 {10.1109/MM.2011.96},
  file =	 {IEEE Xplore Abstract
                  Record:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/ZU63C6JN/abs_all.html:text/html;IEEE
                  Xplore Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/NNX2N2XE/Chen
                  et al. - 2012 - The IBM Blue GeneQ Interconnection
                  Fabric.pdf:application/pdf},
  issn =	 {0272-1732},
  keywords =	 {application scalability, {BG/Q}, Blue {Gene/Q},
                  Decision support systems, {IBM} Blue {Gene/Q}
                  interconnection fabric, interconnection network,
                  interconnect technologies, massively parallel
                  supercomputers, message unit, multiprocessor
                  interconnection networks, network interface, network
                  interface architecture, parallel computer
                  architecture, parallel machines, router
                  architecture, routing algorithms, routing algorithms
                  and techniques},
  owner =	 {principe},
  timestamp =	 {2014.03.21}
}

@ARTICLE{chen_ibm_2012,
  author =	 {Chen, Dong and Eisley, {N.A.} and Heidelberger,
                  P. and Senger, {R.M.}  and Sugawara, Y. and Kumar,
                  S. and Salapura, V. and Satterfield, D. and
                  Steinmacher-Burow, B. and Parker, J.},
  title =	 {The {IBM} Blue {Gene/Q} Interconnection Fabric},
  journal =	 {{IEEE} Micro},
  year =	 2012,
  volume =	 32,
  pages =	 {32--43},
  number =	 1,
  abstract =	 {This article describes the {IBM} Blue {Gene/Q}
                  interconnection network and message unit. Blue
                  {Gene/Q} is the third generation in the {IBM} Blue
                  Gene line of massively parallel supercomputers and
                  can be scaled to 20 petaflops and beyond. For better
                  application scalability and performance, Blue
                  {Gene/Q} has new routing algorithms and techniques
                  to parallelize the injection and reception of
                  packets in the network interface.},
  doi =		 {10.1109/MM.2011.96},
  file =	 {IEEE Xplore Abstract
                  Record:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/ZU63C6JN/abs_all.html:text/html;IEEE
                  Xplore Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/NNX2N2XE/Chen
                  et al. - 2012 - The IBM Blue GeneQ Interconnection
                  Fabric.pdf:application/pdf},
  issn =	 {0272-1732},
  keywords =	 {application scalability, {BG/Q}, Blue {Gene/Q},
                  Decision support systems, {IBM} Blue {Gene/Q}
                  interconnection fabric, interconnection network,
                  interconnect technologies, massively parallel
                  supercomputers, message unit, multiprocessor
                  interconnection networks, network interface, network
                  interface architecture, parallel computer
                  architecture, parallel machines, router
                  architecture, routing algorithms, routing algorithms
                  and techniques},
  owner =	 {principe},
  timestamp =	 {2014.03.21}
}

@article{chen_phase-field_2002,
  title =	 {Phase-{Field} {Models} for {Microstructure}
                  {Evolution}},
  volume =	 32,
  url =
                  {http://dx.doi.org/10.1146/annurev.matsci.32.112001.132041},
  doi =		 {10.1146/annurev.matsci.32.112001.132041},
  abstract =	 {The phase-field method has recently emerged as a
                  powerful computational approach to modeling and
                  predicting mesoscale morphological and
                  microstructure evolution in materials. It describes
                  a microstructure using a set of conserved and
                  nonconserved field variables that are continuous
                  across the interfacial regions. The temporal and
                  spatial evolution of the field variables is governed
                  by the Cahn-Hilliard nonlinear diffusion equation
                  and the Allen-Cahn relaxation equation. With the
                  fundamental thermodynamic and kinetic information as
                  the input, the phase-field method is able to predict
                  the evolution of arbitrary morphologies and complex
                  microstructures without explicitly tracking the
                  positions of interfaces. This paper briefly reviews
                  the recent advances in developing phase-field models
                  for various materials processes including
                  solidification, solid-state structural phase
                  transformations, grain growth and coarsening, domain
                  evolution in thin films, pattern formation on
                  surfaces, dislocation microstructures, crack
                  propagation, and electromigration.},
  number =	 1,
  urldate =	 {2015-12-29},
  journal =	 {Annual Review of Materials Research},
  author =	 {Chen, Long-Qing},
  year =	 2002,
  keywords =	 {computer simulation, grain growth, morphological
                  evolution, phase transformations, solidification},
  pages =	 {113--140},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/X3JEC38Z/Chen
                  - 2002 - Phase-Field Models for Microstructure
                  Evolution.pdf:application/pdf}
}

@article{chevalier_pt-scotch:_2008,
  series =	 {Parallel Matrix Algorithms and Applications},
  title =	 {{PT-Scotch:} A tool for efficient parallel graph
                  ordering},
  volume =	 34,
  issn =	 {0167-8191},
  shorttitle =	 {{PT-Scotch}},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0167819107001342},
  doi =		 {10.1016/j.parco.2007.12.001},
  abstract =	 {The parallel ordering of large graphs is a difficult
                  problem, because on the one hand minimum degree
                  algorithms do not parallelize well, and on the other
                  hand the obtainment of high quality orderings with
                  the nested dissection algorithm requires efficient
                  graph bipartitioning heuristics, the best sequential
                  implementations of which are also hard to
                  parallelize. This paper presents a set of
                  algorithms, implemented in the {PT-Scotch} software
                  package, which allows one to order large graphs in
                  parallel, yielding orderings the quality of which is
                  only slightly worse than the one of state-of-the-art
                  sequential algorithms. Our implementation uses the
                  classical nested dissection approach but relies on
                  several novel features to solve the parallel graph
                  bipartitioning problem. Thanks to these
                  improvements, {PT-Scotch} produces consistently
                  better orderings than {ParMeTiS} on large numbers of
                  processors.},
  number =	 {6--8},
  urldate =	 {2014-05-15},
  journal =	 {Parallel Computing},
  author =	 {Chevalier, C. and Pellegrini, F.},
  year =	 2008,
  keywords =	 {Distributed-memory computer, Multi-threading,
                  Parallel graph ordering, Parallel nested dissection},
  pages =	 {318--331},
  file =	 {ScienceDirect Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/TBBMHGEK/Chevalier
                  y Pellegrini - 2008 - PT-Scotch A tool for efficient
                  parallel graph ord.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/R39CQUC6/S0167819107001342.html:text/html}
}

@article{chevalier_pt-scotch:_2008,
  series =	 {Parallel Matrix Algorithms and Applications},
  title =	 {{PT-Scotch:} A tool for efficient parallel graph
                  ordering},
  volume =	 34,
  issn =	 {0167-8191},
  shorttitle =	 {{PT-Scotch}},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0167819107001342},
  doi =		 {10.1016/j.parco.2007.12.001},
  abstract =	 {The parallel ordering of large graphs is a difficult
                  problem, because on the one hand minimum degree
                  algorithms do not parallelize well, and on the other
                  hand the obtainment of high quality orderings with
                  the nested dissection algorithm requires efficient
                  graph bipartitioning heuristics, the best sequential
                  implementations of which are also hard to
                  parallelize. This paper presents a set of
                  algorithms, implemented in the {PT-Scotch} software
                  package, which allows one to order large graphs in
                  parallel, yielding orderings the quality of which is
                  only slightly worse than the one of state-of-the-art
                  sequential algorithms. Our implementation uses the
                  classical nested dissection approach but relies on
                  several novel features to solve the parallel graph
                  bipartitioning problem. Thanks to these
                  improvements, {PT-Scotch} produces consistently
                  better orderings than {ParMeTiS} on large numbers of
                  processors.},
  number =	 {6--8},
  urldate =	 {2014-05-15},
  journal =	 {Parallel Computing},
  author =	 {Chevalier, C. and Pellegrini, F.},
  year =	 2008,
  keywords =	 {Distributed-memory computer, Multi-threading,
                  Parallel graph ordering, Parallel nested dissection},
  pages =	 {318--331},
  file =	 {ScienceDirect Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/TBBMHGEK/Chevalier
                  y Pellegrini - 2008 - PT-Scotch A tool for efficient
                  parallel graph ord.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/R39CQUC6/S0167819107001342.html:text/html}
}

@article{chiumenti_1,
  title =	 {Numerical simulation and experimental calibration of
                  Additive Manufacturing by blown powder
                  technology. {P}art {I}: thermal analysis},
  author =	 {M. Chiumenti and X. Lin and M. Cervera and W. Lei
                  and Y. Zheng and W. Huang},
  journal = {Rapid Prototyping Journal},
  volume = {In press},
  year = {2017}
}

@article{chiumenti_2,
	title = {Numerical modeling of the electron beam welding and its experimental validation},
	volume = {121},
	issn = {0168-874X},
	url = {http://www.sciencedirect.com/science/article/pii/S0168874X16301378},
	doi = {10.1016/j.finel.2016.07.003},
	urldate = {2017-01-10},
	journal = {Finite Elements in Analysis and Design},
	author = {Chiumenti, M. and Cervera, M. and Dialami, N. and Wu, B. and Jinwei, L. and Agelet de Saracibar, C.},
	year = {2016},
	keywords = {Electron Beam Welding (EBW), Phase-change, Plasticity, Thermo-mechanical},
	pages = {118--133}
}

@article{chiumenti_3,
  title =	 {A simplified Finite Element model to compute distortions in Selective Laser Melting},
  author =	 {M. Chiumenti and T. Varona and I. Setien and M. Cervera and A. Echevarria},
  journal =	 {In preparation},
  year =	 2017
}

@article{chiumenti_numerical_2016,
	title = {Numerical modeling of the electron beam welding and its experimental validation},
	volume = {121},
	issn = {0168-874X},
	url = {http://www.sciencedirect.com/science/article/pii/S0168874X16301378},
	doi = {10.1016/j.finel.2016.07.003},
	abstract = {Electron Beam Welding ({EBW}) is a highly efficient and precise welding method increasingly used within the manufacturing chain and of growing importance in different industrial environments such as the aeronautical and aerospace sectors. This is because, compared to other welding processes, {EBW} induces lower distortions and residual stresses due to the lower and more focused heat input along the welding line.

This work describes the formulation adopted for the numerical simulation of the {EBW} process as well as the experimental work carried out to calibrate and validate it.

The numerical simulation of {EBW} involves the interaction of thermal, mechanical and metallurgical phenomena. For this reason, in this work the numerical framework couples the heat transfer process to the stress analysis to maximize accuracy. An in-house multi-physics {FE} software is used to deal with the numerical simulation. The definition of an ad hoc moving heat source is proposed to simulate the {EB} power surface distribution and the corresponding absorption within the work-piece thickness. Both heat conduction and heat radiation models are considered to dissipate the heat through the boundaries of the component. The material behavior is characterized by an apropos thermo-elasto-viscoplastic constitutive model. Titanium-alloy Ti6A14V is the target material of this work.

From the experimental side, the {EB} welding machine, the vacuum chamber characteristics and the corresponding operative setting are detailed. Finally, the available facilities to record the temperature evolution at different thermo-couple locations as well as to measure both distortions and residual stresses are described. Numerical results are compared with the experimental evidence.},
	pages = {118--133},
        journal      = {Finite Elements in Analysis and Design},
	journaltitle = {Finite Elements in Analysis and Design},
	shortjournal = {Finite Elements in Analysis and Design},
	author = {Chiumenti, M. and Cervera, M. and Dialami, N. and Wu, B. and Jinwei, L. and Agelet de Saracibar, C.},
        year   = {2016},
	urldate = {2017-01-10},
	date = {2016-11-15},
	keywords = {Electron Beam Welding ({EBW}), Phase-change, Plasticity, Thermo-mechanical},
	file = {Chiumenti et al. - 2016 - Numerical modeling of the electron beam welding an.pdf:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/I8RIKX6J/Chiumenti et al. - 2016 - Numerical modeling of the electron beam welding an.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/IKND9SDQ/S0168874X16301378.html:text/html}
}

@article{christlieb_parallel_2010,
  title =	 {Parallel High-Order Integrators},
  volume =	 32,
  url =		 {http://dx.doi.org/10.1137/09075740X},
  abstract =	 {In this work we discuss a class of defect correction
                  methods which is easily adapted to create parallel
                  time integrators for multicore architectures and is
                  ideally suited for developing methods which can be
                  order adaptive in time. The method is based on
                  integral deferred correction (IDC), which was itself
                  motivated by spectral deferred correction by Dutt,
                  Greengard, and Rokhlin [BIT, 40 (2000),
                  pp. 241-266]. The method presented here is a revised
                  formulation of explicit IDC, dubbed revisionist IDC
                  (RIDC), which can achieve pth-order accuracy in
                  “wall-clock time” comparable to a single forward
                  Euler simulation on problems where the time to
                  evaluate the right-hand side of a system of
                  differential equations is greater than latency costs
                  of interprocessor communication, such as in the case
                  of the N-body problem. The key idea is to rewrite
                  the defect correction framework so that, after
                  initial start-up costs, each correction loop can be
                  lagged behind the previous correction loop in a
                  manner that facilitates running the predictor and
                  \$M=p-1\$ correctors in parallel on an interval
                  which has K steps, where \$K{\textbackslash}gg
                  p\$. We prove that given an rth-order Runge-Kutta
                  method in both the prediction and M correction loops
                  of RIDC, then the method is order
                  \$r{\textbackslash}times(M+1)\$. The parallelization
                  in RIDC uses a small number of cores (the number of
                  processors used is limited by the order one wants to
                  achieve). Using a four-core CPU, it is natural to
                  think about fourth-order RIDC built with forward
                  Euler, or eighth-order RIDC constructed with
                  second-order Runge-Kutta. Numerical tests on an
                  N-body simulation show that RIDC methods can be
                  significantly faster than popular Runge-Kutta
                  methods such as the classical fourth-order
                  Runge-Kutta scheme. In a PDE setting, one can
                  imagine coupling RIDC time integrators with parallel
                  spatial evaluators, thereby increasing the
                  parallelization. The ideas behind RIDC extend to
                  implicit and semi-implicit IDC and have high
                  potential in this area.},
  number =	 2,
  journal =	 {SIAM J. Sci. Comput.},
  author =	 {Christlieb, Andrew J. and Macdonald, Colin B. and
                  Ong, Benjamin W.},
  year =	 2010,
  pages =	 {818--835}
}

@incollection{cockburn_development_2000,
  series =	 {Lecture {Notes} in {Computational} {Science} and
                  {Engineering}},
  title =	 {The Development of Discontinuous {Galerkin} Methods},
  copyright =	 {©2000 Springer-Verlag Berlin Heidelberg},
  isbn =	 {978-3-642-64098-8 978-3-642-59721-3},
  url =
                  {http://link.springer.com/chapter/10.1007/978-3-642-59721-3_1},
  abstract =	 {In this paper, we present an overview of the
                  evolution of the discontinuous Galerkin methods
                  since their introduction in 1973 by Reed and Hill,
                  in the framework of neutron transport, until their
                  most recent developments. We show how these methods
                  made their way into the main stream of computational
                  fluid dynamics and how they are quickly finding use
                  in a wide variety of applications. We review the
                  theoretical and algorithmic aspects of these methods
                  as well as their applications to equations including
                  nonlinear conservation laws, the compressible
                  Navier-Stokes equations, and Hamilton-Jacobi-like
                  equations.},
  number =	 11,
  urldate =	 {2013-10-14},
  booktitle =	 {Discontinuous {Galerkin} {Methods}},
  publisher =	 {Springer Berlin Heidelberg},
  author =	 {Cockburn, Bernardo and Karniadakis, George E. and
                  Shu, Chi-Wang},
  editor =	 {Cockburn, Bernardo and Karniadakis, George E. and
                  Shu, Chi-Wang},
  year =	 2000,
  keywords =	 {Computational Intelligence, Computational
                  Mathematics and Numerical Analysis, Math
                  Applications in Computer Science, Mathematical
                  Methods in Physics, Numerical and Computational
                  Physics},
  pages =	 {3--50},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/6TUXWMGU/Cockburn
                  et al. - 2000 - The Development of Discontinuous
                  Galerkin
                  Methods.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/5IJG8DIK/978-3-642-59721-3_1.html:text/html}
}

@article{cockburn_discontinuous_2003,
  title =	 {Discontinuous {Galerkin} methods},
  volume =	 83,
  copyright =	 {Copyright © 2003 WILEY-VCH Verlag GmbH \& Co. KGaA,
                  Weinheim},
  issn =	 {1521-4001},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/zamm.200310088/abstract},
  doi =		 {10.1002/zamm.200310088},
  abstract =	 {This paper is a short essay on discontinuous
                  Galerkin methods intended for a very wide
                  audience. We present the discontinuous Galerkin
                  methods and describe and discuss their main
                  features. Since the methods use completely
                  discontinuous approximations, they produce mass
                  matrices that are block-diagonal. This renders the
                  methods highly parallelizable when applied to
                  hyperbolic problems. Another consequence of the use
                  of discontinuous approximations is that these
                  methods can easily handle irregular meshes with
                  hanging nodes and approximations that have
                  polynomials of different degrees in different
                  elements. They are thus ideal for use with adaptive
                  algorithms. Moreover, the methods are locally
                  conservative (a property highly valued by the
                  computational fluid dynamics community) and, in
                  spite of providing discontinuous approximations,
                  stable, and high-order accurate. Even more, when
                  applied to non-linear hyperbolic problems, the
                  discontinuous Galerkin methods are able to capture
                  highly complex solutions presenting discontinuities
                  with high resolution. In this paper, we concentrate
                  on the exposition of the ideas behind the devising
                  of these methods as well as on the mechanisms that
                  allow them to perform so well in such a variety of
                  problems.},
  number =	 11,
  urldate =	 {2015-10-27},
  journal =	 {ZAMM - Journal of Applied Mathematics and Mechanics
                  / Zeitschrift für Angewandte Mathematik und
                  Mechanik},
  author =	 {Cockburn, B.},
  year =	 2003,
  keywords =	 {Discontinuous Galerkin methods, Finite element
                  methods},
  pages =	 {731--754},
  file =
                  {Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/T8FSW3NU/abstract.html:text/html}
}

@article{cockburn_unified_2009,
  title =	 {Unified Hybridization of Discontinuous {G}alerkin,
                  Mixed, and Continuous Galerkin Methods for Second
                  Order Elliptic Problems},
  volume =	 47,
  issn =	 {0036-1429},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/070706616},
  doi =		 {10.1137/070706616},
  abstract =	 {We introduce a unifying framework for hybridization
                  of finite element methods for second order elliptic
                  problems. The methods fitting in the framework are a
                  general class of mixed-dual finite element methods
                  including hybridized mixed, continuous Galerkin,
                  nonconforming, and a new, wide class of hybridizable
                  discontinuous Galerkin methods. The distinctive
                  feature of the methods in this framework is that the
                  only globally coupled degrees of freedom are those
                  of an approximation of the solution defined only on
                  the boundaries of the elements. Since the associated
                  matrix is sparse, symmetric, and positive definite,
                  these methods can be efficiently
                  implemented. Moreover, the framework allows, in a
                  single implementation, the use of different methods
                  in different elements or subdomains of the
                  computational domain, which are then automatically
                  coupled. Finally, the framework brings about a new
                  point of view, thanks to which it is possible to see
                  how to devise novel methods displaying very
                  localized and simple mortaring techniques, as well
                  as methods permitting an even further reduction of
                  the number of globally coupled degrees of freedom.,
                  We introduce a unifying framework for hybridization
                  of finite element methods for second order elliptic
                  problems. The methods fitting in the framework are a
                  general class of mixed-dual finite element methods
                  including hybridized mixed, continuous Galerkin,
                  nonconforming, and a new, wide class of hybridizable
                  discontinuous Galerkin methods. The distinctive
                  feature of the methods in this framework is that the
                  only globally coupled degrees of freedom are those
                  of an approximation of the solution defined only on
                  the boundaries of the elements. Since the associated
                  matrix is sparse, symmetric, and positive definite,
                  these methods can be efficiently
                  implemented. Moreover, the framework allows, in a
                  single implementation, the use of different methods
                  in different elements or subdomains of the
                  computational domain, which are then automatically
                  coupled. Finally, the framework brings about a new
                  point of view, thanks to which it is possible to see
                  how to devise novel methods displaying very
                  localized and simple mortaring techniques, as well
                  as methods permitting an even further reduction of
                  the number of globally coupled degrees of freedom.},
  number =	 2,
  urldate =	 {2014-05-15},
  journal =	 {{SIAM} Journal on Numerical Analysis},
  author =	 {Cockburn, B. and Gopalakrishnan, J. and Lazarov, R.},
  year =	 2009,
  pages =	 {1319--1365},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/2P3VPDHE/Cockburn
                  et al. - 2009 - Unified Hybridization of
                  Discontinuous Galerkin,
                  M.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/JA4URE8G/070706616.html:text/html}
}

@article{codina_design_2013,
  title =	 {On the design of discontinuous {Galerkin} methods
                  for elliptic problems based on hybrid formulations},
  volume =	 263,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782513001217},
  doi =		 {10.1016/j.cma.2013.05.004},
  abstract =	 {The objective of this paper is to present a new
                  framework for the design of discontinuous Galerkin
                  (dG) methods for elliptic problems. The idea is to
                  start from a hybrid formulation of the problem
                  involving as unknowns the main field in the interior
                  of the element domains and its fluxes and traces on
                  the element boundaries. Rather than working with
                  this three-field formulation, fluxes are modeled
                  using finite difference expressions and then the
                  traces are determined by imposing continuity of
                  fluxes, although other strategies could be
                  devised. This procedure is applied to four elliptic
                  problems, namely, the convection–diffusion equation
                  (in the diffusion dominated regime), the Stokes
                  problem, the Darcy problem and the Maxwell
                  problem. We justify some well known dG methods with
                  some modifications that in fact allow to improve the
                  performance of the original methods, particularly
                  when the physical properties are discontinuous.},
  urldate =	 {2014-06-12},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Codina, Ramon and Badia, Santiago},
  year =	 2013,
  keywords =	 {Discontinuous Galerkin, Elliptic problems, Hybrid
                  formulations, Stabilized finite element methods},
  pages =	 {158--168}
}

@article{codina_pressure_2006,
  title =	 {On some pressure segregation methods of
                  fractional-step type for the finite element
                  approximation of incompressible flow problems},
  volume =	 195,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S004578250500232X},
  doi =		 {10.1016/j.cma.2004.06.048},
  abstract =	 {In this paper we treat several aspects related to
                  time integration methods for the incompressible
                  Navier–Stokes equations that allow to uncouple the
                  calculation of the velocities and the pressure. The
                  first family of schemes consists of classical
                  fractional step methods, of which we discuss several
                  possibilities for the pressure extrapolation and the
                  time integration of first and second order. The
                  second family consists of schemes based on an
                  explicit treatment of the pressure in the momentum
                  equation followed by a Poisson equation for the
                  pressure. It turns out that this “staggered”
                  treatment of the velocity and the pressure is
                  stable. Finally, we present predictor–corrector
                  methods based on the above schemes that aim to
                  converge to the solution of the monolithic time
                  integration method. Apart from presenting these
                  schemes and check its numerical performance, we also
                  present a complete set of stability results for the
                  fractional step methods that are independent of the
                  space stability of the velocity–pressure
                  interpolation, that is, of the classical inf–sup
                  condition.},
  number =	 {23--24},
  urldate =	 {2012-02-10},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Codina, Ramon and Badia, Santiago},
  year =	 2006,
  keywords =	 {Fractional step methods, Incompressible
                  Navier-Stokes equations},
  pages =	 {2900--2918}
}

@article{codina_time_2007,
  title =	 {Time dependent subscales in the stabilized finite
                  element approximation of incompressible flow
                  problems},
  volume =	 196,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782507000035},
  doi =		 {10.1016/j.cma.2007.01.002},
  abstract =	 {In this paper we analyze a stabilized finite element
                  approximation for the incompressible Navier–Stokes
                  equations based on the subgrid-scale concept. The
                  essential point is that we explore the properties of
                  the discrete formulation that results allowing the
                  subgrid-scales to depend on time. This apparently
                  “natural” idea avoids several inconsistencies of
                  previous formulations and also opens the door to
                  generalizations.},
  number =	 {21--24},
  urldate =	 {2012-02-10},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Codina, Ramon and Principe, Javier and Guasch, Oriol
                  and Badia, Santiago},
  year =	 2007,
  keywords =	 {Incompressible flows, Stabilized finite elements,
                  Transient subscales},
  pages =	 {2413--2430}
}

@article{colomes_assessment_2015,
  title =	 {Assessment of variational multiscale models for the
                  large eddy simulation of turbulent incompressible
                  flows},
  volume =	 285,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782514004113},
  doi =		 {10.1016/j.cma.2014.10.041},
  abstract =	 {In this work we study the performance of some
                  variational multiscale models (VMS) in the large
                  eddy simulation (LES) of turbulent flows. We
                  consider VMS models obtained by different subgrid
                  scale approximations which include either static or
                  dynamic subscales, linear or nonlinear multiscale
                  splitting, and different choices of the subscale
                  space. After a brief review of these models, we
                  discuss some implementation aspects particularly
                  relevant to the simulation of turbulent flows,
                  namely the use of a skew symmetric form of the
                  convective term and the computation of projections
                  when orthogonal subscales are used. We analyze the
                  energy conservation (and numerical dissipation) of
                  the alternative VMS formulations, which is
                  numerically evaluated. In the numerical study, we
                  have considered three well known problems: the decay
                  of homogeneous isotropic turbulence, the
                  Taylor–Green vortex problem and the turbulent flow
                  in a channel. We compare the results obtained using
                  different VMS models, paying special attention to
                  the effect of using orthogonal subscale spaces. The
                  VMS results are also compared against classical LES
                  scheme based on filtering and the dynamic
                  Smagorinsky closure. Altogether, our results show
                  the tremendous potential of VMS for the numerical
                  simulation of turbulence. Further, we study the
                  sensitivity of VMS to the algorithmic constants and
                  analyze the behavior in the small time step
                  limit. We have also carried out a computational cost
                  comparison of the different formulations. Out of
                  these experiments, we can state that the numerical
                  results obtained with the different VMS formulations
                  (as far as they converge) are quite
                  similar. However, some choices are prone to
                  instabilities and the results obtained in terms of
                  computational cost are certainly different. The
                  dynamic orthogonal subscales model turns out to be
                  best in terms of efficiency and robustness.},
  urldate =	 {2015-01-12},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Colom\'es, Oriol and Badia, Santiago and Codina,
                  Ramon and Principe, Javier},
  year =	 2015,
  keywords =	 {large eddy simulation, stabilization, Turbulence,
                  Variational multiscale},
  pages =	 {32--63},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/TGD5QNH5/Colomés
                  et al. - 2015 - Assessment of variational multiscale
                  models for th.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/MNKD2FNG/S0045782514004113.html:text/html}
}

@article{colomes_mixed_2016,
	title = {Mixed finite element methods with convection stabilization for the large eddy simulation of incompressible turbulent flows},
	volume = {304},
	issn = {0045-7825},
	url = {http://www.sciencedirect.com/science/article/pii/S0045782516300561},
	doi = {10.1016/j.cma.2016.02.026},
	abstract = {The variational multiscale method thought as an implicit large eddy simulation model for turbulent flows has been shown to be an alternative to the widely used physical-based models. This method is traditionally combined with equal-order velocity–pressure pairs, since it provides pressure stabilization. In this work, we consider a different approach, based on inf–sup stable elements and convection-only stabilization. In order to do so, we consider a symmetric projection stabilization of the convective term using an orthogonal subscale decomposition. The accuracy and efficiency of this method compared with residual-based algebraic subgrid scales and orthogonal subscales methods for equal-order interpolation is assessed in this paper. Moreover, when inf–sup stable elements are used, the grad–div stabilization term has been shown to be essential to guarantee accurate solutions. Hence, a study of the influence of such term in the large eddy simulation of turbulent incompressible flows is also performed. Furthermore, a recursive block preconditioning strategy has been considered for the resolution of the problem with an implicit treatment of the projection terms. Two different benchmark tests have been solved: the Taylor–Green Vortex flow with R e = 1600 , and the Turbulent Channel Flow at R e τ = 395 and R e τ = 590 .},
	pages = {294--318},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	journaltitle = {Computer Methods in Applied Mechanics and Engineering},
	shortjournal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Colomés, Oriol and Badia, Santiago and Principe, Javier},
	urldate = {2016-03-16},
	date = {2016-06-01},
        year={2016},
	keywords = {Block recursive preconditioning, Grad–div stabilization, large eddy simulation, Turbulence, Variational multiscale},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/TQWDFVAM/Colomés et al. - 2016 - Mixed finite element methods with convection stabi.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/M25BHBPD/S0045782516300561.html:text/html}
}

@article{colomes_segregated_2016,
  title =	 {Segregated {Runge}-{Kutta} methods for the
                  incompressible {Navier}-{Stokes} equations},
  volume =	 105,
  issn =	 {1097-0207},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nme.4987/abstract},
  doi =		 {10.1002/nme.4987},
  abstract =	 {In this work, we propose Runge–Kutta time
                  integration schemes for the incompressible
                  Navier–Stokes equations with two salient
                  properties. First, velocity and pressure
                  computations are segregated at the time integration
                  level, without the need to perform additional
                  fractional step techniques that spoil high orders of
                  accuracy. Second, the proposed methods keep the same
                  order of accuracy for both velocities and
                  pressures. The segregated Runge–Kutta methods are
                  motivated as an implicit–explicit Runge–Kutta time
                  integration of the projected Navier–Stokes system
                  onto the discrete divergence-free space, and its
                  re-statement in a velocity–pressure setting using a
                  discrete pressure Poisson equation. We have analysed
                  the preservation of the discrete divergence
                  constraint for segregated Runge–Kutta methods and
                  their relation (in their fully explicit version)
                  with existing half-explicit methods. We have
                  performed a detailed numerical experimentation for a
                  wide set of schemes (from first to third order),
                  including implicit and IMEX integration of viscous
                  and convective terms, for incompressible laminar and
                  turbulent flows. Further, segregated Runge–Kutta
                  schemes with adaptive time stepping are
                  proposed. Copyright © 2015 John Wiley \& Sons, Ltd.},
  language =	 {en},
  number =	 5,
  urldate =	 {2016-01-13},
  journal =	 {International Journal for Numerical Methods in
                  Engineering},
  author =	 {Colom\'es, Oriol and Badia, Santiago},
  year =	 2016,
  keywords =	 {Adaptive time stepping, High-order, incompressible
                  Navier–Stokes, pressure-segregation, Runge–Kutta,
                  time integration},
  pages =	 {372--400},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/BFKFBMGK/Colomés
                  and Badia - 2016 - Segregated Runge–Kutta methods
                  for the
                  incompressi.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/UHB8WXXQ/abstract.html:text/html}
}

@article{colomes_segregated_2017,
	title = {Segregated Runge–Kutta time integration of convection-stabilized mixed finite element schemes for wall-unresolved {LES} of incompressible flows},
	volume = {313},
	issn = {0045-7825},
	url = {http://www.sciencedirect.com/science/article/pii/S0045782516312555},
	doi = {10.1016/j.cma.2016.09.040},
	abstract = {In this work, we develop a high-performance numerical framework for the large eddy simulation ({LES}) of incompressible flows. The spatial discretization of the nonlinear system is carried out using mixed finite element ({FE}) schemes supplemented with symmetric projection stabilization of the convective term and a penalty term for the divergence constraint. These additional terms introduced at the discrete level have been proved to act as implicit {LES} models. In order to perform meaningful wall-unresolved simulations, we consider a weak imposition of the boundary conditions using a Nitsche’s-type scheme, where the tangential component penalty term is designed to act as a wall law. Next, segregated Runge–Kutta ({SRK}) schemes (recently proposed by the authors for laminar flow problems) are applied to the {LES} simulation of turbulent flows. By the introduction of a penalty term on the trace of the acceleration, these methods exhibit excellent stability properties for both implicit and explicit treatment of the convective terms. {SRK} schemes are excellent for large-scale simulations, since they reduce the computational cost of the linear system solves by splitting velocity and pressure computations at the time integration level, leading to two uncoupled systems. The pressure system is a Darcy-type problem that can easily be preconditioned using a traditional block-preconditioning scheme that only requires a Poisson solver. At the end, only coercive systems have to be solved, which can be effectively preconditioned by multilevel domain decomposition schemes, which are both optimal and scalable. The framework is applied to the Taylor–Green and turbulent channel flow benchmarks in order to prove the accuracy of the convection-stabilized mixed {FEs} as {LES} models and {SRK} time integrators. The scalability of the preconditioning techniques (in space only) has also been proven for one step of the {SRK} scheme for the Taylor–Green flow using uniform meshes. Moreover, a turbulent flow around a {NACA} profile is solved to show the applicability of the proposed algorithms for a realistic problem.},
	pages = {189--215},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	journaltitle = {Computer Methods in Applied Mechanics and Engineering},
	shortjournal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Colomés, Oriol and Badia, Santiago},
	urldate = {2017-01-10},
	date = {2017-01-01},
        year = {2017},
	keywords = {large eddy simulation, Runge–Kutta, Turbulence, Variational multiscale, Wall models},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/QM3NZ876/Colomés and Badia - 2017 - Segregated Runge–Kutta time integration of convect.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/TVNT347W/S0045782516312555.html:text/html}
}

@techreport{coteer_complimentary_2012,
  title =	 {{3D} opportunity: Additive manufacturing paths to
                  performance, innovation, and growth},
  url =
                  {http://www.scansourceposbarcode.com/~/media/pos-and-barcode-us/Programs/scansource-3d/training/media/dr14_3d_opportunity.pdf},
  urldate =	 {2015-12-29},
  author =	 {Cotteler, Mark and Joyce, Jim},
  institution =	 {Deloitte University Press},
  year =	 2014,
  file =
                  {DR14_3D_Opportunity.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/IC98WKRE/DR14_3D_Opportunity.pdf:application/pdf}
}

@article{craeghs_feedback_2010,
  series =	 {Laser {Assisted} {Net} {Shape} {Engineering} 6,
                  {Proceedings} of the {LANE} 2010, {Part} 2},
  title =	 {Feedback control of {Layerwise} {Laser} {Melting}
                  using optical sensors},
  volume =	 {5, Part B},
  issn =	 {1875-3892},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S1875389210005043},
  doi =		 {10.1016/j.phpro.2010.08.078},
  abstract =	 {Layerwise Laser Melting (LLM) is a layerwise
                  production technique enabling the production of
                  complex metallic parts. Thin powder layers are
                  molten according to a predefined scan pattern by
                  means of a laser source. Nowadays constant process
                  parameters are used throughout the build, leading
                  for some geometries to an overly thick feature size
                  or overheating at downfacing surfaces. In this paper
                  a monitoring and control system is presented which
                  enables monitoring the melt pool continously at high
                  speed throughout the building process. The signals
                  from the sensors can be incorporated in a real-time
                  control loop, in this way enabling feedback control
                  of the process parameters. In this paper the
                  experimental set-up will be first shown. Next the
                  dynamic relation between the melt pool and the
                  process parameters is identified. Finally the proof
                  of concept for feedback control is demonstrated with
                  experimental results.},
  urldate =	 {2015-12-29},
  journal =	 {Physics Procedia},
  author =	 {Craeghs, Tom and Bechmann, Florian and Berumen,
                  Sebastian and Kruth, Jean-Pierre},
  year =	 2010,
  keywords =	 {Feedback control, Layerwise laser melting,
                  Monitoring, Optical sensors},
  pages =	 {505--514},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/6IRJVE7Z/Craeghs
                  et al. - 2010 - Feedback control of Layerwise Laser
                  Melting using .pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/UGZV7VQI/S1875389210005043.html:text/html}
}

@article{cyr_stabilization_2012,
  title =	 {Stabilization and scalable block preconditioning for
                  the {Navier}--{Stokes} equations},
  volume =	 231,
  issn =	 {0021-9991},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0021999111005195},
  doi =		 {10.1016/j.jcp.2011.09.001},
  abstract =	 {This study compares several block-oriented
                  preconditioners for the stabilized finite element
                  discretization of the incompressible Navier-Stokes
                  equations. This includes standard additive Schwarz
                  domain decomposition methods, aggressive coarsening
                  multigrid, and three preconditioners based on an
                  approximate block LU factorization, specifically
                  SIMPLEC, LSC, and PCD. Robustness is considered with
                  a particular focus on the impact that different
                  stabilization methods have on preconditioner
                  performance. Additionally, parallel scaling studies
                  are undertaken. The numerical results indicate that
                  aggressive coarsening multigrid, LSC and PCD all
                  have good algorithmic scalability. Coupling this
                  with the fact that block methods can be applied to
                  systems arising from stable mixed discretizations
                  implies that these techniques are a promising
                  direction for developing scalable methods for
                  Navier-Stokes.},
  number =	 2,
  urldate =	 {2015-10-27},
  journal =	 {Journal of Computational Physics},
  author =	 {Cyr, Eric C. and Shadid, John N. and Tuminaro,
                  Raymond S.},
  year =	 2012,
  keywords =	 {Block preconditioning, Fully-implicit, Large-scale
                  parallel, multilevel preconditioner, Navier-Stokes,
                  Stabilized finite element},
  pages =	 {345--363},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/3RCMZX8T/Cyr
                  et al. - 2012 - Stabilization and scalable block
                  preconditioning f.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/FGE4FXQX/S0021999111005195.html:text/html}
}

@article{d_hu_modelling_2003,
  title =	 {Modelling and measuring the thermal behaviour of the
                  molten pool in closed-loop controlled laser-based
                  additive manufacturing},
  volume =	 217,
  issn =	 {0954-4054},
  doi =		 {10.1243/095440503321628125},
  number =	 4,
  journal =	 {Proceedings of the Institution of Mechanical
                  Engineers, Part B: Journal of Engineering
                  Manufacture},
  author =	 {D Hu, R. Kovacevic},
  year =	 2003,
  pages =	 {441--452},
  file =	 {Modelling and measuring the thermal behaviour of the
                  molten pool in closed-loop controlled laser-based
                  additive
                  manufacturing:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/W8PWCKCE/269625554_Modelling_and_measuring_the_thermal_behaviour_of_the_molten_pool_in_closed-loop_contr.html:text/html}
}

@misc{dakota,
  title =	 {Dakota, A Multilevel Parallel Object-Oriented Framework for Design Optimization, Parameter Estimation, Uncertainty Quantification, and Sensitivity Analysis},
  howpublished = {\url{https://dakota.sandia.gov/}},
  urldate =	 {2017-01-11}
}

@TechReport{dakota-report,
  author = 	 {Adams, B.M. and Bauman, L.E. and Bohnhoff, W.J. and Dalbey, K.R. and Ebeida, M.S. and Eddy, J.P. and Eldred, M.S. and Hough, P.D. and Hu, K.T. and Jakeman, J.D. and Stephens, J.A. and Swiler, L.P. and Vigil, D.M. and Wildey, T.M.},
  title = 	 {Dakota, A Multilevel Parallel Object-Oriented Framework for Design Optimization, Parameter Estimation, Uncertainty Quantification, and Sensitivity Analysis: Version 6.0 User’s Manual},
  institution =  {Sandia Technical Report SAND2014-4633},
  year = 	 {2015}
}

@techreport{deloitte,
  title =	 {{3D} opportunity for quality assurance and parts
                  qualification},
  url =
                  {http://dupress.com/articles/3d-printing-quality-assurance-in-manufacturing/},
  urldate =	 {2015-12-29},
  institution =	 {Deloitte University Press},
  author =	 {Ian Wing and Rob Gorham and Brenna Sniderman},
  year =	 2015
}

@article{deng_polynomial_2008-1,
	title = {Polynomial splines over hierarchical T-meshes},
	volume = {70},
	issn = {1524-0703},
	url = {http://www.sciencedirect.com/science/article/pii/S1524070308000039},
	doi = {10.1016/j.gmod.2008.03.001},
	abstract = {In this paper, we introduce a new type of splines—polynomial splines over hierarchical T-meshes (called {PHT}-splines) to model geometric objects. {PHT}-splines are a generalization of B-splines over hierarchical T-meshes. We present the detailed construction process of spline basis functions over T-meshes which have the same important properties as B-splines do, such as nonnegativity, local support and partition of unity. As two fundamental operations, cross insertion and cross removal of {PHT}-splines are discussed. With the new splines, surface models can be constructed efficiently and adaptively to fit open or closed mesh models, where only linear systems of equations with a few unknowns are involved. With this approach, a {NURBS} surface can be efficiently simplified into a {PHT}-spline which dramatically reduces the superfluous control points of the {NURBS} surface. Furthermore, {PHT}-splines allow for several important types of geometry processing in a natural and efficient manner, such as conversion of a {PHT}-spline into an assembly of tensor-product spline patches, and shape simplification of {PHT}-splines over a coarser T-mesh. {PHT}-splines not only inherit many good properties of Sederberg’s T-splines such as adaptivity and locality, but also extend T-splines in several aspects except that they are only C 1 continuous. For example, {PHT}-splines are polynomial instead of rational; cross insertion/removal of {PHT}-splines is local and simple.},
	pages = {76--86},
	number = {4},
	journal      = {Graphical Models},
	journaltitle = {Graphical Models},
	shortjournal = {Graphical Models},
	author = {Deng, Jiansong and Chen, Falai and Li, Xin and Hu, Changqi and Tong, Weihua and Yang, Zhouwang and Feng, Yuyu},
        year = {2018},
	urldate = {2017-01-02},
	date = {2008-07},
	keywords = {local refinement, {NURBS}, Shape simplification, Surface fitting, T-mesh, T-splines},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/7MQBPZ4H/Deng et al. - 2008 - Polynomial splines over hierarchical T-meshes.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/QHFUI2B5/S1524070308000039.html:text/html}
}

@article{deparis_fluidstructure_2006,
	title = {Fluid–structure algorithms based on Steklov–Poincaré operators},
	volume = {195},
	issn = {0045-7825},
	url = {http://www.sciencedirect.com/science/article/pii/S0045782505005190},
	doi = {10.1016/j.cma.2005.09.029},
	series = {John H. Argyris Memorial Issue. Part {II}},
	abstract = {In this paper we review some classical algorithms for fluid–structure interaction problems and we propose an alternative viewpoint mutuated from the domain decomposition theory. This approach yields preconditioned Richardson iterations on the Steklov–Poincaré nonlinear equation at the fluid–structure interface.},
	pages = {5797--5812},
	number = {41},
        journal = {Computer Methods in Applied Mechanics and Engineering},
	journaltitle = {Computer Methods in Applied Mechanics and Engineering},
	shortjournal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Deparis, Simone and Discacciati, Marco and Fourestey, Gilles and Quarteroni, Alfio},
	urldate = {2017-01-13},
	date = {2006-08-15},
        year = {2006},
	keywords = {domain decomposition, finite element method, Fluid–structure interaction, Linear and nonlinear interface operators, Steklov–Poincaré equation},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/9VDQG9PX/Deparis et al. - 2006 - Fluid–structure algorithms based on Steklov–Poinca.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/V4ITZDHR/S0045782505005190.html:text/html}
}

@unpublished{dipietro:hal-00839487,
  hal_id =	 {hal-00839487},
  url =		 {http://hal.archives-ouvertes.fr/hal-00839487},
  title =	 {{A posteriori error estimates, stopping criteria,
                  and adaptivity for multiphase compositional Darcy
                  flows in porous media}},
  author =	 {Di Pietro, Daniele Antonio and Flauraud, Eric and
                  Vohral{\'\i}k, Martin and Yousef, Soleiman},
  abstract =	 {{In this paper we derive a posteriori error
                  estimates for the compositional model of multiphase
                  Darcy flow in porous media, consisting of a system
                  of strongly coupled nonlinear unsteady partial
                  differential and algebraic equations. We show how to
                  control the dual norm of the residual augmented by a
                  nonconformity evaluation term by fully computable
                  estimators. We then decompose the estimators into
                  the space, time, linearization, and algebraic error
                  components. This allows to formulate criteria for
                  stopping the iterative algebraic solver and the
                  iterative linearization solver when the
                  corresponding error components do not affect
                  significantly the overall error. Moreover, the
                  spatial and temporal error components can be
                  balanced by time step and space mesh adaptation. Our
                  analysis applies to a broad class of standard
                  numerical methods, and is independent of the
                  linearization and of the iterative algebraic solvers
                  employed. We exemplify it for the two-point finite
                  volume method with fully implicit Euler time
                  stepping, the Newton linearization, and the GMRes
                  algebraic solver. Numerical results on two real-life
                  reservoir engineering examples confirm that
                  significant computational gains can be achieved
                  thanks to our adaptive stopping criteria, already on
                  fixed meshes, without any noticeable loss of
                  precision.}},
  keywords =	 {A posteriori error analysis, adaptive algorithms,
                  compositional Darcy flow, finite volume methods.},
  language =	 {Anglais},
  affiliation =	 {Institut de Math{\'e}matiques et de Mod{\'e}lisation
                  de Montpellier - I3M , IFP Energies Nouvelles -
                  IFPEN , POMDAPI - Inria Paris-Rocquencourt ,
                  Laboratoire Jacques-Louis Lions - LJLL},
  collaboration ={ERT "Enhanced oil recovery and geological
                  sequestration of CO2 : mesh adaptivity, a posteriori
                  error control, and other advanced techniques" },
  year =	 2013,
  pdf =
                  {http://hal.archives-ouvertes.fr/hal-00839487/PDF/compositional.pdf},
}

@unpublished{dipietro:hal-00839487,
  hal_id =	 {hal-00839487},
  url =		 {http://hal.archives-ouvertes.fr/hal-00839487},
  title =	 {{A posteriori error estimates, stopping criteria,
                  and adaptivity for multiphase compositional Darcy
                  flows in porous media}},
  author =	 {Di Pietro, Daniele Antonio and Flauraud, Eric and
                  Vohral{\'\i}k, Martin and Yousef, Soleiman},
  abstract =	 {{In this paper we derive a posteriori error
                  estimates for the compositional model of multiphase
                  Darcy flow in porous media, consisting of a system
                  of strongly coupled nonlinear unsteady partial
                  differential and algebraic equations. We show how to
                  control the dual norm of the residual augmented by a
                  nonconformity evaluation term by fully computable
                  estimators. We then decompose the estimators into
                  the space, time, linearization, and algebraic error
                  components. This allows to formulate criteria for
                  stopping the iterative algebraic solver and the
                  iterative linearization solver when the
                  corresponding error components do not affect
                  significantly the overall error. Moreover, the
                  spatial and temporal error components can be
                  balanced by time step and space mesh adaptation. Our
                  analysis applies to a broad class of standard
                  numerical methods, and is independent of the
                  linearization and of the iterative algebraic solvers
                  employed. We exemplify it for the two-point finite
                  volume method with fully implicit Euler time
                  stepping, the Newton linearization, and the GMRes
                  algebraic solver. Numerical results on two real-life
                  reservoir engineering examples confirm that
                  significant computational gains can be achieved
                  thanks to our adaptive stopping criteria, already on
                  fixed meshes, without any noticeable loss of
                  precision.}},
  keywords =	 {A posteriori error analysis, adaptive algorithms,
                  compositional Darcy flow, finite volume methods.},
  language =	 {Anglais},
  affiliation =	 {Institut de Math{\'e}matiques et de Mod{\'e}lisation
                  de Montpellier - I3M , IFP Energies Nouvelles -
                  IFPEN , POMDAPI - Inria Paris-Rocquencourt ,
                  Laboratoire Jacques-Louis Lions - LJLL},
  collaboration ={ERT "Enhanced oil recovery and geological
                  sequestration of CO2 : mesh adaptivity, a posteriori
                  error control, and other advanced techniques" },
  year =	 2013,
  pdf =
                  {http://hal.archives-ouvertes.fr/hal-00839487/PDF/compositional.pdf},
}

@article{dohrmann_approximate_2007,
  title =	 {An approximate {BDDC} preconditioner},
  volume =	 14,
  copyright =	 {This article is a U.S. Government work and is in the
                  public domain in the U.S.A. Published in 2006 by
                  John Wiley \& Sons, Ltd.},
  issn =	 {1099-1506},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nla.514/abstract},
  doi =		 {10.1002/nla.514},
  abstract =	 {The balancing domain decomposition by constraints
                  (BDDC) preconditioner requires direct solutions of
                  two linear systems for each substructure and one
                  linear system for a global coarse problem. The
                  computations and memory needed for these solutions
                  can be prohibitive if any one system is too
                  large. We investigate an approach for addressing
                  this issue based on approximating the direct
                  solutions using preconditioners. In order to retain
                  the attractive numerical properties of the exact
                  version of BDDC, some of the preconditioners must
                  possess a property related to the substructure null
                  spaces. We describe a simple method to equip
                  preconditioners with this property. Numerical
                  results demonstrate the usefulness of the approach
                  and confirm the theory. Published in 2006 by John
                  Wiley \& Sons, Ltd.},
  language =	 {en},
  number =	 2,
  urldate =	 {2013-12-18},
  journal =	 {Numerical Linear Algebra with Applications},
  author =	 {Dohrmann, C. R.},
  year =	 2007,
  keywords =	 {BDDC, domain decomposition, FETI-DP, multigrid,
                  preconditioners},
  pages =	 {149--168},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/7HENCTTX/Dohrmann
                  - 2007 - An approximate BDDC
                  preconditioner.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/PJVAFEI6/abstract\;jsessionid=A17FDB41032855E3600ACF3C056883E9.html:text/html}
}

@article{dohrmann_preconditioner_2003,
  title =	 {A Preconditioner for Substructuring Based on
                  Constrained Energy Minimization},
  volume =	 25,
  issn =	 10648275,
  url =
                  {http://link.aip.org/link/SJOCE3/v25/i1/p246/s1&Agg=doi},
  doi =		 {10.1137/S1064827502412887},
  number =	 1,
  urldate =	 {2012-02-27},
  journal =	 {SIAM Journal on Scientific Computing},
  author =	 {Dohrmann, Clark R.},
  year =	 2003,
  pages =	 {246--258},
  file =	 {A Preconditioner for Substructuring Based on
                  Constrained Energy Minimization | Browse SISC - SIAM
                  J. on Scientific
                  Computing:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/83MW5DET/p246_s1.html:text/html;GetPDFServlet.pdf:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/KU5PNTBQ/GetPDFServlet.pdf:application/pdf}
}

@article{dokken_polynomial_2013,
	title = {Polynomial splines over locally refined box-partitions},
	volume = {30},
	issn = {0167-8396},
	url = {http://www.sciencedirect.com/science/article/pii/S0167839613000113},
	doi = {10.1016/j.cagd.2012.12.005},
	abstract = {We address progressive local refinement of splines defined on axes parallel box-partitions and corresponding box-meshes in any space dimension. The refinement is specified by a sequence of mesh-rectangles (axes parallel hyperrectangles) in the mesh defining the spline spaces. In the 2-variate case a mesh-rectangle is a knotline segment. When starting from a tensor-mesh this refinement process builds what we denote an {LR}-mesh, a special instance of a box-mesh. On the {LR}-mesh we obtain a collection of hierarchically scaled B-splines, denoted {LR} B-splines, that forms a nonnegative partition of unity and spans the complete piecewise polynomial space on the mesh when the mesh construction follows certain simple rules. The dimensionality of the spline space can be determined using some recent dimension formulas.},
	pages = {331--356},
	number = {3},
        journal      = {Computer Aided Geometric Design},
	journaltitle = {Computer Aided Geometric Design},
	shortjournal = {Computer Aided Geometric Design},
	author = {Dokken, Tor and Lyche, Tom and Pettersen, Kjell Fredrik},
        year = {2013},
	urldate = {2016-12-30},
	date = {2013-03},
	keywords = {Box-partitions, Dimension of spline spaces, Isogeometric analysis, Locally refined tensor product B-splines, {LR}-meshes},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/HSVWQCDC/Dokken et al. - 2013 - Polynomial splines over locally refined box-partit.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/9DA64J3E/S0167839613000113.html:text/html}
}

@ARTICLE{dongarra_international_2011,
  author =	 {Dongarra, Jack and Beckman, Pete and Moore, Terry
                  and Aerts, Patrick and Aloisio, Giovanni and Andre,
                  Jean-Claude and Barkai, David and Berthou, Jean-Yves
                  and Boku, Taisuke and Braunschweig, Bertrand and
                  Cappello, Franck and Chapman, Barbara and Chi,
                  Xuebin and Choudhary, Alok and Dosanjh, Sudip and
                  Dunning, Thom and Fiore, Sandro and Geist, Al and
                  Gropp, Bill and Harrison, Robert and Hereld, Mark
                  and Heroux, Michael and Hoisie, Adolfy and Hotta,
                  Koh and Jin, Zhong and Ishikawa, Yutaka and Johnson,
                  Fred and Kale, Sanjay and Kenway, Richard and Keyes,
                  David and Kramer, Bill and Labarta, Jesus and
                  Lichnewsky, Alain and Lippert, Thomas and Lucas, Bob
                  and Maccabe, Barney and Matsuoka, Satoshi and
                  Messina, Paul and Michielse, Peter and Mohr, Bernd
                  and Mueller, Matthias S. and Nagel, Wolfgang E. and
                  Nakashima, Hiroshi and Papka, Michael E. and Reed,
                  Dan and Sato, Mitsuhisa and Seidel, Ed and Shalf,
                  John and Skinner, David and Snir, Marc and Sterling,
                  Thomas and Stevens, Rick and Streitz, Fred and
                  Sugar, Bob and Sumimoto, Shinji and Tang, William
                  and Taylor, John and Thakur, Rajeev and Trefethen,
                  Anne and Valero, Mateo and Steen, Aad van der and
                  Vetter, Jeffrey and Williams, Peg and Wisniewski,
                  Robert and Yelick, Kathy},
  title =	 {The International Exascale Software Project roadmap},
  journal =	 {International Journal of High Performance Computing
                  Applications},
  year =	 2011,
  volume =	 25,
  pages =	 {3--60},
  number =	 1,
  abstract =	 {Over the last 20 years, the open-source community
                  has provided more and more software on which the
                  worldâs high-performance computing systems
                  depend for performance and productivity. The
                  community has invested millions of dollars and years
                  of effort to build key components.  However,
                  although the investments in these separate software
                  elements have been tremendously valuable, a great
                  deal of productivity has also been lost because of
                  the lack of planning, coordination, and key
                  integration of technologies necessary to make them
                  work together smoothly and efficiently, both within
                  individual petascale systems and between different
                  systems. It seems clear that this completely
                  uncoordinated development model will not provide the
                  software needed to support the unprecedented
                  parallelism required for peta/ exascale computation
                  on millions of cores, or the flexibility required to
                  exploit new hardware models and features, such as
                  transactional memory, speculative execution, and
                  graphics processing units. This report describes the
                  work of the community to prepare for the challenges
                  of exascale computing, ultimately combing their
                  efforts in a coordinated International Exascale
                  Software Project.},
  doi =		 {10.1177/1094342010391989},
  file =	 {Full Text
                  PDF:/home/principe/Javier/System/Config/firefox/zotero/storage/VRQJSJHA/Dongarra
                  et al. - 2011 - The International Exascale Software
                  Project
                  roadma.pdf:application/pdf;Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/M64TFIAR/3.html:text/html},
  issn =	 {1094-3420, 1741-2846},
  keywords =	 {exascale computing, high-performance computing,
                  software stack},
  language =	 {en},
  owner =	 {principe},
  timestamp =	 {2014.03.21},
  url =		 {http://hpc.sagepub.com/content/25/1/3},
  urldate =	 {2014-03-20}
}

@ARTICLE{dongarra_international_2011,
  author =	 {Dongarra, Jack and Beckman, Pete and Moore, Terry
                  and Aerts, Patrick and Aloisio, Giovanni and Andre,
                  Jean-Claude and Barkai, David and Berthou, Jean-Yves
                  and Boku, Taisuke and Braunschweig, Bertrand and
                  Cappello, Franck and Chapman, Barbara and Chi,
                  Xuebin and Choudhary, Alok and Dosanjh, Sudip and
                  Dunning, Thom and Fiore, Sandro and Geist, Al and
                  Gropp, Bill and Harrison, Robert and Hereld, Mark
                  and Heroux, Michael and Hoisie, Adolfy and Hotta,
                  Koh and Jin, Zhong and Ishikawa, Yutaka and Johnson,
                  Fred and Kale, Sanjay and Kenway, Richard and Keyes,
                  David and Kramer, Bill and Labarta, Jesus and
                  Lichnewsky, Alain and Lippert, Thomas and Lucas, Bob
                  and Maccabe, Barney and Matsuoka, Satoshi and
                  Messina, Paul and Michielse, Peter and Mohr, Bernd
                  and Mueller, Matthias S. and Nagel, Wolfgang E. and
                  Nakashima, Hiroshi and Papka, Michael E. and Reed,
                  Dan and Sato, Mitsuhisa and Seidel, Ed and Shalf,
                  John and Skinner, David and Snir, Marc and Sterling,
                  Thomas and Stevens, Rick and Streitz, Fred and
                  Sugar, Bob and Sumimoto, Shinji and Tang, William
                  and Taylor, John and Thakur, Rajeev and Trefethen,
                  Anne and Valero, Mateo and Steen, Aad van der and
                  Vetter, Jeffrey and Williams, Peg and Wisniewski,
                  Robert and Yelick, Kathy},
  title =	 {The International Exascale Software Project roadmap},
  journal =	 {International Journal of High Performance Computing
                  Applications},
  year =	 2011,
  volume =	 25,
  pages =	 {3--60},
  number =	 1,
  abstract =	 {Over the last 20 years, the open-source community
                  has provided more and more software on which the
                  worldâs high-performance computing systems
                  depend for performance and productivity. The
                  community has invested millions of dollars and years
                  of effort to build key components.  However,
                  although the investments in these separate software
                  elements have been tremendously valuable, a great
                  deal of productivity has also been lost because of
                  the lack of planning, coordination, and key
                  integration of technologies necessary to make them
                  work together smoothly and efficiently, both within
                  individual petascale systems and between different
                  systems. It seems clear that this completely
                  uncoordinated development model will not provide the
                  software needed to support the unprecedented
                  parallelism required for peta/ exascale computation
                  on millions of cores, or the flexibility required to
                  exploit new hardware models and features, such as
                  transactional memory, speculative execution, and
                  graphics processing units. This report describes the
                  work of the community to prepare for the challenges
                  of exascale computing, ultimately combing their
                  efforts in a coordinated International Exascale
                  Software Project.},
  doi =		 {10.1177/1094342010391989},
  file =	 {Full Text
                  PDF:/home/principe/Javier/System/Config/firefox/zotero/storage/VRQJSJHA/Dongarra
                  et al. - 2011 - The International Exascale Software
                  Project
                  roadma.pdf:application/pdf;Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/M64TFIAR/3.html:text/html},
  issn =	 {1094-3420, 1741-2846},
  keywords =	 {exascale computing, high-performance computing,
                  software stack},
  language =	 {en},
  owner =	 {principe},
  timestamp =	 {2014.03.21},
  url =		 {http://hpc.sagepub.com/content/25/1/3},
  urldate =	 {2014-03-20}
}


@article{dryja_bddc_2007,
  title =	 {{BDDC} methods for discontinuous {Galerkin}
                  discretization of elliptic problems},
  volume =	 23,
  issn =	 {0885-064X},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0885064X07000325},
  doi =		 {10.1016/j.jco.2007.02.003},
  abstract =	 {A discontinuous Galerkin (DG) discretization of
                  Dirichlet problem for second-order elliptic
                  equations with discontinuous coefficients in 2-D is
                  considered. For this discretization, balancing
                  domain decomposition with constraints (BDDC)
                  algorithms are designed and analyzed as an additive
                  Schwarz method (ASM). The coarse and local problems
                  are defined using special partitions of unity and
                  edge constraints. Under certain assumptions on the
                  coefficients and the mesh sizes across ∂ Ω i , where
                  the Ω i are disjoint subregions of the original
                  region Ω , a condition number estimate C ( 1 + max i
                  log ( H i / h i ) ) 2 is established with C
                  independent of h i , H i and the jumps of the
                  coefficients. The algorithms are well suited for
                  parallel computations and can be straightforwardly
                  extended to the 3-D problems. Results of numerical
                  tests are included which confirm the theoretical
                  results and the necessity of the imposed
                  assumptions.},
  number =	 {4-6},
  journal =	 {Journal of Complexity},
  author =	 {Dryja, Maksymilian and Galvis, Juan and Sarkis,
                  Marcus},
  year =	 2007,
  keywords =	 {BDDC algorithms, Discontinuous Galerkin method,
                  Elliptic problems with discontinuous coefficients,
                  finite element method, Interior penalty
                  discretization, preconditioners, Schwarz methods},
  pages =	 {715--739},
  file =	 {ScienceDirect Full Text
                  PDF:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/VGI68EDN/Dryja
                  et al. - 2007 - BDDC methods for discontinuous
                  Galerkin
                  discretiza.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/65ABUG9P/Dryja
                  et al. - 2007 - BDDC methods for discontinuous
                  Galerkin discretiza.html:text/html}
}

@article{dryja_bddc_2007,
  title =	 {{BDDC} methods for discontinuous {G}alerkin
                  discretization of elliptic problems},
  volume =	 23,
  issn =	 {0885-{064X}},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0885064X07000325},
  doi =		 {10.1016/j.jco.2007.02.003},
  abstract =	 {A discontinuous Galerkin ({DG)} discretization of
                  Dirichlet problem for second-order elliptic
                  equations with discontinuous coefficients in 2-D is
                  considered. For this discretization, balancing
                  domain decomposition with constraints ({BDDC)}
                  algorithms are designed and analyzed as an additive
                  Schwarz method ({ASM).} The coarse and local
                  problems are defined using special partitions of
                  unity and edge constraints. Under certain
                  assumptions on the coefficients and the mesh sizes
                  across ∂ Ω i , where the Ω i are disjoint subregions
                  of the original region Ω , a condition number
                  estimate C ( 1 + max i log ( H i / h i ) ) 2 is
                  established with C independent of h i , H i and the
                  jumps of the coefficients. The algorithms are well
                  suited for parallel computations and can be
                  straightforwardly extended to the 3-D
                  problems. Results of numerical tests are included
                  which confirm the theoretical results and the
                  necessity of the imposed assumptions.},
  number =	 {4--6},
  journal =	 {Journal of Complexity},
  author =	 {Dryja, Maksymilian and Galvis, Juan and Sarkis,
                  Marcus},
  year =	 2007,
  keywords =	 {{BDDC} algorithms, Discontinuous Galerkin method,
                  Elliptic problems with discontinuous coefficients,
                  finite element method, Interior penalty
                  discretization, preconditioners, Schwarz methods},
  pages =	 {715--739},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/3FMTKJRK/Dryja
                  et al. - 2007 - BDDC methods for discontinuous
                  Galerkin
                  discretiza.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/P84FKIDN/Dryja
                  et al. - 2007 - BDDC methods for discontinuous
                  Galerkin discretiza.html:text/html}
}

@article{elman_preconditioning_2005,
  title =	 {Preconditioning strategies for models of
                  incompressible flow},
  volume =	 25,
  issn =	 {0885-7474, 1573-7691},
  url =
                  {http://www.springerlink.com/content/h70w41779870h8gm/},
  doi =		 {10.1007/BF02728995},
  number =	 {1-2},
  urldate =	 {2012-02-08},
  journal =	 {Journal of Scientific Computing},
  author =	 {Elman, H. C.},
  year =	 2005,
  pages =	 {347--366},
  file =	 {Journal of Scientific Computing, Volume 25, Numbers
                  1-2 -
                  SpringerLink:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/THNEER42/h70w41779870h8gm.html:text/html}
}

@article{elman_preconditioning_2005,
  title =	 {Preconditioning strategies for models of
                  incompressible flow},
  volume =	 25,
  issn =	 {0885-7474, 1573-7691},
  url =
                  {http://www.springerlink.com/content/h70w41779870h8gm/},
  doi =		 {10.1007/BF02728995},
  number =	 {1-2},
  urldate =	 {2012-02-08},
  journal =	 {Journal of Scientific Computing},
  author =	 {Elman, H. C.},
  year =	 2005,
  pages =	 {347--366},
  file =	 {Journal of Scientific Computing, Volume 25, Numbers
                  1-2 -
                  SpringerLink:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/THNEER42/h70w41779870h8gm.html:text/html}
}

@article{emmett_toward_2012,
  title =	 {Toward an efficient parallel in time method for
                  partial differential equations},
  volume =	 7,
  issn =	 {2157-5452, 1559-3940},
  url =		 {http://msp.org/camcos/2012/7-1/p04.xhtml},
  doi =		 {10.2140/camcos.2012.7.105},
  language =	 {en},
  number =	 1,
  urldate =	 {2014-05-15},
  journal =	 {Communications in Applied Mathematics and
                  Computational Science},
  author =	 {Emmett, Matthew and Minion, Michael},
  year =	 2012,
  pages =	 {105--132},
  file =	 {Communications in Applied Mathematics and
                  Computational Science Vol. 7, No. 1,
                  2012:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/U77P34M5/p04.html:application/xhtml+xml}
}

@article{emmett_toward_2012,
  title =	 {Toward an efficient parallel in time method for
                  partial differential equations},
  volume =	 7,
  issn =	 {2157-5452, 1559-3940},
  url =		 {http://msp.org/camcos/2012/7-1/p04.xhtml},
  doi =		 {10.2140/camcos.2012.7.105},
  language =	 {en},
  number =	 1,
  urldate =	 {2014-05-15},
  journal =	 {Communications in Applied Mathematics and
                  Computational Science},
  author =	 {Emmett, Matthew and Minion, Michael},
  year =	 2012,
  pages =	 {105--132},
  file =	 {Communications in Applied Mathematics and
                  Computational Science Vol. 7, No. 1,
                  2012:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/U77P34M5/p04.html:application/xhtml+xml}
}

@article{espinoza_sommerfeld_2014,
  title =	 {A {Sommerfeld} non-reflecting boundary condition for
                  the wave equation in mixed form},
  volume =	 276,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782514001017},
  doi =		 {10.1016/j.cma.2014.03.015},
  abstract =	 {In this paper we develop numerical approximations of
                  the wave equation in mixed form supplemented with
                  non-reflecting boundary conditions (NRBCs) of
                  Sommerfeld-type on artificial boundaries for
                  truncated domains. We consider three different
                  variational forms for this problem, depending on the
                  functional space for the solution, in particular, in
                  what refers to the regularity required on artificial
                  boundaries. Then, stabilized finite element methods
                  that can mimic these three functional settings are
                  described. Stability and convergence analyses of
                  these stabilized formulations including the NRBC are
                  presented. Additionally, numerical convergence test
                  are evaluated for various polynomial interpolations,
                  stabilization methods and variational
                  forms. Finally, several benchmark problems are
                  solved to determine the accuracy of these methods in
                  2D and 3D.},
  urldate =	 {2014-06-12},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Espinoza, Hector and Codina, Ramon and Badia,
                  Santiago},
  year =	 2014,
  keywords =	 {35F46, 35L05, 35L50, 35L65, 35M13, 65J10, 65M12,
                  65M15, 65M60, 76M10, Artificial boundary condition,
                  Non-reflecting boundary condition, Open boundary
                  condition, Stabilized finite element methods,
                  Variational multi-scale method, Wave equation},
  pages =	 {122--148}
}

@article{espinoza_time_2015,
  title =	 {On some time marching schemes for the stabilized
                  finite element approximation of the mixed wave
                  equation},
  volume =	 296,
  issn =	 00457825,
  url =
                  {http://linkinghub.elsevier.com/retrieve/pii/S0045782515002285},
  doi =		 {10.1016/j.cma.2015.07.016},
  language =	 {en},
  urldate =	 {2015-10-22},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Espinoza, Hector and Codina, Ramon and Badia,
                  Santiago},
  year =	 2015,
  pages =	 {295--326},
  file =
                  {1-s2.0-S0045782515002285-main.pdf:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/6M7B7UPW/1-s2.0-S0045782515002285-main.pdf:application/pdf}
}

@incollection{estellers_robust_2015,
  title =	 {Robust {Poisson} {Surface} {Reconstruction}},
  url =
                  {http://link.springer.com/chapter/10.1007/978-3-319-18461-6_42},
  urldate =	 {2016-01-04},
  booktitle =	 {Scale {Space} and {Variational} {Methods} in
                  {Computer} {Vision}},
  publisher =	 {Springer},
  author =	 {Estellers, Virginia and Scott, Michael and Tew,
                  Kevin and Soatto, Stefano},
  editor =	 {Aujol, J.-F. and Nikolova, M. and Papadakis, N.},
  year =	 2015,
  pages =	 {525--537},
  volume =	 9807,
  series =	 {Lecture Notes in Computer Science},
  file =
                  {estellersSTS15.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/3CK5PPTV/estellersSTS15.pdf:application/pdf}
}

@article{estellers_robust_2015-1,
  title =	 {Robust {Surface} {Reconstruction}},
  url =		 {ftp://ftp.math.ucla.edu/pub/camreport/cam15-57.pdf},
  urldate =	 {2016-01-04},
  author =	 {Estellers, V. and Scott, M. A. and Soatto, S.},
  year =	 2015,
  journal =	 {Submitted},
  file =
                  {Estellers2015RR.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/U9NPIXV6/Estellers2015RR.pdf:application/pdf}
}

@incollection{falgout_hypre:_2002,
  series =	 {Lecture Notes in Computer Science},
  title =	 {hypre: A Library of High Performance
                  Preconditioners},
  copyright =	 {©2002 Springer-Verlag Berlin Heidelberg},
  isbn =	 {978-3-540-43594-5, 978-3-540-47789-1},
  shorttitle =	 {hypre},
  url =
                  {http://link.springer.com/chapter/10.1007/3-540-47789-6_66},
  abstract =	 {hypre is a software library for the solution of
                  large, sparse linear systems on massively parallel
                  computers. Its emphasis is on modern powerful and
                  scalable preconditioners. hypre provides various
                  conceptual interfaces to enable application users to
                  access the library in the way they naturally think
                  about their problems. This paper presents the
                  conceptual interfaces in hypre. An overview of the
                  preconditioners that are available in hypre is
                  given, including some numerical results that show
                  the efficiency of the library.},
  language =	 {en},
  number =	 2331,
  urldate =	 {2014-05-15},
  booktitle =	 {Computational Science -- {ICCS} 2002},
  publisher =	 {Springer Berlin Heidelberg},
  author =	 {Falgout, Robert D. and Yang, Ulrike Meier},
  editor =	 {Sloot, Peter M. A. and Hoekstra, Alfons G. and Tan,
                  C. J. Kenneth and Dongarra, Jack J.},
  year =	 2002,
  keywords =	 {Computational Mathematics and Numerical Analysis,
                  Computer Communication Networks, Mathematical and
                  Computational Physics, Mathematics of Computing,
                  Software {Engineering/Programming} and Operating
                  Systems, Theory of Computation},
  pages =	 {632--641},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/QIZ4MBJG/Falgout
                  and Yang - 2002 - hypre A Library of High
                  Performance
                  Preconditione.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/J36JJTMJ/10.html:text/html}
}

@incollection{falgout_hypre:_2002,
  series =	 {Lecture Notes in Computer Science},
  title =	 {hypre: A Library of High Performance
                  Preconditioners},
  copyright =	 {©2002 Springer-Verlag Berlin Heidelberg},
  isbn =	 {978-3-540-43594-5, 978-3-540-47789-1},
  shorttitle =	 {hypre},
  url =
                  {http://link.springer.com/chapter/10.1007/3-540-47789-6_66},
  abstract =	 {hypre is a software library for the solution of
                  large, sparse linear systems on massively parallel
                  computers. Its emphasis is on modern powerful and
                  scalable preconditioners. hypre provides various
                  conceptual interfaces to enable application users to
                  access the library in the way they naturally think
                  about their problems. This paper presents the
                  conceptual interfaces in hypre. An overview of the
                  preconditioners that are available in hypre is
                  given, including some numerical results that show
                  the efficiency of the library.},
  language =	 {en},
  number =	 2331,
  urldate =	 {2014-05-15},
  booktitle =	 {Computational Science — {ICCS} 2002},
  publisher =	 {Springer Berlin Heidelberg},
  author =	 {Falgout, Robert D. and Yang, Ulrike Meier},
  editor =	 {Sloot, Peter M. A. and Hoekstra, Alfons G. and Tan,
                  C. J. Kenneth and Dongarra, Jack J.},
  year =	 2002,
  keywords =	 {Computational Mathematics and Numerical Analysis,
                  Computer Communication Networks, Mathematical and
                  Computational Physics, Mathematics of Computing,
                  Software {Engineering/Programming} and Operating
                  Systems, Theory of Computation},
  pages =	 {632--641},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/QIZ4MBJG/Falgout
                  and Yang - 2002 - hypre A Library of High
                  Performance
                  Preconditione.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/J36JJTMJ/10.html:text/html}
}

@article{falgout_parallel_2014,
  title =	 {Parallel time integration with multigrid},
  volume =	 36,
  issn =	 {1064-8275},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/130944230},
  doi =		 {10.1137/130944230},
  abstract =	 {We consider optimal-scaling multigrid solvers for
                  the linear systems that arise from the
                  discretization of problems with evolutionary
                  behavior. Typically, solution algorithms for
                  evolution equations are based on a time-marching
                  approach, solving sequentially for one time step
                  after the other. Parallelism in these traditional
                  time-integration techniques is limited to spatial
                  parallelism. However, current trends in computer
                  architectures are leading toward systems with more,
                  but not faster, processors. Therefore, faster
                  compute speeds must come from greater
                  parallelism. One approach to achieving parallelism
                  in time is with multigrid, but extending classical
                  multigrid methods for elliptic operators to this
                  setting is not straightforward. In this paper, we
                  present a nonintrusive, optimal-scaling
                  time-parallel method based on multigrid reduction
                  (MGR). We demonstrate optimality of our
                  multigrid-reduction-in-time algorithm (MGRIT) for
                  solving diffusion equations in two and three space
                  dimensions in numerical experiments. Furthermore,
                  through both parallel performance models and actual
                  parallel numerical results, we show that we can
                  achieve significant speedup in comparison to
                  sequential time marching on modern architectures.,
                  We consider optimal-scaling multigrid solvers for
                  the linear systems that arise from the
                  discretization of problems with evolutionary
                  behavior. Typically, solution algorithms for
                  evolution equations are based on a time-marching
                  approach, solving sequentially for one time step
                  after the other. Parallelism in these traditional
                  time-integration techniques is limited to spatial
                  parallelism. However, current trends in computer
                  architectures are leading toward systems with more,
                  but not faster, processors. Therefore, faster
                  compute speeds must come from greater
                  parallelism. One approach to achieving parallelism
                  in time is with multigrid, but extending classical
                  multigrid methods for elliptic operators to this
                  setting is not straightforward. In this paper, we
                  present a nonintrusive, optimal-scaling
                  time-parallel method based on multigrid reduction
                  (MGR). We demonstrate optimality of our
                  multigrid-reduction-in-time algorithm (MGRIT) for
                  solving diffusion equations in two and three space
                  dimensions in numerical experiments. Furthermore,
                  through both parallel performance models and actual
                  parallel numerical results, we show that we can
                  achieve significant speedup in comparison to
                  sequential time marching on modern architectures.},
  number =	 6,
  urldate =	 {2014-12-24},
  journal =	 {SIAM Journal on Scientific Computing},
  author =	 {Falgout, R. and Friedhoff, S. and Kolev, T. and
                  MacLachlan, S. and Schroder, J.},
  year =	 2014,
  pages =	 {C635--C661},
  file =	 {Full Text
                  PDF:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/XVE4MQ3K/Falgout
                  et al. - 2014 - Parallel Time Integration with
                  Multigrid.pdf:application/pdf;Snapshot:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/ZR64PGNF/130944230.html:text/html}
},

@article{farhat_feti-dp:_2001,
  title =	 {{FETI-DP:} a dual-primal unified {FETI} method-part
                  I: A faster alternative to the two-level {FETI}
                  method},
  volume =	 50,
  copyright =	 {Copyright © 2001 John Wiley \& Sons, Ltd.},
  issn =	 {1097-0207},
  shorttitle =	 {{FETI-DP}},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nme.76/abstract},
  doi =		 {10.1002/nme.76},
  abstract =	 {The {FETI} method and its two-level extension
                  ({FETI-2)} are two numerically scalable domain
                  decomposition methods with Lagrange multipliers for
                  the iterative solution of second-order solid
                  mechanics and fourth-order beam, plate and shell
                  structural problems, {respectively.The} {FETI-2}
                  method distinguishes itself from the basic or
                  one-level {FETI} method by a second set of Lagrange
                  multipliers that are introduced at the subdomain
                  cross-points to enforce at each iteration the exact
                  continuity of a subset of the displacement field at
                  these specific locations. In this paper, we present
                  a dual-primal formulation of the {FETI-2} concept
                  that eliminates the need for that second set of
                  Lagrange multipliers, and unifies all previously
                  developed one-level and two-level {FETI} algorithms
                  into a single dual-primal {FETI-DP} method. We show
                  that this new {FETI-DP} method is numerically
                  scalable for both second-order and fourth-order
                  problems. We also show that it is more robust and
                  more computationally efficient than existing {FETI}
                  solvers, particularly when the number of subdomains
                  and/or processors is very large. Copyright © 2001
                  John Wiley \& Sons, Ltd.},
  language =	 {en},
  number =	 7,
  urldate =	 {2013-01-18},
  journal =	 {International Journal for Numerical Methods in
                  Engineering},
  author =	 {Farhat, Charbel and Lesoinne, Michel and {LeTallec},
                  Patrick and Pierson, Kendall and Rixen, Daniel},
  year =	 2001,
  keywords =	 {domain decomposition, Iterative methods, numerical
                  scalability},
  pages =	 {1523--1544},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/AS6VDICE/Farhat
                  et al. - 2001 - FETI-DP a dual-primal unified FETI
                  method—part
                  I.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/GJH53U89/abstract.html:text/html}
}

@article{farhat_feti-dp:_2001,
  title =	 {{FETI-DP:} a dual-primal unified {FETI} method—part
                  I: A faster alternative to the two-level {FETI}
                  method},
  volume =	 50,
  copyright =	 {Copyright © 2001 John Wiley \& Sons, Ltd.},
  issn =	 {1097-0207},
  shorttitle =	 {{FETI-DP}},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nme.76/abstract},
  doi =		 {10.1002/nme.76},
  abstract =	 {The {FETI} method and its two-level extension
                  ({FETI-2)} are two numerically scalable domain
                  decomposition methods with Lagrange multipliers for
                  the iterative solution of second-order solid
                  mechanics and fourth-order beam, plate and shell
                  structural problems, {respectively.The} {FETI-2}
                  method distinguishes itself from the basic or
                  one-level {FETI} method by a second set of Lagrange
                  multipliers that are introduced at the subdomain
                  cross-points to enforce at each iteration the exact
                  continuity of a subset of the displacement field at
                  these specific locations. In this paper, we present
                  a dual-primal formulation of the {FETI-2} concept
                  that eliminates the need for that second set of
                  Lagrange multipliers, and unifies all previously
                  developed one-level and two-level {FETI} algorithms
                  into a single dual-primal {FETI-DP} method. We show
                  that this new {FETI-DP} method is numerically
                  scalable for both second-order and fourth-order
                  problems. We also show that it is more robust and
                  more computationally efficient than existing {FETI}
                  solvers, particularly when the number of subdomains
                  and/or processors is very large. Copyright © 2001
                  John Wiley \& Sons, Ltd.},
  language =	 {en},
  number =	 7,
  urldate =	 {2013-01-18},
  journal =	 {International Journal for Numerical Methods in
                  Engineering},
  author =	 {Farhat, Charbel and Lesoinne, Michel and {LeTallec},
                  Patrick and Pierson, Kendall and Rixen, Daniel},
  year =	 2001,
  keywords =	 {domain decomposition, Iterative methods, numerical
                  scalability},
  pages =	 {1523--1544},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/AS6VDICE/Farhat
                  et al. - 2001 - FETI-DP a dual-primal unified FETI
                  method—part
                  I.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/GJH53U89/abstract.html:text/html}
}

@misc{fempar,
  title =	 {\texttt{FEMPAR}: Finite Element Multiphysics PARallel solvers},
  howpublished = {\url{https://www.fempar.org}},
  urldate =	 {2017-01-11}
}

@misc{damask,
  title =	 {\texttt{DAMASK} -- the {D}üsseldorf Advanced Material Simulation Kit},
  howpublished = {\url{https://damask.mpie.de/Home/WebHome}},
  urldate =	 {2017-01-11}
}

@ARTICLE{fempar-am,
  author =	 {Badia, S. and Chiumenti, M. and Mart\'in, A. F. and Neiva, E.},
  title =	 {\texttt{FEMPAR-AM}: A parallel framework for the simulation of metal additive manufacturing},
  journal =	 {In preparation},
  year =	 2018
}

@article{fempar-manual,
	title = {{FEMPAR}: {An} {Object}-{Oriented} {Parallel} {Finite} {Element} {Framework}},
	issn = {1134-3060, 1886-1784},
	shorttitle = {{FEMPAR}},
	url = {https://link.springer.com/article/10.1007/s11831-017-9244-1},
	doi = {10.1007/s11831-017-9244-1},
	abstract = {FEMPAR is an open source object oriented Fortran200X scientific software library for the high-performance scalable simulation of complex multiphysics problems governed by partial differential equations at large scales, by exploiting state-of-the-art supercomputing resources. It is a highly modularized, flexible, and extensible library, that provides a set of modules that can be combined to carry out the different steps of the simulation pipeline. FEMPAR includes a rich set of algorithms for the discretization step, namely (arbitrary-order) grad, div, and curl-conforming finite element methods, discontinuous Galerkin methods, B-splines, and unfitted finite element techniques on cut cells, combined with h-adaptivity. The linear solver module relies on state-of-the-art bulk-asynchronous implementations of multilevel domain decomposition solvers for the different discretization alternatives and block-preconditioning techniques for multiphysics problems. FEMPAR is a framework that provides users with out-of-the-box state-of-the-art discretization techniques and highly scalable solvers for the simulation of complex applications, hiding the dramatic complexity of the underlying algorithms. But it is also a framework for researchers that want to experience with new algorithms and solvers, by providing a highly extensible framework. In this work, the first one in a series of articles about FEMPAR, we provide a detailed introduction to the software abstractions used in the discretization module and the related geometrical module. We also provide some ingredients about the assembly of linear systems arising from finite element discretizations, but the software design of complex scalable multilevel solvers is postponed to a subsequent work.},
	language = {en},
	urldate = {2018-02-05},
	journal = {Archives of Computational Methods in Engineering},
	author = {Badia, S. and Martín, A. F. and Principe, J.},
	year = {2017},
	pages = {1--77},
	file = {Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/67TVEWJJ/Badia et al. - 2017 - FEMPAR An Object-Oriented Parallel Finite Element.pdf:application/pdf;Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/MTYJV2BX/s11831-017-9244-1.html:text/html}
}


@article{fish_concurrent_2007,
  title =	 {Concurrent {AtC} coupling based on a blend of the
                  continuum stress and the atomistic force},
  volume =	 196,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782507002277},
  doi =		 {10.1016/j.cma.2007.05.020},
  abstract =	 {A concurrent atomistic to continuum (AtC) coupling
                  method is presented in this paper. The problem
                  domain is decomposed into an atomistic sub-domain
                  where fine scale features need to be resolved, a
                  continuum sub-domain which can adequately describe
                  the macroscale deformation and an overlap interphase
                  sub-domain that has a blended description of the
                  two. The problem is formulated in terms of
                  equilibrium equations with a blending between the
                  continuum stress and the atomistic force in the
                  interphase. Coupling between the continuum and the
                  atomistics is established by imposing constraints
                  between the continuum solution and the atomistic
                  solution over the interphase sub-domain in a weak
                  sense. Specifically, in the examples considered
                  here, the atomistic domain is modeled by the
                  aluminum embedded atom method (EAM) inter-atomic
                  potential developed by Ercolessi and Adams
                  [F. Ercolessi, J.B. Adams, Interatomic potentials
                  from first-principles calculations: the
                  force-matching method, Europhys. Lett. 26 (1994)
                  583] and the continuum domain is a linear elastic
                  model consistent with the EAM potential. The
                  formulation is subjected to patch tests to
                  demonstrate its ability to represent the constant
                  strain modes and the rigid body modes. Numerical
                  examples are illustrated with comparisons to
                  reference atomistic solution.},
  number =	 {45--48},
  urldate =	 {2012-02-10},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Fish, Jacob and Nuggehally, Mohan A. and Shephard,
                  Mark S. and Picu, Catalin R. and Badia, Santiago and
                  Parks, Michael L. and Gunzburger, Max},
  year =	 2007,
  keywords =	 {Atomistic to continuum coupling, Concurrent
                  multiscale, Overlap domain decomposition},
  pages =	 {4548--4560}
}

@article{francois_recent_2015,
  series =	 {{IUTAM} {Symposium} on {Multiphase} {Flows} with
                  {Phase} {Change}: {Challenges} and {Opportunities}},
  title =	 {Recent {Numerical} and {Algorithmic} {Advances}
                  within the {Volume} {Tracking} {Framework} for
                  {Modeling} {Interfacial} {Flows}},
  volume =	 15,
  issn =	 {2210-9838},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S2210983815001480},
  doi =		 {10.1016/j.piutam.2015.04.037},
  abstract =	 {A review of recent advances made in numerical
                  methods and algorithmswithin the volume tracking
                  framework is presented. The volume tracking method,
                  also known as the volume-of-fluid method has become
                  an established numerical approach to model and
                  simulate interfacial flows. Its advantage is its
                  strict mass conservation. However, because the
                  interface is not explicitly tracked but captured via
                  the material volume fraction on a fixed mesh,
                  accurate estimation of the interface position, its
                  geometric properties andmodeling of interfacial
                  physics in the volume tracking framework
                  remaindifficult. Several improvements have been made
                  over the last decade to address these challenges. In
                  this paper, the multimaterial interface
                  reconstruction method via power diagram, curvature
                  estimation via heights and mean values and the
                  balanced-force algorithm for surface tension are
                  highlighted.},
  urldate =	 {2015-12-29},
  journal =	 {Procedia IUTAM},
  author =	 {Fran\c{c}ois, Marianne M.},
  year =	 2015,
  keywords =	 {curvature, interface reconstruction, Surface
                  tension, volume-of-fluid, volume tracking},
  pages =	 {270--277},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/VMDVHWMI/François
                  - 2015 - Recent Numerical and Algorithmic Advances
                  within t.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/7ZH2PZQ9/S2210983815001480.html:text/html}
}

%% hal-00839487, version 2
%% http://hal.archives-ouvertes.fr/hal-00839487

@Manual{freefem++_software,
  author =	 {Fr\'ederich Hecht},
  title =	 {FreeFem++ User's manual. 3rd edition, {V}ersion
                  3.22},
  url =		 {http://www.freefem.org/ff++/ftp/freefem++doc.pdf},
  note =	 "Available at
                  \url{http://www.freefem.org/ff++/ftp/freefem++doc.pdf}",
  institution =	 {{L}aboratoire {J}acques-{L}ouis {L}ions,
                  {U}niversité {P}ierre et {M}arie {C}urie, {P}aris},
  year =	 2013,
}

@Manual{freefem++_software,
  author =	 {Fr\'ederich Hecht},
  title =	 {FreeFem++ User's manual. 3rd edition, {V}ersion
                  3.22},
  url =		 {http://www.freefem.org/ff++/ftp/freefem++doc.pdf},
  note =	 "Available at
                  \url{http://www.freefem.org/ff++/ftp/freefem++doc.pdf}",
  institution =	 {{L}aboratoire {J}acques-{L}ouis {L}ions,
                  {U}niversité {P}ierre et {M}arie {C}urie, {P}aris},
  year =	 2013,
}

@article{galvis_domain_2010,
  title =	 {Domain {Decomposition} {Preconditioners} for
                  {Multiscale} {Flows} in {High}-{Contrast} {Media}},
  volume =	 8,
  issn =	 {1540-3459},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/090751190},
  doi =		 {10.1137/090751190},
  abstract =	 {In this paper, we study domain decomposition
                  preconditioners for multiscale flows in
                  high-contrast media. We consider flow equations
                  governed by elliptic equations in heterogeneous
                  media with a large contrast in the coefficients. Our
                  main goal is to develop domain decomposition
                  preconditioners with the condition number that is
                  independent of the contrast when there are
                  variations within coarse regions. This is
                  accomplished by designing coarse-scale spaces and
                  interpolators that represent important features of
                  the solution within each coarse region. The
                  important features are characterized by the
                  connectivities of high-conductivity regions. To
                  detect these connectivities, we introduce an
                  eigenvalue problem that automatically detects
                  high-conductivity regions via a large gap in the
                  spectrum. A main observation is that this eigenvalue
                  problem has a few small, asymptotically vanishing
                  eigenvalues. The number of these small eigenvalues
                  is the same as the number of connected
                  high-conductivity regions. The coarse spaces are
                  constructed such that they span eigenfunctions
                  corresponding to these small eigenvalues. These
                  spaces are used within two-level additive Schwarz
                  preconditioners as well as overlapping methods for
                  the Schur complement to design preconditioners. We
                  show that the condition number of the preconditioned
                  systems is independent of the contrast. More
                  detailed studies are performed for the case when the
                  high-conductivity region is connected within coarse
                  block neighborhoods. Our numerical experiments
                  confirm the theoretical results presented in this
                  paper., In this paper, we study domain decomposition
                  preconditioners for multiscale flows in
                  high-contrast media. We consider flow equations
                  governed by elliptic equations in heterogeneous
                  media with a large contrast in the coefficients. Our
                  main goal is to develop domain decomposition
                  preconditioners with the condition number that is
                  independent of the contrast when there are
                  variations within coarse regions. This is
                  accomplished by designing coarse-scale spaces and
                  interpolators that represent important features of
                  the solution within each coarse region. The
                  important features are characterized by the
                  connectivities of high-conductivity regions. To
                  detect these connectivities, we introduce an
                  eigenvalue problem that automatically detects
                  high-conductivity regions via a large gap in the
                  spectrum. A main observation is that this eigenvalue
                  problem has a few small, asymptotically vanishing
                  eigenvalues. The number of these small eigenvalues
                  is the same as the number of connected
                  high-conductivity regions. The coarse spaces are
                  constructed such that they span eigenfunctions
                  corresponding to these small eigenvalues. These
                  spaces are used within two-level additive Schwarz
                  preconditioners as well as overlapping methods for
                  the Schur complement to design preconditioners. We
                  show that the condition number of the preconditioned
                  systems is independent of the contrast. More
                  detailed studies are performed for the case when the
                  high-conductivity region is connected within coarse
                  block neighborhoods. Our numerical experiments
                  confirm the theoretical results presented in this
                  paper.},
  number =	 4,
  urldate =	 {2014-10-06},
  journal =	 {Multiscale Modeling \& Simulation},
  author =	 {Galvis, J. and Efendiev, Y.},
  year =	 2010,
  pages =	 {1461--1483},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/56B2ASN3/Galvis
                  and Efendiev - 2010 - Domain Decomposition
                  Preconditioners for
                  Multiscal.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/HPZZHXTE/090751190.html:text/html}
}

@ARTICLE{gander_analysis_2007,
  author =	 {Gander, Martin J. and Vandewalle, Stefan},
  title =	 {Analysis of the Parareal {Time}--{Parallel}
                  {Time}--{Integration} Method},
  journal =	 {{SIAM} Journal on Scientific Computing},
  year =	 2007,
  volume =	 29,
  pages =	 {556--578},
  number =	 2,
  doi =		 {10.1137/05064607X},
  file =	 {Analysis of the Parareal TimeâParallel
                  TimeâIntegration Method : SIAM Journal on
                  Scientific Computing: Vol. 29, No. 2 (Society for
                  Industrial and Applied
                  Mathematics):/home/principe/Javier/System/Config/firefox/zotero/storage/C528TNRQ/05064607X.html:text/html},
  issn =	 {1064-8275, 1095-7197},
  language =	 {en},
  owner =	 {principe},
  timestamp =	 {2014.03.22},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/05064607X},
  urldate =	 {2014-03-22}
}

@ARTICLE{gander_analysis_2007,
  author =	 {Gander, Martin J. and Vandewalle, Stefan},
  title =	 {Analysis of the Parareal {Time}--{Parallel}
                  {Time}--{Integration} Method},
  journal =	 {{SIAM} Journal on Scientific Computing},
  year =	 2007,
  volume =	 29,
  pages =	 {556--578},
  number =	 2,
  doi =		 {10.1137/05064607X},
  file =	 {Analysis of the Parareal TimeâParallel
                  TimeâIntegration Method : SIAM Journal on
                  Scientific Computing: Vol. 29, No. 2 (Society for
                  Industrial and Applied
                  Mathematics):/home/principe/Javier/System/Config/firefox/zotero/storage/C528TNRQ/05064607X.html:text/html},
  issn =	 {1064-8275, 1095-7197},
  language =	 {en},
  owner =	 {principe},
  timestamp =	 {2014.03.22},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/05064607X},
  urldate =	 {2014-03-22}
}

@article{gander_paraexp:_2013,
  title =	 {{ParaExp}: A Parallel Integrator for Linear
                  Initial--Value Problems},
  volume =	 35,
  issn =	 {1064-8275},
  url =		 {http://dx.doi.org/10.1137/110856137},
  doi =		 {10.1137/110856137},
  abstract =	 {A novel parallel algorithm for the integration of
                  linear initial-value problems is proposed. This
                  algorithm is based on the simple observation that
                  homogeneous problems can typically be integrated
                  much faster than inhomogeneous problems. An
                  overlapping time-domain decomposition is utilized to
                  obtain decoupled inhomogeneous and homogeneous
                  subproblems, and a near-optimal Krylov method is
                  used for the fast exponential integration of the
                  homogeneous subproblems. We present an error
                  analysis and discuss the parallel scaling of our
                  algorithm. The efficiency of this approach is
                  demonstrated with numerical examples.},
  number =	 2,
  journal =	 {SIAM J. Sci. Comput.},
  author =	 {Gander, Martin J. and G\"{u}ttel, Stefan},
  year =	 2013,
  pages =	 {C123--C142}
}

@incollection{gander_parareal_2013,
  series =	 {Lecture {Notes} in {Computational} {Science} and
                  {Engineering}},
  title =	 {Parareal {Schwarz} Waveform Relaxation Methods},
  volume =	 91,
  isbn =	 {978-3-642-35275-1},
  abstract =	 {Domain Decomposition Methods in Science and
                  Engineering XX},
  booktitle =	 {Domain {Decomposition} {Methods} in {Science} and
                  {Engineering} {XX}},
  publisher =	 {Springer},
  author =	 {Gander, Martin J. and Li, Rong-Jian and Jiang,
                  Yao-Lin},
  editor =	 {Bank, Randolph and Holst, Michael and Widlund, Olof
                  and Xu, Jinchao},
  year =	 2013,
  pages =	 {451--458}
}

@article{ganeriwala_multiphysics_2014,
  series =	 {6th {CIRP} {International} {Conference} on {High}
                  {Performance} {Cutting}, {HPC}2014},
  title =	 {Multiphysics {Modeling} and {Simulation} of
                  {Selective} {Laser} {Sintering} {Manufacturing}
                  {Processes}},
  volume =	 14,
  issn =	 {2212-8271},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S2212827114001589},
  doi =		 {10.1016/j.procir.2014.03.015},
  abstract =	 {A significant percentage of materials used in
                  industry start in particulate form. In many modern
                  applications, these systems undergo processing which
                  necessitate a multiphysical analysis. Several
                  manufacturing applications have arisen that involve
                  the multiphysical response of particulate systems in
                  the presence of strongly coupled electromagnetic,
                  optical, and thermal fields. The significant
                  multifield coupling requires methods that can
                  capture the unique and essential physics of these
                  systems. Specifically, in this work, the modeling
                  and simulation of selective laser sintering of
                  particulate materials is discussed. Such processes
                  involve harnessing optical energy to heat and fuse
                  powdered materials together in an additive
                  process. Selective laser sintering allows for the
                  rapid manufacturing or prototyping of parts with
                  complex geometries. In order to simulate such a
                  process in a rapid manner, the approach pursued by
                  the authors is to develop a computational tool by
                  assembling relatively simple, physically meaningful,
                  models at the small scale, for many interacting
                  particles. This allows for much more refined
                  estimates of the resulting overall system
                  temperature and, ultimately, its change of phase
                  from a solid, to a liquid, and possibly even to a
                  gas. Large-scale three-dimensional examples are
                  provided.},
  urldate =	 {2015-12-29},
  journal =	 {Procedia CIRP},
  author =	 {Ganeriwala, Rishi and Zohdi, Tarek I.},
  year =	 2014,
  keywords =	 {additive manufacturing, rapid manufacturing, rapid
                  protoyping, selective laser sintering},
  pages =	 {299--304},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/5XQXU776/Ganeriwala
                  and Zohdi - 2014 - Multiphysics Modeling and
                  Simulation of Selective
                  .pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/EIBRIF62/S2212827114001589.html:text/html}
}

@article{geers_multi-scale_2010,
  series =	 {Fourth {International} {Conference} on {Advanced}
                  {COmputational} {Methods} in {ENgineering} ({ACOMEN}
                  2008)},
  title =	 {Multi-scale computational homogenization: {Trends}
                  and challenges},
  volume =	 234,
  issn =	 {0377-0427},
  shorttitle =	 {Multi-scale computational homogenization},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0377042709005536},
  doi =		 {10.1016/j.cam.2009.08.077},
  abstract =	 {In the past decades, considerable progress had been
                  made in bridging the mechanics of materials to other
                  disciplines, e.g. downscaling to the field of
                  materials science or upscaling to the field of
                  structural engineering. Within this wide context,
                  this paper reviews the state-of-the-art of a
                  particular, yet powerful, method, i.e. computational
                  homogenization. The paper discusses the main trends
                  since the early developments of this approach up to
                  the ongoing contributions and upcoming challenges in
                  the field.},
  number =	 7,
  urldate =	 {2016-01-04},
  journal =	 {Journal of Computational and Applied Mathematics},
  author =	 {Geers, M. G. D. and Kouznetsova, V. G. and
                  Brekelmans, W. A. M.},
  year =	 2010,
  keywords =	 {coarse graining, Homogenization, multi-scale},
  pages =	 {2175--2182},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/IAWZS56T/Geers
                  et al. - 2010 - Multi-scale computational
                  homogenization Trends
                  a.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/I9RNFQPN/S0377042709005536.html:text/html}
}

@article{geuzaine_gmsh:_2009,
  title =	 {Gmsh: A 3-D finite element mesh generator with
                  built-in pre- and post-processing facilities},
  volume =	 79,
  copyright =	 {Copyright © 2009 John Wiley \& Sons, Ltd.},
  issn =	 {1097-0207},
  shorttitle =	 {Gmsh},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nme.2579/abstract},
  doi =		 {10.1002/nme.2579},
  abstract =	 {Gmsh is an open-source 3-D finite element grid
                  generator with a build-in {CAD} engine and
                  post-processor. Its design goal is to provide a
                  fast, light and user-friendly meshing tool with
                  parametric input and advanced visualization
                  capabilities. This paper presents the overall
                  philosophy, the main design choices and some of the
                  original algorithms implemented in Gmsh. Copyright ©
                  2009 John Wiley \& Sons, Ltd.},
  language =	 {en},
  number =	 11,
  urldate =	 {2014-05-15},
  journal =	 {International Journal for Numerical Methods in
                  Engineering},
  author =	 {Geuzaine, Christophe and Remacle, Jean-Fran\c{c}ois},
  year =	 2009,
  keywords =	 {computer-aided design, finite element method, mesh
                  generation, post-processing, open-source software},
  pages =	 {1309--1331},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/KWZN4VT5/Geuzaine
                  and Remacle - 2009 - Gmsh A 3-D finite element mesh
                  generator with
                  bui.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/SUFIUHC6/abstract.html:text/html}
}

@article{geuzaine_gmsh:_2009,
  title =	 {Gmsh: A {3-D} finite element mesh generator with
                  built-in pre- and post-processing facilities},
  volume =	 79,
  copyright =	 {Copyright © 2009 John Wiley \& Sons, Ltd.},
  issn =	 {1097-0207},
  shorttitle =	 {Gmsh},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nme.2579/abstract},
  doi =		 {10.1002/nme.2579},
  abstract =	 {Gmsh is an open-source 3-D finite element grid
                  generator with a build-in {CAD} engine and
                  post-processor. Its design goal is to provide a
                  fast, light and user-friendly meshing tool with
                  parametric input and advanced visualization
                  capabilities. This paper presents the overall
                  philosophy, the main design choices and some of the
                  original algorithms implemented in Gmsh. Copyright ©
                  2009 John Wiley \& Sons, Ltd.},
  language =	 {en},
  number =	 11,
  urldate =	 {2014-05-15},
  journal =	 {International Journal for Numerical Methods in
                  Engineering},
  author =	 {Geuzaine, Christophe and Remacle, Jean-Fran\c{c}ois},
  year =	 2009,
  keywords =	 {computer-aided design, finite element method, mesh
                  generation, post-processing, open-source software},
  pages =	 {1309--1331},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/KWZN4VT5/Geuzaine
                  and Remacle - 2009 - Gmsh A 3-D finite element mesh
                  generator with
                  bui.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/SUFIUHC6/abstract.html:text/html}
}

@article{giannelli_strongly_2014,
	title = {Strongly stable bases for adaptively refined multilevel spline spaces},
	volume = {40},
	issn = {1019-7168, 1572-9044},
	url = {http://link.springer.com.recursos.biblioteca.upc.edu/article/10.1007/s10444-013-9315-2},
	doi = {10.1007/s10444-013-9315-2},
	abstract = {The problem of constructing a normalized hierarchical basis for adaptively refined spline spaces is addressed. Multilevel representations are defined in terms of a hierarchy of basis functions, reflecting different levels of refinement. When the hierarchical model is constructed by considering an underlying sequence of bases \{Γℓ\}ℓ=0,…,N−1\{Γℓ\}ℓ=0,…,N−1{\textbackslash}\{{\textbackslash}Gamma {\textasciicircum}\{{\textbackslash}ell \}{\textbackslash}\}\_\{{\textbackslash}ell =0,{\textbackslash}ldots ,N-1\} with properties analogous to classical tensor-product B-splines, we can define a set of locally supported basis functions that form a partition of unity and possess the property of coefficient preservation, i.e., they preserve the coefficients of functions represented with respect to one of the bases ΓℓΓℓ{\textbackslash}Gamma {\textasciicircum}\{{\textbackslash}ell \}. Our construction relies on a certain truncation procedure, which eliminates the contributions of functions from finer levels in the hierarchy to coarser level ones. Consequently, the support of the original basis functions defined on coarse grids is possibly reduced according to finer levels in the hierarchy. This truncation mechanism not only decreases the overlapping of basis supports, but it also guarantees strong stability of the construction. In addition to presenting the theory for the general framework, we apply it to hierarchically refined tensor-product spline spaces, under certain reasonable assumptions on the given knot configuration.},
	pages = {459--490},
	number = {2},
        journal      = {Advances in Computational Mathematics},
	journaltitle = {Advances in Computational Mathematics},
	shortjournal = {Adv Comput Math},
	author = {Giannelli, Carlotta and Jüttler, Bert and Speleers, Hendrik},
        year    = {2014},
	urldate = {2017-01-02},
	date = {2014-04-01},
	langid = {english},
	file = {Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/72ZAN73X/Giannelli et al. - 2014 - Strongly stable bases for adaptively refined multi.pdf:application/pdf;Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/T24MPR5G/s10444-013-9315-2.html:text/html}
}

@article{giles_multilevel_2008,
  title =	 {Multilevel {Monte} {Carlo} {Path} {Simulation}},
  volume =	 56,
  issn =	 {0030-364X},
  url =
                  {http://pubsonline.informs.org/doi/abs/10.1287/opre.1070.0496},
  doi =		 {10.1287/opre.1070.0496},
  abstract =	 {We show that multigrid ideas can be used to reduce
                  the computational complexity of estimating an
                  expected value arising from a stochastic
                  differential equation using Monte Carlo path
                  simulations. In the simplest case of a Lipschitz
                  payoff and a Euler discretisation, the computational
                  cost to achieve an accuracy of O(ϵ) is reduced from
                  O(ϵ−3) to O(ϵ−2 (log ϵ)2). The analysis is supported
                  by numerical results showing significant
                  computational savings.},
  number =	 3,
  urldate =	 {2016-01-15},
  journal =	 {Operations Research},
  author =	 {Giles, Michael B.},
  year =	 2008,
  pages =	 {607--617},
  file =
                  {Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/RGRHVSKF/opre.1070.html:text/html}
}

@article{giraud_parallel_2008,
  series =	 {Parallel Matrix Algorithms and Applications},
  title =	 {Parallel scalability study of hybrid preconditioners
                  in three dimensions},
  volume =	 34,
  issn =	 {0167-8191},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0167819108000197},
  doi =		 {10.1016/j.parco.2008.01.006},
  abstract =	 {In this paper we study the parallel scalability of
                  variants of additive Schwarz preconditioners for
                  three dimensional non-overlapping domain
                  decomposition methods. To alleviate the
                  computational cost, both in terms of memory and
                  floating-point complexity, we investigate variants
                  based on a sparse approximation or on mixed 32- and
                  64-bit calculation. The robustness of the
                  preconditioners is illustrated on a set of linear
                  systems arising from the finite element
                  discretization of elliptic {PDEs} through extensive
                  parallel experiments on up to 1000 processors. Their
                  efficiency from a numerical and parallel performance
                  view point are studied.},
  number =	 {6--8},
  urldate =	 {2014-05-15},
  journal =	 {Parallel Computing},
  author =	 {Giraud, L. and Haidar, A. and Watson, L. T.},
  year =	 2008,
  keywords =	 {Elliptic {PDE}, Iterative methods, Large sparse
                  linear systems, parallel computing, Preconditioned
                  Krylov solvers, Sparse direct solvers},
  pages =	 {363--379},
  file =	 {ScienceDirect Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/V4CUJAM7/Giraud
                  et al. - 2008 - Parallel scalability study of hybrid
                  preconditione.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/UGKVGAHN/S0167819108000197.html:text/html}
}

@article{giraud_parallel_2008,
  series =	 {Parallel Matrix Algorithms and Applications},
  title =	 {Parallel scalability study of hybrid preconditioners
                  in three dimensions},
  volume =	 34,
  issn =	 {0167-8191},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0167819108000197},
  doi =		 {10.1016/j.parco.2008.01.006},
  abstract =	 {In this paper we study the parallel scalability of
                  variants of additive Schwarz preconditioners for
                  three dimensional non-overlapping domain
                  decomposition methods. To alleviate the
                  computational cost, both in terms of memory and
                  floating-point complexity, we investigate variants
                  based on a sparse approximation or on mixed 32- and
                  64-bit calculation. The robustness of the
                  preconditioners is illustrated on a set of linear
                  systems arising from the finite element
                  discretization of elliptic {PDEs} through extensive
                  parallel experiments on up to 1000 processors. Their
                  efficiency from a numerical and parallel performance
                  view point are studied.},
  number =	 {6--8},
  urldate =	 {2014-05-15},
  journal =	 {Parallel Computing},
  author =	 {Giraud, L. and Haidar, A. and Watson, L. T.},
  year =	 2008,
  keywords =	 {Elliptic {PDE}, Iterative methods, Large sparse
                  linear systems, parallel computing, Preconditioned
                  Krylov solvers, Sparse direct solvers},
  pages =	 {363--379},
  file =	 {ScienceDirect Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/V4CUJAM7/Giraud
                  et al. - 2008 - Parallel scalability study of hybrid
                  preconditione.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/UGKVGAHN/S0167819108000197.html:text/html}
}

@article{goldak_new_1984,
  title =	 {A new finite element model for welding heat sources},
  volume =	 15,
  issn =	 {0360-2141, 1543-1916},
  url =		 {http://link.springer.com/article/10.1007/BF02667333},
  doi =		 {10.1007/BF02667333},
  abstract =	 {A mathematical model for weld heat sources based on
                  a Gaussian distribution of power density in space is
                  presented. In particular a double ellipsoidal
                  geometry is proposed so that the size and shape of
                  the heat source can be easily changed to model both
                  the shallow penetration arc welding processes and
                  the deeper penetration laser and electron beam
                  processes. In addition, it has the versatility and
                  flexibility to handle non-axisymmetric cases such as
                  strip electrodes or dissimilar metal
                  joining. Previous models assumed circular or
                  spherical symmetry. The computations are performed
                  with ASGARD, a nonlinear transient finite element
                  (FEM) heat flow program developed for the thermal
                  stress analysis of welds.* Computed temperature
                  distributions for submerged arc welds in thick
                  workpieces are compared to the measured values
                  reported by Christensen1 and the FEM calculated
                  values (surface heat source model) of Krutz and
                  Segerlind.2 In addition the computed thermal history
                  of deep penetration electron beam welds are compared
                  to measured values reported by Chong.3 The agreement
                  between the computed and measured values is shown to
                  be excellent.},
  language =	 {en},
  number =	 2,
  urldate =	 {2016-01-15},
  journal =	 {Metallurgical Transactions B},
  author =	 {Goldak, John and Chakravarti, Aditya and Bibby,
                  Malcolm},
  year =	 1984,
  keywords =	 {Characterization and Evaluation of Materials,
                  Manufacturing, Machines, Tools, Materials Science,
                  Metallic Materials, Numerical and Computational
                  Methods in Engineering, Operating Procedures,
                  Materials Treatment},
  pages =	 {299--305},
  file =
                  {Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/M8W7V995/10.html:text/html}
}

@inproceedings{gong_microstructural_2014,
  title =	 {Microstructural characterization and modeling of
                  beam speed effects on {Ti}-6Al-4V by electron beam
                  additive manufacturing},
  url =
                  {http://www.researchgate.net/profile/Xibing_Gong/publication/265963691_Microstructural_Characterization_and_Modeling_of_Beam_Speed_Effects_on_Ti-6Al-4V_by_Electron_Beam_Additive_Manufacturing/links/54219b1b0cf2a39f4af59d21.pdf},
  urldate =	 {2016-01-15},
  booktitle =	 {Proceedings of the 25th Annual International Solid
                  FreeForm Fabrication Symposium - An Additive
                  Manufacturing Conference},
  author =	 {Gong, X. and Lydon, J. and Cooper, K. and Chou, K.},
  year =	 2014,
  pages =	 {459--469},
  file =
                  {2014-039-Gong.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/KJTMWTDD/2014-039-Gong.pdf:application/pdf}
}

@article{gong_phase-field_2015,
  title =	 {Phase-{Field} {Modeling} of {Microstructure}
                  {Evolution} in {Electron} {Beam} {Additive}
                  {Manufacturing}},
  volume =	 67,
  issn =	 {1047-4838, 1543-1851},
  url =
                  {http://link.springer.com/article/10.1007/s11837-015-1352-5},
  doi =		 {10.1007/s11837-015-1352-5},
  abstract =	 {In this study, the microstructure evolution in the
                  powder-bed electron beam additive manufacturing
                  (EBAM) process is studied using phase-field
                  modeling. In essence, EBAM involves a rapid
                  solidification process and the properties of a build
                  partly depend on the solidification behavior as well
                  as the microstructure of the build material. Thus,
                  the prediction of microstructure evolution in EBAM
                  is of importance for its process
                  optimization. Phase-field modeling was applied to
                  study the microstructure evolution and solute
                  concentration of the Ti-6Al-4V alloy in the EBAM
                  process. The effect of undercooling was investigated
                  through the simulations; the greater the
                  undercooling, the faster the dendrite grows. The
                  microstructure simulations show multiple
                  columnar-grain growths, comparable with experimental
                  results for the tested range.},
  language =	 {en},
  number =	 5,
  urldate =	 {2016-01-15},
  journal =	 {JOM},
  author =	 {Gong, Xibing and Chou, Kevin},
  year =	 2015,
  keywords =	 {Chemistry/Food Science, general, Earth Sciences,
                  general, Engineering, general, Environment, general,
                  Physics, general},
  pages =	 {1176--1182},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/4UQCKW7M/Gong
                  and Chou - 2015 - Phase-Field Modeling of
                  Microstructure Evolution
                  i.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/UDNUFDBF/10.html:text/html}
}

@article{gong_review_2014,
  title =	 {Review on powder-based electron beam additive
                  manufacturing technology},
  volume =	 1,
  issn =	 {2265-4224},
  url =		 {http://mfr.edp-open.org/10.1051/mfreview/2014001},
  doi =		 {10.1051/mfreview/2014001},
  urldate =	 {2015-12-29},
  journal =	 {Manufacturing Review},
  author =	 {Gong, Xibing and Anderson, Ted and Chou, Kevin},
  year =	 2014,
  pages =	 2,
  file =
                  {mfreview140001.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/SU3U77SX/mfreview140001.pdf:application/pdf}
}

@article{gottlieb_strong_2001,
	title = {Strong Stability-Preserving High-Order Time Discretization Methods},
	volume = {43},
	issn = {0036-1445},
	url = {http://epubs.siam.org.recursos.biblioteca.upc.edu/doi/abs/10.1137/S003614450036757X},
	doi = {10.1137/S003614450036757X},
	abstract = {In this paper we review and further develop a class of strong stability-preserving ({SSP}) high-order time discretizations for semidiscrete method of lines approximations of partial differential equations. Previously termed {TVD} (total variation diminishing) time discretizations, these high-order time discretization methods preserve the strong stability properties of first-order Euler time stepping and have proved very useful, especially in solving hyperbolic partial differential equations. The new developments in this paper include the construction of optimal explicit {SSP} linear Runge--Kutta methods, their application to the strong stability of coercive approximations, a systematic study of explicit {SSP} multistep methods for nonlinear problems, and the study of the {SSP} property of implicit Runge--Kutta and multistep methods.},
	pages = {89--112},
	number = {1},
        journal =  {{SIAM} Review},
	journaltitle = {{SIAM} Review},
	shortjournal = {{SIAM} Rev.},
	author = {Gottlieb, S. and Shu, C. and Tadmor, E.},
        year = {2001},
	urldate = {2017-01-10},
	date = {2001-01-01},
	file = {Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/AQT5N9QJ/S003614450036757X.html:text/html}
}

@article{gusarov_modeling_2010,
  series =	 {Laser {Assisted} {Net} {Shape} {Engineering} 6,
                  {Proceedings} of the {LANE} 2010, {Part} 2},
  title =	 {Modeling the interaction of laser radiation with
                  powder bed at selective laser melting},
  volume =	 {5, Part B},
  issn =	 {1875-3892},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S1875389210004918},
  doi =		 {10.1016/j.phpro.2010.08.065},
  abstract =	 {Obtaining uniform single vectors well attached to
                  the substrate is necessary and sufficient for
                  fabrication of complex-shape parts of high quality
                  by selective laser melting. The temperature
                  distribution in the laser/powder interaction zone
                  and the shape of the melt pool is numerically
                  calculated by the proposed model of coupled
                  radiation and heat transfer applicable to single
                  vectors. The analysis of the capillary stability of
                  the segmental cylinder applied to the calculated
                  melt pool estimates the stability of the process
                  depending on the scanning velocity, powder layer
                  thickness, and the material optical and thermal
                  properties.},
  urldate =	 {2016-01-15},
  journal =	 {Physics Procedia},
  author =	 {Gusarov, A. V. and Smurov, I.},
  year =	 2010,
  keywords =	 {Cappilary stability, Heat transfer, Powder Bed,
                  Radiation transfer, Selective laser melting},
  pages =	 {381--394},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/JRFQ78NT/Gusarov
                  and Smurov - 2010 - Modeling the interaction of
                  laser radiation with
                  p.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/X8QT2WM4/S1875389210004918.html:text/html}
}

@incollection{hackl_fe2-simulation_2010,
  title =	 {{FE}$^2$-{Simulation} of {Microheterogeneous}
                  {Steels} {Based} on {Statistically} {Similar}
                  {RVEs}},
  volume =	 21,
  series =	 {IUTAM Bookseries},
  isbn =	 {978-90-481-9194-9 978-90-481-9195-6},
  url =
                  {http://www.springerlink.com/index/10.1007/978-90-481-9195-6_2},
  urldate =	 {2015-12-29},
  booktitle =	 {{IUTAM} {Symposium} on {Variational} {Concepts} with
                  {Applications} to the {Mechanics} of {Materials}},
  publisher =	 {Springer},
  author =	 {Balzani, D. and Schröder, J. and Brands, D.},
  editor =	 {Hackl, Klaus},
  year =	 2010,
  pages =	 {15--28},
  file =
                  {9789048191949_Excerpt_002.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/NAQ3XHHA/9789048191949_Excerpt_002.pdf:application/pdf}
}

@book{hairer_solving_1993,
  title =	 {Solving Ordinary Differential Equations {II:} Stiff
                  and Differential-Algebraic Problems},
  isbn =	 9783540604525,
  shorttitle =	 {Solving Ordinary Differential Equations {II}},
  abstract =	 {The subject of this book is the solution of stiff
                  differential equations and of differential-algebraic
                  systems (differential equations with
                  constraints). The book is divided into four
                  chapters. The beginning of each chapter is of
                  introductory nature, followed by practical
                  applications, the discussion of numerical results,
                  theoretical investigations on the order and
                  accuracy, linear and nonlinear stability,
                  convergence and asymptotic expansions. Stiff and
                  differential-algebraic problems arise everywhere in
                  scientific computations (e.g., in physics,
                  chemistry, biology, control engineering, electrical
                  network analysis, mechanical systems). Many
                  applications as well as computer programs are
                  presented.},
  language =	 {en},
  publisher =	 {Springer},
  author =	 {Hairer, Ernst and Wanner, Gerhard},
  year =	 1993,
  keywords =	 {Mathematics / Applied, Mathematics / Calculus,
                  Mathematics / Differential Equations / General,
                  Mathematics / General, Mathematics / Mathematical
                  Analysis, Mathematics / Number Systems, Science /
                  Chemistry / Physical \& Theoretical, Science /
                  Physics / Mathematical \& Computational}
}

@book{hairer_solving_1993,
  title =	 {Solving Ordinary Differential Equations {II:} Stiff
                  and Differential-Algebraic Problems},
  isbn =	 9783540604525,
  shorttitle =	 {Solving Ordinary Differential Equations {II}},
  abstract =	 {The subject of this book is the solution of stiff
                  differential equations and of differential-algebraic
                  systems (differential equations with
                  constraints). The book is divided into four
                  chapters. The beginning of each chapter is of
                  introductory nature, followed by practical
                  applications, the discussion of numerical results,
                  theoretical investigations on the order and
                  accuracy, linear and nonlinear stability,
                  convergence and asymptotic expansions. Stiff and
                  differential-algebraic problems arise everywhere in
                  scientific computations (e.g., in physics,
                  chemistry, biology, control engineering, electrical
                  network analysis, mechanical systems). Many
                  applications as well as computer programs are
                  presented.},
  language =	 {en},
  publisher =	 {Springer},
  author =	 {Hairer, Ernst and Wanner, Gerhard},
  year =	 1993,
  keywords =	 {Mathematics / Applied, Mathematics / Calculus,
                  Mathematics / Differential Equations / General,
                  Mathematics / General, Mathematics / Mathematical
                  Analysis, Mathematics / Number Systems, Science /
                  Chemistry / Physical \& Theoretical, Science /
                  Physics / Mathematical \& Computational}
}

@ARTICLE{haring_ibm_2012,
  author =	 {Haring, {R.A.} and Ohmacht, M. and Fox, {T.W.} and
                  Gschwind, {M.K.}  and Satterfield, {D.L.} and
                  Sugavanam, K. and Coteus, {P.W.} and Heidelberger,
                  P. and Blumrich, {M.A.} and Wisniewski, {R.W.} and
                  Gara, A. and Chiu, {G.L.-T.} and Boyle, {P.A.} and
                  Chist, {N.H.}  and Kim, Changhoan},
  title =	 {The {IBM} Blue {Gene/Q} Compute Chip},
  journal =	 {{IEEE} Micro},
  year =	 2012,
  volume =	 32,
  pages =	 {48--60},
  number =	 2,
  abstract =	 {Blue {Gene/Q} aims to build a massively parallel
                  high-performance computing system out of
                  power-efficient processor chips, resulting in
                  power-efficient, cost-efficient, and floor-space-
                  efficient systems.  Focusing on reliability during
                  design helps with scaling to large systems and
                  lowers the total cost of ownership. This article
                  examines the architecture and design of the Compute
                  chip, which combines processors, memory, and
                  communication functions on a single chip.},
  doi =		 {10.1109/MM.2011.108},
  file =	 {IEEE Xplore Abstract
                  Record:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/3A9RWPQB/abs_all.html:text/html;IEEE
                  Xplore Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/BMB9UKED/Haring
                  et al. - 2012 - The IBM Blue GeneQ Compute
                  Chip.pdf:application/pdf},
  issn =	 {0272-1732},
  keywords =	 {Bandwidth, floor-space-efficient systems, Hardware,
                  {IBM} Blue Gene-Q Compute chip design, large and
                  medium ("mainframe") computers, large systems
                  scaling, memory architecture, Message systems,
                  microprocessor chips, microprocessors and
                  microcomputers, multicore/single-chip
                  multiprocessors, multiple data stream architectures
                  (multiprocessors), multithreaded processors,
                  parallel high-performance computing system, parallel
                  processing, parallel processors, power-efficient
                  processor chips, Prefetching, Registers,
                  reliability, {SIMD} processors, single chip
                  communication functions, speculative multithreading,
                  Super (very large) computers, support for
                  multithreaded execution},
  owner =	 {principe},
  timestamp =	 {2014.03.21}
}

@ARTICLE{haring_ibm_2012,
  author =	 {Haring, {R.A.} and Ohmacht, M. and Fox, {T.W.} and
                  Gschwind, {M.K.}  and Satterfield, {D.L.} and
                  Sugavanam, K. and Coteus, {P.W.} and Heidelberger,
                  P. and Blumrich, {M.A.} and Wisniewski, {R.W.} and
                  Gara, A. and Chiu, {G.L.-T.} and Boyle, {P.A.} and
                  Chist, {N.H.}  and Kim, Changhoan},
  title =	 {The {IBM} Blue {Gene/Q} Compute Chip},
  journal =	 {{IEEE} Micro},
  year =	 2012,
  volume =	 32,
  pages =	 {48--60},
  number =	 2,
  abstract =	 {Blue {Gene/Q} aims to build a massively parallel
                  high-performance computing system out of
                  power-efficient processor chips, resulting in
                  power-efficient, cost-efficient, and floor-space-
                  efficient systems.  Focusing on reliability during
                  design helps with scaling to large systems and
                  lowers the total cost of ownership. This article
                  examines the architecture and design of the Compute
                  chip, which combines processors, memory, and
                  communication functions on a single chip.},
  doi =		 {10.1109/MM.2011.108},
  file =	 {IEEE Xplore Abstract
                  Record:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/3A9RWPQB/abs_all.html:text/html;IEEE
                  Xplore Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/BMB9UKED/Haring
                  et al. - 2012 - The IBM Blue GeneQ Compute
                  Chip.pdf:application/pdf},
  issn =	 {0272-1732},
  keywords =	 {Bandwidth, floor-space-efficient systems, Hardware,
                  {IBM} Blue Gene-Q Compute chip design, large and
                  medium ("mainframe") computers, large systems
                  scaling, memory architecture, Message systems,
                  microprocessor chips, microprocessors and
                  microcomputers, multicore/single-chip
                  multiprocessors, multiple data stream architectures
                  (multiprocessors), multithreaded processors,
                  parallel high-performance computing system, parallel
                  processing, parallel processors, power-efficient
                  processor chips, Prefetching, Registers,
                  reliability, {SIMD} processors, single chip
                  communication functions, speculative multithreading,
                  Super (very large) computers, support for
                  multithreaded execution},
  owner =	 {principe},
  timestamp =	 {2014.03.21}
}

% This file was created with JabRef 2.7b.
% Encoding: UTF8

@ARTICLE{heroux_software_2009,
  author =	 {Heroux, Michael A.},
  title =	 {Software Challenges for Extreme Scale Computing:
                  Going From Petascale to Exascale Systems},
  journal =	 {International Journal of High Performance Computing
                  Applications},
  year =	 2009,
  volume =	 23,
  pages =	 {437--439},
  number =	 4,
  abstract =	 {Preparing applications for a transition from
                  petascale to exascale systems will require a very
                  large investment in several areas of software
                  research and development. The introduction of
                  manycore nodes, the abundance of parallelism, an
                  increase in system faults (including soft errors)
                  and a complicated, multi-component software
                  environment are some of the most challenging issues
                  we face. In this paper we address four topics we
                  believe to be the most the challenging issues and
                  therefore the greatest opportunities for making
                  effective next-generation scalable
                  applications. First and foremost is the need to
                  transform existing applications to run on manycore
                  platforms and properly design new applications. This
                  is particularly challenging in the absence of a
                  standard, portable manycore programming environment,
                  but we can make progress in this direction while
                  manycore programming models are developed. Second is
                  promoting advanced modeling and simulation
                  capabilities such as embedded optimization and
                  uncertainty quantification that lead to higher
                  quality results and orders of magnitude more
                  parallelism. Third is progress toward fault
                  resilience in applications, a critical need as
                  system reliability degrades. Fourth and finally is a
                  qualitative improvement in software design,
                  including the social aspects, as exascale software
                  systems will be increasingly multi-team and
                  multi-faceted efforts.},
  doi =		 {10.1177/1094342009347711},
  file =
                  {Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/DEMP5747/437.html:text/html},
  issn =	 {1094-3420, 1741-2846},
  keywords =	 {advanced modeling and simulation, exascale
                  computing, fault resilient applications, parallel
                  programming models, software engineering for
                  computational science and engineering},
  language =	 {en},
  owner =	 {principe},
  shorttitle =	 {Software Challenges for Extreme Scale Computing},
  timestamp =	 {2014.03.21},
  url =		 {http://hpc.sagepub.com/content/23/4/437},
  urldate =	 {2014-03-21}
}

@ARTICLE{heroux_software_2009,
  author =	 {Heroux, Michael A.},
  title =	 {Software Challenges for Extreme Scale Computing:
                  Going From Petascale to Exascale Systems},
  journal =	 {International Journal of High Performance Computing
                  Applications},
  year =	 2009,
  volume =	 23,
  pages =	 {437--439},
  number =	 4,
  abstract =	 {Preparing applications for a transition from
                  petascale to exascale systems will require a very
                  large investment in several areas of software
                  research and development. The introduction of
                  manycore nodes, the abundance of parallelism, an
                  increase in system faults (including soft errors)
                  and a complicated, multi-component software
                  environment are some of the most challenging issues
                  we face. In this paper we address four topics we
                  believe to be the most the challenging issues and
                  therefore the greatest opportunities for making
                  effective next-generation scalable
                  applications. First and foremost is the need to
                  transform existing applications to run on manycore
                  platforms and properly design new applications. This
                  is particularly challenging in the absence of a
                  standard, portable manycore programming environment,
                  but we can make progress in this direction while
                  manycore programming models are developed. Second is
                  promoting advanced modeling and simulation
                  capabilities such as embedded optimization and
                  uncertainty quantification that lead to higher
                  quality results and orders of magnitude more
                  parallelism. Third is progress toward fault
                  resilience in applications, a critical need as
                  system reliability degrades. Fourth and finally is a
                  qualitative improvement in software design,
                  including the social aspects, as exascale software
                  systems will be increasingly multi-team and
                  multi-faceted efforts.},
  doi =		 {10.1177/1094342009347711},
  file =
                  {Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/DEMP5747/437.html:text/html},
  issn =	 {1094-3420, 1741-2846},
  keywords =	 {advanced modeling and simulation, exascale
                  computing, fault resilient applications, parallel
                  programming models, software engineering for
                  computational science and engineering},
  language =	 {en},
  owner =	 {principe},
  shorttitle =	 {Software Challenges for Extreme Scale Computing},
  timestamp =	 {2014.03.21},
  url =		 {http://hpc.sagepub.com/content/23/4/437},
  urldate =	 {2014-03-21}
}

@article{hierro_shock_2016,
	title = {Shock capturing techniques for $hp$-adaptive finite elements},
	volume = {309},
	issn = {0045-7825},
	url = {http://www.sciencedirect.com/science/article/pii/S0045782516305862},
	doi = {10.1016/j.cma.2016.06.017},
	abstract = {The aim of this work is to propose an h p -adaptive algorithm for discontinuous Galerkin methods that is capable to detect the discontinuities and sharp layers and avoid the spurious oscillation of the solution around them. In order to control the spurious oscillations, artificial viscosity is used with the particularity that it is only applied around the layers where the solution changes abruptly. To do so, a novel troubled-cell detector has been developed in order to mark the elements around those layers and to impose linear order in them. The detector takes advantage of the evolution of the value of the gradient through the adaptive process.},
	pages = {532--553},
        year = {2016},
        journal = {Computer Methods in Applied Mechanics and Engineering},
	journaltitle = {Computer Methods in Applied Mechanics and Engineering},
	shortjournal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Hierro, Alba and Badia, Santiago and Kus, Pavel},
	urldate = {2017-01-10},
	date = {2016-09-01},
	keywords = {Convection–diffusion, Convection-dominated flows, Discontinuous Galerkin, h  p  -adaptivity, shock capturing, Troubled-cell detector},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/C72IGK4M/Hierro et al. - 2016 - Shock capturing techniques for -adaptive finite el.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/Z2WXKQDR/S0045782516305862.html:text/html}
}

@article{hodge_implementation_2014,
  title =	 {Implementation of a thermomechanical model for the
                  simulation of selective laser melting},
  volume =	 54,
  issn =	 {0178-7675, 1432-0924},
  url =
                  {http://link.springer.com/article/10.1007/s00466-014-1024-2},
  doi =		 {10.1007/s00466-014-1024-2},
  abstract =	 {Selective laser melting (SLM) is an additive
                  manufacturing process in which multiple, successive
                  layers of metal powders are heated via laser in
                  order to build a part. Modeling of SLM requires
                  consideration of both heat transfer and solid
                  mechanics. The present work describes continuum
                  modeling of SLM as envisioned for eventual support
                  of part-scale modeling of this fabrication process
                  to determine end-state information such as residual
                  stresses and distortion. The determination of the
                  evolving temperatures is dependent on the material,
                  the state of the material (powder or solid), the
                  specified heating, and the configuration. Similarly,
                  the current configuration is dependent on the
                  temperatures, the powder-solid state, and the
                  constitutive models. A multi-physics numerical
                  formulation is required to solve such problems. This
                  article describes the problem formulation, numerical
                  method, and constitutive parameters necessary to
                  solve such a problem. Additionally, various
                  verification and example problems are simulated in
                  the parallel, multi-physics finite element code
                  Diablo, and the results presented herein.},
  language =	 {en},
  number =	 1,
  urldate =	 {2016-01-15},
  journal =	 {Computational Mechanics},
  author =	 {Hodge, N. E. and Ferencz, R. M. and Solberg, J. M.},
  year =	 2014,
  keywords =	 {Classical Continuum Physics, Computational Science
                  and Engineering, Theoretical and Applied Mechanics},
  pages =	 {33--51},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/266DT463/Hodge
                  et al. - 2014 - Implementation of a thermomechanical
                  model for
                  the.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/N5CGHZPA/10.html:text/html}
}

@article{huet_application_1990,
  title =	 {Application of variational concepts to size effects
                  in elastic heterogeneous bodies},
  volume =	 38,
  issn =	 {0022-5096},
  url =
                  {http://www.sciencedirect.com/science/article/pii/0022509690900412},
  doi =		 {10.1016/0022-5096(90)90041-2},
  abstract =	 {Heterogeneous materials that are not statistically
                  uniform are considered. The basic assumptions and
                  conditions underlying the concept of effective
                  properties are reviewed. A general procedure for the
                  experimental or numerical evaluation of the overall
                  properties of the material when the concept of a
                  representative volume does not apply or cannot be
                  used is proposed and then applied to the elastic
                  case as a first example. Three kinds of results are
                  obtained: relationships between experimental results
                  obtained on a large specimen and on an appropriate
                  set of smaller ones, hierarchies between families of
                  specimens of various sizes, absolute bounds for the
                  two limiting cases where the size goes to zero, or
                  to the representative volume when it
                  exists. Possible applications and extensions are
                  mentioned.},
  number =	 6,
  urldate =	 {2016-01-15},
  journal =	 {Journal of the Mechanics and Physics of Solids},
  author =	 {Huet, C.},
  year =	 1990,
  pages =	 {813--841},
  file =	 {ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/VN35B9H9/0022509690900412.html:text/html}
}

@article{hughes_isogeometric_2005,
  title =	 {Isogeometric analysis: {CAD}, finite elements,
                  {NURBS}, exact geometry and mesh refinement},
  volume =	 194,
  issn =	 {0045-7825},
  shorttitle =	 {Isogeometric analysis},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782504005171},
  doi =		 {10.1016/j.cma.2004.10.008},
  abstract =	 {The concept of isogeometric analysis is
                  proposed. Basis functions generated from NURBS
                  (Non-Uniform Rational B-Splines) are employed to
                  construct an exact geometric model. For purposes of
                  analysis, the basis is refined and/or its order
                  elevated without changing the geometry or its
                  parameterization. Analogues of finite element h- and
                  p-refinement schemes are presented and a new, more
                  efficient, higher-order concept, k-refinement, is
                  introduced. Refinements are easily implemented and
                  exact geometry is maintained at all levels without
                  the necessity of subsequent communication with a CAD
                  (Computer Aided Design) description. In the context
                  of structural mechanics, it is established that the
                  basis functions are complete with respect to affine
                  transformations, meaning that all rigid body motions
                  and constant strain states are exactly
                  represented. Standard patch tests are likewise
                  satisfied. Numerical examples exhibit optimal rates
                  of convergence for linear elasticity problems and
                  convergence to thin elastic shell solutions. A
                  k-refinement strategy is shown to converge toward
                  monotone solutions for advection-diffusion processes
                  with sharp internal and boundary layers, a very
                  surprising result. It is argued that isogeometric
                  analysis is a viable alternative to standard,
                  polynomial-based, finite element analysis and
                  possesses several advantages.},
  number =	 {39-41},
  urldate =	 {2016-01-14},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Hughes, T. J. R. and Cottrell, J. A. and Bazilevs,
                  Y.},
  year =	 2005,
  keywords =	 {Boundary layers, CAD, Convergence, finite element
                  analysis, Fluid dynamics, Geometry, h-refinement,
                  Internal layers, k-refinement, Mesh refinement,
                  NURBS, p-refinement, shells, Structural analysis},
  pages =	 {4135--4195},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/DH9J36EN/Hughes
                  et al. - 2005 - Isogeometric analysis CAD, finite
                  elements, NURBS.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/FABSTMKR/S0045782504005171.html:text/html}
}

@book{jeulin_mechanics_2014,
  title =	 {Mechanics of {Random} and {Multiscale}
                  {Microstructures}},
  isbn =	 {978-3-7091-2780-3},
  abstract =	 {This book reviews recent theoretical, computational
                  and experimental developments in mechanics of random
                  and multiscale solid materials. The aim is to
                  provide tools for better understanding and
                  prediction of the effects of stochastic
                  (non-periodic) microstructures on materials’
                  mesoscopic and macroscopic properties. Particular
                  topics involve a review of experimental techniques
                  for the microstructure description, a survey of key
                  methods of probability theory applied to the
                  description and representation of microstructures by
                  random modes, static and dynamic elasticity and
                  non-linear problems in random media via variational
                  principles, stochastic wave propagation, Monte Carlo
                  simulation of random continuous and discrete media,
                  fracture statistics models, and computational
                  micromechanics.},
  language =	 {en},
  publisher =	 {Springer},
  author =	 {Jeulin, Dominique and Ostoja-Starzewski, Martin},
  year =	 2014,
  keywords =	 {Computers / Computer Graphics, Computers / Computer
                  Simulation, Computers / Desktop Applications /
                  Design \& Graphics, Computers / Intelligence (AI) \&
                  Semantics, Computers / Optical Data Processing,
                  Computers / Software Development \& Engineering /
                  General, Science / Physics / General, Science /
                  Physics / Mathematical \& Computational, Technology
                  \& Engineering / Mechanical}
}

@article{john_adaptive_2010,
  title =	 {Adaptive time step control for the incompressible
                  {Navier-Stokes} equations},
  volume =	 199,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782509003417},
  doi =		 {10.1016/j.cma.2009.10.005},
  abstract =	 {Adaptive time stepping is an important tool in
                  Computational Fluid Dynamics for controlling the
                  accuracy of simulations and for enhancing their
                  efficiency. This paper presents a systematic study
                  of three classes of implicit and linearly implicit
                  time stepping schemes with adaptive time step
                  control applied to a {2D} laminar flow around a
                  cylinder: θ -schemes, diagonal-implicit
                  {Runge-Kutta} ({DIRK)} methods and
                  {Rosenbrock-Wanner} ({ROW)} methods. The time step
                  is controlled using embedded methods. It is shown
                  that several {ROW} methods clearly outperform the
                  more standard θ -schemes and the {DIRK} methods. The
                  results depend on a prescribed tolerance in the time
                  step control algorithm, whose appropriate choice
                  varies from scheme to scheme.},
  number =	 {9--12},
  urldate =	 {2013-04-17},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {John, Volker and Rang, Joachim},
  year =	 2010,
  keywords =	 {Adaptive time step control, {DIRK} methods, Implicit
                  θ -schemes, incompressible {Navier-Stokes}
                  equations, {ROW} methods},
  pages =	 {514--524},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/S7P23BDA/John
                  y Rang - 2010 - Adaptive time step control for the
                  incompressible .pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/DQHU9FUT/S0045782509003417.html:text/html}
}

@article{john_adaptive_2010,
  title =	 {Adaptive time step control for the incompressible
                  {Navier-Stokes} equations},
  volume =	 199,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782509003417},
  doi =		 {10.1016/j.cma.2009.10.005},
  abstract =	 {Adaptive time stepping is an important tool in
                  Computational Fluid Dynamics for controlling the
                  accuracy of simulations and for enhancing their
                  efficiency. This paper presents a systematic study
                  of three classes of implicit and linearly implicit
                  time stepping schemes with adaptive time step
                  control applied to a {2D} laminar flow around a
                  cylinder: θ -schemes, diagonal-implicit
                  {Runge-Kutta} ({DIRK)} methods and
                  {Rosenbrock-Wanner} ({ROW)} methods. The time step
                  is controlled using embedded methods. It is shown
                  that several {ROW} methods clearly outperform the
                  more standard θ -schemes and the {DIRK} methods. The
                  results depend on a prescribed tolerance in the time
                  step control algorithm, whose appropriate choice
                  varies from scheme to scheme.},
  number =	 {9--12},
  urldate =	 {2013-04-17},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {John, Volker and Rang, Joachim},
  year =	 2010,
  keywords =	 {Adaptive time step control, {DIRK} methods, Implicit
                  θ -schemes, incompressible {Navier-Stokes}
                  equations, {ROW} methods},
  pages =	 {514--524},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/S7P23BDA/John
                  y Rang - 2010 - Adaptive time step control for the
                  incompressible .pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/DQHU9FUT/S0045782509003417.html:text/html}
}

@article{kale_charm++:_1993,
	title = {{CHARM}++: a portable concurrent object oriented system based on {C}++},
	volume = {28},
	issn = {03621340},
	shorttitle = {{CHARM}++},
	url = {http://portal.acm.org/citation.cfm?doid=167962.165874},
	doi = {10.1145/167962.165874},
	language = {en},
	number = {10},
	urldate = {2017-01-13},
	journal = {ACM SIGPLAN Notices},
	author = {Kale, Laxmikant V. and Krishnan, Sanjeev},
	year = {1993},
	pages = {91--108}
}

@techreport{karypis_software_2013,
  author =	 {Karypis, G.},
  title =	 {A software package for partitioning unstructured
                  graphs, partitioning meshes, and computing
                  fill-reducing orderings of sparse
                  matrices. {V}ersion 5.1.0},
  url =
                  {http://glaros.dtc.umn.edu/gkhome/fetch/sw/metis/manual.pdf},
  note =	 "Available at
                  \url{http://glaros.dtc.umn.edu/gkhome/fetch/sw/metis/manual.pdf}",
  institution =	 {University of Minnesota, Department of Computer
                  Science and Engineering, Minneapolis, {MN}},
  year =	 2013,
  file =
                  {manual.pdf:/Users/sbadia/Work/zotero/storage/UXUJFRPI/manual.pdf:application/pdf}
}

@techreport{karypis_software_2013,
  author =	 {Karypis, G.},
  title =	 {A software package for partitioning unstructured
                  graphs, partitioning meshes, and computing
                  fill-reducing orderings of sparse
                  matrices. {V}ersion 5.1.0},
  url =
                  {http://glaros.dtc.umn.edu/gkhome/fetch/sw/metis/manual.pdf},
  note =	 "Available at
                  \url{http://glaros.dtc.umn.edu/gkhome/fetch/sw/metis/manual.pdf}",
  institution =	 {University of Minnesota, Department of Computer
                  Science and Engineering, Minneapolis, {MN}},
  year =	 2013,
  file =
                  {manual.pdf:/Users/sbadia/Work/zotero/storage/UXUJFRPI/manual.pdf:application/pdf}
}

@article{kaufman_calphad_2014,
  title =	 {{CALPHAD}, first and second generation -- {Birth} of
                  the materials genome},
  volume =	 70,
  issn =	 {1359-6462},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S1359646212007749},
  doi =		 {10.1016/j.scriptamat.2012.12.003},
  abstract =	 {The “materials genome” was given a rather vague
                  definition in US President Obama’s announcement in
                  2011. We argue that the materials genome,
                  analogously with biological genomes, should be
                  defined as a set of information (databases) allowing
                  prediction of a material’s structure, as well as its
                  response to processing and usage conditions. The
                  materials genome is thus encoded in the language of
                  CALPHAD thermodynamics and kinetics, as such
                  databases are major parts of integrated
                  computational materials engineering.},
  urldate =	 {2016-01-28},
  journal =	 {Scripta Materialia},
  author =	 {Kaufman, Larry and {\AA}gren, John},
  year =	 2014,
  pages =	 {3--6},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/Z8SCWPFD/Kaufman
                  and Ågren - 2014 - CALPHAD, first and second
                  generation – Birth of
                  th.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/7SM6FHQE/S1359646212007749.html:text/html}
}

@inproceedings{keller_new_????,
  title =	 {NEW METHOD FOR FAST PREDICTIONS OF RESIDUAL STRESS
                  AND DISTORTION OF AM PARTS},
  url =
                  {http://sffsymposium.engr.utexas.edu/sites/default/files/2014-096-Keller.pdf},
  urldate =	 {2015-12-29},
  booktitle =	 {Proceedings of the 25th Annual International Solid
                  FreeForm Fabrication Symposium - An Additive
                  Manufacturing Conference},
  author =	 {Keller, Nils and Ploshikhin, Vasily},
  year =	 2014,
  pages =	 {1229-1237},
  file =
                  {2014-096-Keller.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/IF8H99IK/2014-096-Keller.pdf:application/pdf}
}

@inproceedings{keller_thermo-mechanical_2013,
  title =	 {Thermo-mechanical {Simulation} of {Additive} {Layer}
                  {Manufacturing} of {Titanium} {Aerospace}
                  structures},
  volume =	 3,
  url =
                  {http://www.researchgate.net/profile/Nils_Keller/publication/275330526_Thermo-mechanical_Simulation_of_Additive_Layer_Manufacturing_of_Titanium_Aerospace_structures/links/5537781d0cf2058efdeabf14.pdf},
  urldate =	 {2015-12-29},
  booktitle =	 {{LightMAT} {Conference}},
  author =	 {Keller, N. and Neugebauer, F. and Xu, H. and
                  Ploshikhin, V.},
  year =	 2013,
  file =
                  {1171-160.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/Z75ZXKXA/1171-160.pdf:application/pdf}
}

@ARTICLE{keyes_multiphysics_2013,
  author =	 {Keyes, David E. and {McInnes}, Lois C. and Woodward,
                  Carol and Gropp, William and Myra, Eric and Pernice,
                  Michael and Bell, John and Brown, Jed and Clo, Alain
                  and Connors, Jeffrey and Constantinescu, Emil and
                  Estep, Don and Evans, Kate and Farhat, Charbel and
                  Hakim, Ammar and Hammond, Glenn and Hansen, Glen and
                  Hill, Judith and Isaac, Tobin and Jiao, Xiangmin and
                  Jordan, Kirk and Kaushik, Dinesh and Kaxiras,
                  Efthimios and Koniges, Alice and Lee, Kihwan and
                  Lott, Aaron and Lu, Qiming and Magerlein, John and
                  Maxwell, Reed and {McCourt}, Michael and Mehl,
                  Miriam and Pawlowski, Roger and Randles, Amanda
                  P. and Reynolds, Daniel and Rivi{\'e}re, Beatrice
                  and R{\"u}de, Ulrich and Scheibe, Tim and Shadid,
                  John and Sheehan, Brendan and Shephard, Mark and
                  Siegel, Andrew and Smith, Barry and Tang, Xianzhu
                  and Wilson, Cian and Wohlmuth, Barbara},
  title =	 {Multiphysics simulations Challenges and
                  opportunities},
  journal =	 {International Journal of High Performance Computing
                  Applications},
  year =	 2013,
  volume =	 27,
  pages =	 {4--83},
  number =	 1,
  abstract =	 {We consider multiphysics applications from
                  algorithmic and architectural perspectives, where
                  âalgorithmicâ includes both
                  mathematical analysis and computational complexity,
                  and âarchitecturalâ includes both
                  software and hardware environments. Many diverse
                  multiphysics applications can be reduced, en route
                  to their computational simulation, to a common
                  algebraic coupling paradigm. Mathematical analysis
                  of multiphysics coupling in this form is not always
                  practical for realistic applications, but model
                  problems representative of applications discussed
                  herein can provide insight. A variety of software
                  frameworks for multiphysics applications have been
                  constructed and refined within disciplinary
                  communities and executed on leading-edge computer
                  systems.  We examine several of these, expose some
                  commonalities among them, and attempt to extrapolate
                  best practices to future systems. From our study, we
                  summarize challenges and forecast opportunities.},
  doi =		 {10.1177/1094342012468181},
  file =	 {Full Text
                  PDF:/home/principe/Javier/System/Config/firefox/zotero/storage/9MUIDD2T/Keyes
                  et al. - 2013 - Multiphysics simulations Challenges
                  and
                  opportunit.pdf:application/pdf;Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/59M6SBB7/4.html:text/html},
  issn =	 {1094-3420, 1741-2846},
  keywords =	 {implicit and explicit algorithms, loose and tight
                  coupling., multimodel, Multiphysics, multirate,
                  multiscale, strong and weak coupling},
  language =	 {en},
  owner =	 {principe},
  timestamp =	 {2014.03.21},
  url =		 {http://hpc.sagepub.com/content/27/1/4},
  urldate =	 {2013-09-30}
}

@ARTICLE{keyes_multiphysics_2013,
  author =	 {Keyes, David E. and {McInnes}, Lois C. and Woodward,
                  Carol and Gropp, William and Myra, Eric and Pernice,
                  Michael and Bell, John and Brown, Jed and Clo, Alain
                  and Connors, Jeffrey and Constantinescu, Emil and
                  Estep, Don and Evans, Kate and Farhat, Charbel and
                  Hakim, Ammar and Hammond, Glenn and Hansen, Glen and
                  Hill, Judith and Isaac, Tobin and Jiao, Xiangmin and
                  Jordan, Kirk and Kaushik, Dinesh and Kaxiras,
                  Efthimios and Koniges, Alice and Lee, Kihwan and
                  Lott, Aaron and Lu, Qiming and Magerlein, John and
                  Maxwell, Reed and {McCourt}, Michael and Mehl,
                  Miriam and Pawlowski, Roger and Randles, Amanda
                  P. and Reynolds, Daniel and Rivi{\'e}re, Beatrice
                  and R{\"u}de, Ulrich and Scheibe, Tim and Shadid,
                  John and Sheehan, Brendan and Shephard, Mark and
                  Siegel, Andrew and Smith, Barry and Tang, Xianzhu
                  and Wilson, Cian and Wohlmuth, Barbara},
  title =	 {Multiphysics simulations Challenges and
                  opportunities},
  journal =	 {International Journal of High Performance Computing
                  Applications},
  year =	 2013,
  volume =	 27,
  pages =	 {4--83},
  number =	 1,
  abstract =	 {We consider multiphysics applications from
                  algorithmic and architectural perspectives, where
                  âalgorithmicâ includes both
                  mathematical analysis and computational complexity,
                  and âarchitecturalâ includes both
                  software and hardware environments. Many diverse
                  multiphysics applications can be reduced, en route
                  to their computational simulation, to a common
                  algebraic coupling paradigm. Mathematical analysis
                  of multiphysics coupling in this form is not always
                  practical for realistic applications, but model
                  problems representative of applications discussed
                  herein can provide insight. A variety of software
                  frameworks for multiphysics applications have been
                  constructed and refined within disciplinary
                  communities and executed on leading-edge computer
                  systems.  We examine several of these, expose some
                  commonalities among them, and attempt to extrapolate
                  best practices to future systems. From our study, we
                  summarize challenges and forecast opportunities.},
  doi =		 {10.1177/1094342012468181},
  file =	 {Full Text
                  PDF:/home/principe/Javier/System/Config/firefox/zotero/storage/9MUIDD2T/Keyes
                  et al. - 2013 - Multiphysics simulations Challenges
                  and
                  opportunit.pdf:application/pdf;Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/59M6SBB7/4.html:text/html},
  issn =	 {1094-3420, 1741-2846},
  keywords =	 {implicit and explicit algorithms, loose and tight
                  coupling., multimodel, Multiphysics, multirate,
                  multiscale, strong and weak coupling},
  language =	 {en},
  owner =	 {principe},
  timestamp =	 {2014.03.21},
  url =		 {http://hpc.sagepub.com/content/27/1/4},
  urldate =	 {2013-09-30}
}

@article{king_overview_2015,
  title =	 {Overview of modelling and simulation of metal powder
                  bed fusion process at {Lawrence} {Livermore}
                  {National} {Laboratory}},
  volume =	 31,
  issn =	 {0267-0836, 1743-2847},
  url =
                  {http://www.maneyonline.com/doi/10.1179/1743284714Y.0000000728},
  doi =		 {10.1179/1743284714Y.0000000728},
  language =	 {en},
  number =	 8,
  urldate =	 {2015-12-29},
  journal =	 {Materials Science and Technology},
  author =	 {King, W. and Anderson, A. T. and Ferencz, R. M. and
                  Hodge, N. E. and Kamath, C. and Khairallah, S. A.},
  year =	 2015,
  pages =	 {957--968},
  file =
                  {1743284714Y.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/NCWTPEVM/1743284714Y.pdf:application/pdf}
}

@article{klassen_evaporation_2014,
  title =	 {Evaporation model for beam based additive
                  manufacturing using free surface lattice {Boltzmann}
                  methods},
  volume =	 47,
  issn =	 {0022-3727},
  url =		 {http://stacks.iop.org/0022-3727/47/i=27/a=275303},
  doi =		 {10.1088/0022-3727/47/27/275303},
  abstract =	 {Evaporation plays an important role in many
                  technical applications including beam-based additive
                  manufacturing processes, such as selective electron
                  beam or selective laser melting (SEBM/SLM). In this
                  paper, we describe an evaporation model which we
                  employ within the framework of a two-dimensional
                  free surface lattice Boltzmann method. With this
                  method, we solve the hydrodynamics as well as
                  thermodynamics of the molten material taking into
                  account the mass and energy losses due to
                  evaporation and the recoil pressure acting on the
                  melt pool. Validation of the numerical model is
                  performed by measuring maximum melt depths and
                  evaporative losses in samples of pure titanium and
                  Ti-6Al-4V molten by an electron beam. Finally, the
                  model is applied to create processing maps for an
                  SEBM process. The results predict that the
                  penetration depth of the electron beam, which is a
                  function of the acceleration voltage, has a
                  significant influence on evaporation effects.},
  language =	 {en},
  number =	 27,
  urldate =	 {2016-01-15},
  journal =	 {Journal of Physics D: Applied Physics},
  author =	 {Klassen, Alexander and Scharowsky, Thorsten and
                  K\"{o}rner, Carolin},
  year =	 2014,
  pages =	 275303
}

@article{klawonn_feti-dp_2015,
  title =	 {{FETI}-{DP} {Methods} with an {Adaptive} {Coarse}
                  {Space}},
  volume =	 53,
  issn =	 {0036-1429, 1095-7170},
  url =		 {http://epubs.siam.org/doi/10.1137/130939675},
  doi =		 {10.1137/130939675},
  language =	 {en},
  number =	 1,
  urldate =	 {2015-12-18},
  journal =	 {SIAM Journal on Numerical Analysis},
  author =	 {Klawonn, A. and Radtke, P. and Rheinbach, O.},
  year =	 2015,
  pages =	 {297--320},
  file =	 {FETI-DP Methods with an Adaptive Coarse
                  Space-Klawonn-Radtke-Rheinbach.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/UNXA6PI8/FETI-DP
                  Methods with an Adaptive Coarse
                  Space-Klawonn-Radtke-Rheinbach.pdf:application/pdf}
}

@article{klawonn_highly_2010,
  title =	 {Highly scalable parallel domain decomposition
                  methods with an application to biomechanics},
  volume =	 90,
  copyright =	 {Copyright © 2010 {WILEY-VCH} Verlag {GmbH} \&
                  Co. {KGaA}, Weinheim},
  issn =	 {1521-4001},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/zamm.200900329/abstract},
  doi =		 {10.1002/zamm.200900329},
  abstract =	 {Highly scalable parallel domain decomposition
                  methods for elliptic partial differential equations
                  are considered with a special emphasis on problems
                  arising in elasticity. The focus of this survey
                  article is on Finite Element Tearing and
                  Interconnecting ({FETI)} methods, a family of
                  nonoverlapping domain decomposition methods where
                  the continuity between the subdomains, in principle,
                  is enforced by the use of Lagrange
                  multipliers. Exact onelevel and dual-primal {FETI}
                  methods as well as related inexact dual-primal
                  variants are described and theoretical convergence
                  estimates are presented together with numerical
                  results confirming the parallel scalability
                  properties of these methods. New aspects such as a
                  hybrid onelevel {FETI/FETI-DP} approach and the
                  behavior of {FETI-DP} for anisotropic elasticity
                  problems are presented. Parallel and numerical
                  scalability of the methods for more than 65 000
                  processor cores of the {JUGENE} supercomputer is
                  shown. An application of a dual-primal {FETI} method
                  to a nontrivial biomechanical problem from nonlinear
                  elasticity, modeling arterial wall stress, is given,
                  showing the robustness of our domain decomposition
                  methods for such problems.},
  language =	 {en},
  number =	 1,
  urldate =	 {2013-08-02},
  journal =	 {{ZAMM} - Journal of Applied Mathematics and
                  Mechanics / Zeitschrift f\"ur Angewandte Mathematik
                  und Mechanik},
  author =	 {Klawonn, A. and Rheinbach, O.},
  year =	 2010,
  keywords =	 {algebraic multigrid, arterial wall., biomechanics,
                  domain decomposition, elasticity, elliptic systems,
                  {FETI}, Finite elements, inexact, Lagrange
                  multipliers, multilevel methods, parallel computing,
                  preconditioners},
  pages =	 {5--32},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/5669IFT7/Klawonn
                  and Rheinbach - 2010 - Highly scalable parallel
                  domain decomposition
                  meth.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/WSVQVCJT/abstract.html:text/html}
}

@article{klawonn_highly_2010,
  title =	 {Highly scalable parallel domain decomposition
                  methods with an application to biomechanics},
  volume =	 90,
  copyright =	 {Copyright © 2010 {WILEY-VCH} Verlag {GmbH} \&
                  Co. {KGaA}, Weinheim},
  issn =	 {1521-4001},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/zamm.200900329/abstract},
  doi =		 {10.1002/zamm.200900329},
  abstract =	 {Highly scalable parallel domain decomposition
                  methods for elliptic partial differential equations
                  are considered with a special emphasis on problems
                  arising in elasticity. The focus of this survey
                  article is on Finite Element Tearing and
                  Interconnecting ({FETI)} methods, a family of
                  nonoverlapping domain decomposition methods where
                  the continuity between the subdomains, in principle,
                  is enforced by the use of Lagrange
                  multipliers. Exact onelevel and dual-primal {FETI}
                  methods as well as related inexact dual-primal
                  variants are described and theoretical convergence
                  estimates are presented together with numerical
                  results confirming the parallel scalability
                  properties of these methods. New aspects such as a
                  hybrid onelevel {FETI/FETI-DP} approach and the
                  behavior of {FETI-DP} for anisotropic elasticity
                  problems are presented. Parallel and numerical
                  scalability of the methods for more than 65 000
                  processor cores of the {JUGENE} supercomputer is
                  shown. An application of a dual-primal {FETI} method
                  to a nontrivial biomechanical problem from nonlinear
                  elasticity, modeling arterial wall stress, is given,
                  showing the robustness of our domain decomposition
                  methods for such problems.},
  language =	 {en},
  number =	 1,
  urldate =	 {2013-08-02},
  journal =	 {{ZAMM} - Journal of Applied Mathematics and
                  Mechanics / Zeitschrift für Angewandte Mathematik
                  und Mechanik},
  author =	 {Klawonn, A. and Rheinbach, O.},
  year =	 2010,
  keywords =	 {algebraic multigrid, arterial wall., biomechanics,
                  domain decomposition, elasticity, elliptic systems,
                  {FETI}, Finite elements, inexact, Lagrange
                  multipliers, multilevel methods, parallel computing,
                  preconditioners},
  pages =	 {5--32},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/5669IFT7/Klawonn
                  and Rheinbach - 2010 - Highly scalable parallel
                  domain decomposition
                  meth.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/WSVQVCJT/abstract.html:text/html}
}

@article{klawonn_nonlinear_2014,
  title =	 {Nonlinear {FETI}-{DP} and {BDDC} {Methods}},
  volume =	 36,
  issn =	 {1064-8275},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/130920563},
  doi =		 {10.1137/130920563},
  abstract =	 {New nonlinear FETI-DP (dual-primal finite element
                  tearing and interconnecting) and BDDC (balancing
                  domain decomposition by constraints) domain
                  decomposition methods are introduced. In all these
                  methods, in each iteration, local nonlinear problems
                  are solved on the subdomains. The new approaches can
                  significantly reduce communication and show a
                  significantly improved performance, especially for
                  problems with localized nonlinearities, compared to
                  a standard Newton--Krylov--FETI-DP or BDDC
                  approach. Moreover, the coarse space of the
                  nonlinear FETI-DP methods can be used to accelerate
                  the Newton convergence. It is also found that the
                  new nonlinear FETI-DP and nonlinear BDDC methods are
                  not as closely related as in the linear
                  context. Numerical results for the p-Laplace
                  operator are presented., New nonlinear FETI-DP
                  (dual-primal finite element tearing and
                  interconnecting) and BDDC (balancing domain
                  decomposition by constraints) domain decomposition
                  methods are introduced. In all these methods, in
                  each iteration, local nonlinear problems are solved
                  on the subdomains. The new approaches can
                  significantly reduce communication and show a
                  significantly improved performance, especially for
                  problems with localized nonlinearities, compared to
                  a standard Newton--Krylov--FETI-DP or BDDC
                  approach. Moreover, the coarse space of the
                  nonlinear FETI-DP methods can be used to accelerate
                  the Newton convergence. It is also found that the
                  new nonlinear FETI-DP and nonlinear BDDC methods are
                  not as closely related as in the linear
                  context. Numerical results for the p-Laplace
                  operator are presented.},
  number =	 2,
  urldate =	 {2015-01-21},
  journal =	 {SIAM Journal on Scientific Computing},
  author =	 {Klawonn, A. and Lanser, M. and Rheinbach, O.},
  year =	 2014,
  pages =	 {A737--A765},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/X8XPCZG3/Klawonn
                  et al. - 2014 - Nonlinear FETI-DP and BDDC
                  Methods.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/SM8TG5KR/130920563.html:text/html}
}

@article{kolossov_3d_2004,
  title =	 {{3D} {FE} simulation for temperature evolution in
                  the selective laser sintering process},
  volume =	 44,
  issn =	 {0890-6955},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S089069550300289X},
  doi =		 {10.1016/j.ijmachtools.2003.10.019},
  abstract =	 {A thermal model of selective laser sintering (SLS)
                  has been developed. The model allows for the
                  non-linear behavior of thermal conductivity and of
                  specific heat due to temperature changes and phase
                  transformations. The temperature evolution and the
                  formation of the sintered part are simulated by a 3D
                  finite element analysis based on continuous media
                  theory. It is shown that the effect of sintering has
                  a strong influence on thermal evolution through
                  changing the thermal properties of the material. The
                  results of the model were experimentally tested and
                  confirmed by temperature measurements.},
  number =	 {2-3},
  urldate =	 {2015-12-29},
  journal =	 {International Journal of Machine Tools and
                  Manufacture},
  author =	 {Kolossov, S. and Boillat, E. and Glardon, R. and
                  Fischer, P. and Locher, M.},
  year =	 2004,
  keywords =	 {finite element method, selective laser sintering},
  pages =	 {117--123},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/2MA2UTQJ/Kolossov
                  et al. - 2004 - 3D FE simulation for temperature
                  evolution in the .pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/SAAIF76A/S089069550300289X.html:text/html}
}

@article{korner_mesoscopic_2011,
  title =	 {Mesoscopic simulation of selective beam melting
                  processes},
  volume =	 211,
  issn =	 {0924-0136},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0924013610003869},
  doi =		 {10.1016/j.jmatprotec.2010.12.016},
  abstract =	 {A 2D lattice Boltzmann model is developed to
                  investigate melting and re-solidification of a
                  randomly packed powder bed under the irradiation of
                  a Gaussian beam during selective beam melting
                  processes. Numerical simulation results are
                  presented where individual powder particles are
                  considered. This approach makes many physical
                  phenomena accessible which cannot be described in a
                  standard continuum picture, e.g. the influence of
                  the relative powder density, the stochastic effect
                  of a randomly packed powder bed, capillary and
                  wetting phenomena. The proposed model, although
                  still 2D, is able to predict many experimental
                  observations such as the well known balling effect.
                  A process map is used to demonstrate the effect of
                  the process parameters, beam powder and scan
                  speed. The simulation results are compared with
                  experimental findings during Selective Electron Beam
                  Melting (SEBM). The comparison shows good agreement
                  between simulation results and experiments.},
  number =	 6,
  urldate =	 {2015-12-29},
  journal =	 {Journal of Materials Processing Technology},
  author =	 {K\"{o}rner, Carolin and Attar, Elham and Heinl,
                  Peter},
  year =	 2011,
  keywords =	 {additive manufacturing, Fluid dynamics, Heat
                  transfer, Lattice Boltzmann method, Selective beam
                  melting},
  pages =	 {978--987},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/Q6X4U9WK/Körner
                  et al. - 2011 - Mesoscopic simulation of selective
                  beam melting pr.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/8SFU36BT/S0924013610003869.html:text/html}
}

@article{krause_enabling_2016,
	title = {Enabling local time stepping in the parallel implicit solution of reaction–diffusion equations via space-time finite elements on shallow tree meshes},
	volume = {277},
	issn = {0096-3003},
	url = {http://www.sciencedirect.com/science/article/pii/S0096300315300059},
	doi = {10.1016/j.amc.2015.12.017},
	abstract = {For many applications, local time stepping offers an interesting and worthwhile alternative to the by now well established global time step control. In fact, local time stepping can allow for a highly detailed resolution of localized features of the solution with strongly reduced computational cost, when compared to global time step control. However local time stepping is not applicable in a straight-forward manner in the context of fully implicit time-discretizations.

Here, we present a method for the efficient parallel adaptive solution of (non-linear) partial differential equations, in particular reaction–diffusion equations, using spatially adapted time step sizes in the context of a fully implicit solution strategy. Our proposed method uses a discontinuous Galerkin method in-time approach within a full space-time approach. Moreover, it is designed from scratch for efficient parallel computation. We employ shallow tree-based mesh data structures in order to ensure a low memory footprint of the adaptive meshes. By solving the time-dependent partial differential equation on a ( d + 1 ) -dimensional non-conforming mesh, space-time adaptivity is naturally achieved. In combination with a discontinuous Galerkin method in-time the size of the arising systems can be precisely controlled.

We additionally introduce and discuss a stabilization scheme for space-time mortar element methods that also has a highly positive impact on the efficiency of preconditioning techniques for the arising systems of equations. We present results from extensive numerical experiments that address the question of convergence and efficiency, linear and non-linear solver performance, parallel scalability up to 2048 cores as well as accuracy for the linear heat equation and a real world, non-linear reaction–diffusion equation from the field of computational electrocardiology.},
	pages = {164--179},
        journal      = {Applied Mathematics and Computation},
	journaltitle = {Applied Mathematics and Computation},
	shortjournal = {Applied Mathematics and Computation},
	author = {Krause, D. and Krause, R.},
        year = {2016},
	urldate = {2017-01-10},
	date = {2016-03-20},
	keywords = {Implicit solution schemes, Local time stepping, parallelization, Reaction-diffusion equations, Space-time adaptivity},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/Q8S4CPV4/Krause and Krause - 2016 - Enabling local time stepping in the parallel impli.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/XMERUNRK/S0096300315300059.html:text/html}
}

@article{kritz_fusion_2009,
  title =	 {Fusion Simulation Project Workshop Report},
  volume =	 28,
  issn =	 {0164-0313},
  url =
                  {http://www.springerlink.com/content/tk2n1640151h2410/abstract/},
  doi =		 {10.1007/s10894-008-9151-4},
  abstract =	 {The mission of the Fusion Simulation Project is to
                  develop a predictive capability for the integrated
                  modeling of magnetically confined plasmas. This
                  {FSP} report adds to the previous activities that
                  defined an approach to integrated modeling in
                  magnetic fusion. These previous activities included
                  a Fusion Energy Sciences Advisory Committee panel
                  that was charged to study integrated simulation in
                  2002. The report of that panel [Journal of Fusion
                  Energy 20 , 135 (2001)] recommended the prompt
                  initiation of a Fusion Simulation Project. In 2003,
                  the Office of Fusion Energy Sciences formed a
                  steering committee that developed a project vision,
                  roadmap, and governance concepts [Journal of Fusion
                  Energy 23, 1 (2004)]. The current {FSP} planning
                  effort involved 46 physicists, applied
                  mathematicians and computer scientists, from 21
                  institutions, formed into four panels and a
                  coordinating committee. These panels were
                  constituted to consider: Status of Physics
                  Components, Required Computational and Applied
                  Mathematics Tools, Integration and Management of
                  Code Components, and Project Structure and
                  Management. The ideas, reported here, are the
                  products of these panels, working together over
                  several months and culminating in a 3-day workshop
                  in May 2007.},
  number =	 1,
  urldate =	 {2012-04-04},
  journal =	 {Journal of Fusion Energy},
  author =	 {Kritz, Arnold and Keyes, David},
  year =	 2009,
  keywords =	 {Engineering},
  pages =	 {1--59},
  file =	 {SpringerLink Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/IZR3FPB4/Kritz
                  and Keyes - 2009 - Fusion Simulation Project
                  Workshop Report.pdf:application/pdf;SpringerLink
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/F8J554VX/Kritz
                  and Keyes - 2009 - Fusion Simulation Project
                  Workshop Report.html:text/html}
}

@article{kundin_phase-field_2015,
  title =	 {Phase-field modeling of the microstructure evolution
                  and heterogeneous nucleation in solidifying ternary
                  {Al}-{Cu}-{Ni} alloys},
  volume =	 83,
  issn =	 {1359-6454},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S135964541400754X},
  doi =		 {10.1016/j.actamat.2014.09.057},
  abstract =	 {We have investigated the microstructure evolution
                  during the isothermal and non-isothermal
                  solidification of ternary Al-Cu-Ni alloys by means
                  of a general multi-phase-field model for an
                  arbitrary number of phases. The stability
                  requirements for the model functions on every dual
                  interface guarantee the absence of “ghost”
                  phases. The aim was to generate a realistic
                  microstructure by coupling the thermodynamic
                  parameters of the phases and the thermodynamically
                  consistent phase-field evolution equations. It is
                  shown that the specially constructed thermal noise
                  terms disturb the stability on the dual interfaces
                  and can produce heterogeneous nucleation of product
                  phases at energetically favorable points. Similar
                  behavior can be observed in triple junctions where
                  the heterogeneous nucleation of a fourth phase is
                  more favorable. Finally, the model predicts the
                  growth of a combined eutectic-like and
                  peritectic-like structure that is comparable to the
                  observed experimental microstructure in various
                  alloys.},
  urldate =	 {2016-01-15},
  journal =	 {Acta Materialia},
  author =	 {Kundin, Julia and Pogorelov, Evgeny and Emmerich,
                  Heike},
  year =	 2015,
  keywords =	 {Heterophase nucleation, Multicomponent, Multiphase,
                  Phase-field model},
  pages =	 {448--459},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/M33IXRE4/Kundin
                  et al. - 2015 - Phase-field modeling of the
                  microstructure
                  evoluti.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/UPKW3KRC/S135964541400754X.html:text/html}
}

@article{kuo_quasi-monte_2012,
  title =	 {Quasi-{Monte} {Carlo} {Finite} {Element} {Methods}
                  for a {Class} of {Elliptic} {Partial} {Differential}
                  {Equations} with {Random} {Coefficients}},
  volume =	 50,
  issn =	 {0036-1429},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/110845537},
  doi =		 {10.1137/110845537},
  abstract =	 {In this paper quasi-Monte Carlo (QMC) methods are
                  applied to a class of elliptic partial differential
                  equations (PDEs) with random coefficients, where the
                  random coefficient is parametrized by a countably
                  infinite number of terms in a Karhunen--Loève
                  expansion. Models of this kind appear frequently in
                  numerical models of physical systems, and in
                  uncertainty quantification. The method uses a QMC
                  method to estimate expected values of linear
                  functionals of the exact or approximate solution of
                  the PDE, with the expected value considered as an
                  infinite dimensional integral in the parameter space
                  corresponding to the randomness induced by the
                  random coefficient. The error analysis, arguably the
                  first rigorous application of the modern theory of
                  QMC in weighted spaces, exploits the regularity with
                  respect to both the physical variables (the
                  variables in the physical domain) and the parametric
                  variables (the parameters corresponding to
                  randomness). In the weighted-space theory of QMC
                  methods, “weights,” describing the varying
                  difficulty of different subsets of the variables,
                  are introduced in order to make sure that the
                  high-dimensional integration problem is
                  tractable. It turns out that the weights arising
                  from the present analysis are of a nonstandard kind,
                  being of neither product nor order dependent form,
                  but instead a hybrid of the two---we refer to these
                  as “product and order dependent weights,” or “POD
                  weights” for short. These POD weights are of a
                  simple enough form to permit a
                  component-by-component construction of a randomly
                  shifted lattice rule that has optimal convergence
                  properties for the given weighted space setting. If
                  the terms in the expansion for the random
                  coefficient have an appropriate decay property, and
                  if we choose POD weights that minimize a certain
                  upper bound on the error, then the solution of the
                  PDE belongs to the joint function space needed for
                  the analysis, and the QMC error (in the sense of a
                  root-mean-square error averaged over shifts) is of
                  order
                  \${\textbackslash}mathcal\{O\}(N{\textasciicircum}\{-1+{\textbackslash}delta\})\$
                  for arbitrary
                  \${\textbackslash}delta{\textgreater}0\$, where
                  \$N\$ denotes the number of sampling points in the
                  parameter space. Moreover, this convergence rate and
                  slower convergence rates under more relaxed
                  conditions are achieved under conditions similar to
                  those found recently by Cohen, De Vore, and Schwab
                  [Found. Comput. Math., 10 (2010), pp. 615--646] for
                  the same model by best \$N\$-term approximation. We
                  analyze the impact of a finite element (FE)
                  discretization on the overall efficiency of the
                  scheme, in terms of accuracy versus overall cost,
                  with results that are comparable to those of the
                  best \$N\$-term approximation.},
  number =	 6,
  urldate =	 {2016-01-15},
  journal =	 {SIAM Journal on Numerical Analysis},
  author =	 {Kuo, F. and Schwab, C. and Sloan, I.},
  year =	 2012,
  pages =	 {3351--3374},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/4Q2D7IM5/Kuo
                  et al. - 2012 - Quasi-Monte Carlo Finite Element
                  Methods for a
                  Cla.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/BVW7T4KX/110845537.html:text/html}
}

@inproceedings{langer_scalable_2012,
	title = {Scalable {Algorithms} for {Distributed}-{Memory} {Adaptive} {Mesh} {Refinement}},
	isbn = {978-0-7695-4907-1 978-1-4673-4790-7},
	url = {http://ieeexplore.ieee.org/document/6374777/},
	doi = {10.1109/SBAC-PAD.2012.48},
	urldate = {2017-01-13},
	publisher = {IEEE},
	author = {Langer, Akhil and Lifflander, Jonathan and Miller, Phil and Pan, Kuo-Chuan and Kale, Laxmikant V. and Ricker, Paul},
	year = {2012},
	pages = {100--107},
        booktitle = {2012 IEEE 24th International Symposium on Computer Architecture and High Performance Computing},
}

@techreport{lanl,
  title =	 {Towards Modeling and Simulation of Additive
                  Manufacturing of Metals at {LANL}},
  urldate =	 {2015-12-29},
  url =
                  {http://sites.nationalacademies.org/cs/groups/pgasite/documents/webpage/pga_168755.pdf},
  institution =	 {National Institute of Standards and Technology},
  author =	 {Marianne M. Fran\c{c}ois},
  year =	 2013
}

@article{larsson_adaptive_2006,
  series =	 {Adaptive {Modeling} and {Simulation}},
  title =	 {Adaptive computational meso-macro-scale modeling of
                  elastic composites},
  volume =	 195,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782505000885},
  doi =		 {10.1016/j.cma.2004.04.012},
  abstract =	 {We consider combined material model and FE-mesh
                  adaptivity in the context of computational
                  meso-macro-scale modeling (CMM), whereby the
                  coupling between different geometrical scales is
                  taken into account. The macro-scale model is then
                  replaced by the homogenized response of a
                  representative volume element (RVE). In practice,
                  the RVE is analyzed as a “small-scale” finite
                  element problem, which is solved for each (Gauss)
                  point in the domain. The high cost of this strategy
                  clearly motivates the use of adaptive techniques
                  based on a posteriori error estimates for the
                  combined model and discretization error, whereby the
                  solution of an appropriate dual problem is
                  utilized. The suggested adaptive procedure is
                  applied to a particle/matrix composite with
                  hyperelastic constituents.},
  number =	 {4-6},
  urldate =	 {2015-12-29},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Larsson, Fredrik and Runesson, Kenneth},
  year =	 2006,
  pages =	 {324--338},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/V363FRPM/Larsson
                  and Runesson - 2006 - Adaptive computational
                  meso-macro-scale modeling
                  o.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/6GJUKQCG/S0045782505000885.html:text/html}
}

@incollection{lassila_model_2014,
  series =	 {{MS}\&{A} - {Modeling}, {Simulation} and
                  {Applications}},
  title =	 {Model {Order} {Reduction} in {Fluid} {Dynamics}:
                  {Challenges} and {Perspectives}},
  copyright =	 {©2014 Springer International Publishing Switzerland},
  isbn =	 {978-3-319-02089-1 978-3-319-02090-7},
  shorttitle =	 {Model {Order} {Reduction} in {Fluid} {Dynamics}},
  url =
                  {http://link.springer.com/chapter/10.1007/978-3-319-02090-7_9},
  abstract =	 {This chapter reviews techniques of model reduction
                  of fluid dynamics systems. Fluid systems are known
                  to be difficult to reduce efficiently due to several
                  reasons. First of all, they exhibit strong
                  nonlinearities — which are mainly related either to
                  nonlinear convection terms and/or some geometric
                  variability — that often cannot be treated by simple
                  linearization. Additional difficulties arise when
                  attempting model reduction of unsteady flows,
                  especially when long-term transient behavior needs
                  to be accurately predicted using reduced order
                  models and more complex features, such as turbulence
                  or multiphysics phenomena, have to be taken into
                  consideration. We first discuss some general
                  principles that apply to many parametric model order
                  reduction problems, then we apply them on steady and
                  unsteady viscous flows modelled by the
                  incompressible Navier-Stokes equations. We address
                  questions of inf-sup stability, certification
                  through error estimation, computational issues and —
                  in the unsteady case — long-time stability of the
                  reduced model. Moreover, we provide an extensive
                  list of literature references.},
  language =	 {en},
  number =	 9,
  urldate =	 {2015-12-31},
  booktitle =	 {Reduced {Order} {Methods} for {Modeling} and
                  {Computational} {Reduction}},
  publisher =	 {Springer International Publishing},
  author =	 {Lassila, Toni and Manzoni, Andrea and Quarteroni,
                  Alfio and Rozza, Gianluigi},
  editor =	 {Quarteroni, Alfio and Rozza, Gianluigi},
  year =	 2014,
  keywords =	 {Appl.Mathematics/Computational Methods of
                  Engineering, Computational Mathematics and Numerical
                  Analysis, Continuum Mechanics and Mechanics of
                  Materials, Mathematical Modeling and Industrial
                  Mathematics, Numerical and Computational Physics,
                  Numeric Computing},
  pages =	 {235--273},
  file =
                  {LMQR_ROMReview.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/BKAGR7QU/LMQR_ROMReview.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/385IKMDG/10.html:text/html}
}

@article{le_bars_solidification_2006,
  title =	 {Solidification of a binary alloy: {Finite}-element,
                  single-domain simulation and new benchmark
                  solutions},
  volume =	 216,
  issn =	 00219991,
  shorttitle =	 {Solidification of a binary alloy},
  url =
                  {http://linkinghub.elsevier.com/retrieve/pii/S0021999105005504},
  doi =		 {10.1016/j.jcp.2005.12.002},
  language =	 {en},
  number =	 1,
  urldate =	 {2015-12-29},
  journal =	 {Journal of Computational Physics},
  author =	 {Le Bars, Michael and Worster, M. Grae},
  year =	 2006,
  pages =	 {247--263},
  file =
                  {74.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/CRRW4TS3/74.pdf:application/pdf}
}

@inproceedings{lee_mesoscopic_????,
  title =	 {Mesoscopic {Simulation} of {Heat} {Transfer} and
                  {Fluid} {Flow} in {Laser} {Powder} {Bed} {Additive}
                  {Manufacturing}},
  url =
                  {http://sffsymposium.engr.utexas.edu/sites/default/files/2015/2015-94-Lee.pdf},
  urldate =	 {2015-12-29},
  booktitle =	 {Proceedings of the International Solid Freeform
                  Fabrication Symposium},
  year =	 2015,
  author =	 {Lee, Y. S. and Zhang, W.},
  file =
                  {2015-94-Lee.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/BRAX4TVB/2015-94-Lee.pdf:application/pdf}
}

@article{lee_multi-scale_2009,
  title =	 {Multi-scale homogenization of moving interface
                  problems with flux jumps: application to
                  solidification},
  volume =	 44,
  issn =	 {0178-7675, 1432-0924},
  shorttitle =	 {Multi-scale homogenization of moving interface
                  problems with flux jumps},
  url =		 {http://link.springer.com/10.1007/s00466-009-0371-x},
  doi =		 {10.1007/s00466-009-0371-x},
  language =	 {en},
  number =	 3,
  urldate =	 {2015-12-29},
  journal =	 {Computational Mechanics},
  author =	 {Lee, Sangmin and Sundararaghavan, Veera},
  year =	 2009,
  pages =	 {297--307},
  file =
                  {mainsolid.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/S66T6HJF/mainsolid.pdf:application/pdf}
}

@article{li_bddc_2006,
  title =	 {{BDDC} Algorithms for Incompressible Stokes
                  Equations},
  volume =	 44,
  issn =	 {0036-1429, 1095-7170},
  url =
                  {http://epubs.siam.org/action/showAbstract?page=2432&volume=44&issue=6&journalCode=sjnaam},
  doi =		 {10.1137/050628556},
  number =	 6,
  urldate =	 {2012-10-11},
  journal =	 {{SIAM} Journal on Numerical Analysis},
  author =	 {Li, Jing and Widlund, Olof},
  year =	 2006,
  pages =	 {2432--2455},
  file =
                  {050628556.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/79C5BSRT/050628556.pdf:application/pdf;BDDC
                  Algorithms for Incompressible Stokes Equations :
                  SIAM Journal on Numerical Analysis: Vol. 44, No. 6
                  (Society for Industrial and Applied
                  Mathematics):/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/9VGU39VF/showAbstract.html:text/html}
},

@article{li_bddc_2006,
  title =	 {{BDDC} Algorithms for Incompressible Stokes
                  Equations},
  volume =	 44,
  issn =	 {0036-1429, 1095-7170},
  url =
                  {http://epubs.siam.org/action/showAbstract?page=2432&volume=44&issue=6&journalCode=sjnaam},
  doi =		 {10.1137/050628556},
  number =	 6,
  urldate =	 {2012-10-11},
  journal =	 {{SIAM} Journal on Numerical Analysis},
  author =	 {Li, Jing and Widlund, Olof},
  year =	 2006,
  pages =	 {2432--2455},
  file =
                  {050628556.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/79C5BSRT/050628556.pdf:application/pdf;BDDC
                  Algorithms for Incompressible Stokes Equations :
                  SIAM Journal on Numerical Analysis: Vol. 44, No. 6
                  (Society for Industrial and Applied
                  Mathematics):/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/9VGU39VF/showAbstract.html:text/html}
}

@ARTICLE{lions_resolution_2001,
  author =	 {Lions, Jacques-Louis and Maday, Yvon and Turinici,
                  Gabriel},
  title =	 {R{\'e}solution {d'EDP} par un sch{\'e}ma en temps
                  "parar{\'e}l"},
  journal =	 {Comptes Rendus de {l'Acad\'emie} des Sciences -
                  Series I - Mathematics},
  year =	 2001,
  volume =	 332,
  pages =	 {661--668},
  number =	 7,
  abstract =	 {On propose dans cette Note un schÃ©ma permettant de
                  profiter d'une architecture parallÃ¨le pour la
                  discrÃ©tisation en temps d'une Ã©quation
                  d'Ã©volution aux dÃ©rivÃ©es partielles. Cette
                  mÃ©thode, basÃ©e sur un schÃ©ma {d'Euler}, combine
                  des rÃ©solutions grossiÃ¨res et des rÃ©solutions
                  fines et indÃ©pendantes en temps en s'inspirant de
                  ce qui est classique en espace. La parallÃ©lisation
                  qui en rÃ©sulte se fait dans la direction temporelle
                  ce qui est en revanche non classique.  Elle a pour
                  principale motivation les problÃ¨mes en temps rÃ©el,
                  d'oÃ¹ la terminologie proposÃ©e de Â«pararÃ©el
                  Â». The purpose of this Note is to propose a time
                  discretization of a partial differential evolution
                  equation that allows for parallel
                  implementations. The method, based on an Euler
                  scheme, combines coarse resolutions and independent
                  fine resolutions in time in the same spirit as
                  standard spacial approximations. The resulting
                  parallel implementation is done in the non standard
                  time direction. Its main goal concerns real time
                  problems, hence the proposed terminology of
                  âpararealâ algorithm.},
  doi =		 {10.1016/S0764-4442(00)01793-6},
  file =	 {ScienceDirect Full Text
                  PDF:/home/principe/Javier/System/Config/firefox/zotero/storage/C28KWQZ7/Lions
                  et al. - 2001 - RÃ©solution d'EDP par un schÃ©ma en
                  temps Â«pararÃ©el .pdf:application/pdf;ScienceDirect
                  Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/2CTXJZX4/S0764444200017936.html:text/html},
  issn =	 {0764-4442},
  owner =	 {principe},
  timestamp =	 {2014.03.22},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0764444200017936},
  urldate =	 {2014-03-22}
}

@ARTICLE{lions_resolution_2001,
  author =	 {Lions, Jacques-Louis and Maday, Yvon and Turinici,
                  Gabriel},
  title =	 {R{\'e}solution {d'EDP} par un sch{\'e}ma en temps
                  "parar{\'e}l"},
  journal =	 {Comptes Rendus de {l'Acad\'emie} des Sciences -
                  Series I - Mathematics},
  year =	 2001,
  volume =	 332,
  pages =	 {661--668},
  number =	 7,
  abstract =	 {On propose dans cette Note un schÃ©ma permettant de
                  profiter d'une architecture parallÃ¨le pour la
                  discrÃ©tisation en temps d'une Ã©quation
                  d'Ã©volution aux dÃ©rivÃ©es partielles. Cette
                  mÃ©thode, basÃ©e sur un schÃ©ma {d'Euler}, combine
                  des rÃ©solutions grossiÃ¨res et des rÃ©solutions
                  fines et indÃ©pendantes en temps en s'inspirant de
                  ce qui est classique en espace. La parallÃ©lisation
                  qui en rÃ©sulte se fait dans la direction temporelle
                  ce qui est en revanche non classique.  Elle a pour
                  principale motivation les problÃ¨mes en temps rÃ©el,
                  d'oÃ¹ la terminologie proposÃ©e de Â«pararÃ©el
                  Â». The purpose of this Note is to propose a time
                  discretization of a partial differential evolution
                  equation that allows for parallel
                  implementations. The method, based on an Euler
                  scheme, combines coarse resolutions and independent
                  fine resolutions in time in the same spirit as
                  standard spacial approximations. The resulting
                  parallel implementation is done in the non standard
                  time direction. Its main goal concerns real time
                  problems, hence the proposed terminology of
                  âpararealâ algorithm.},
  doi =		 {10.1016/S0764-4442(00)01793-6},
  file =	 {ScienceDirect Full Text
                  PDF:/home/principe/Javier/System/Config/firefox/zotero/storage/C28KWQZ7/Lions
                  et al. - 2001 - RÃ©solution d'EDP par un schÃ©ma en
                  temps Â«pararÃ©el .pdf:application/pdf;ScienceDirect
                  Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/2CTXJZX4/S0764444200017936.html:text/html},
  issn =	 {0764-4442},
  owner =	 {principe},
  timestamp =	 {2014.03.22},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0764444200017936},
  urldate =	 {2014-03-22}
}

@techreport{liou_multiscale_2015,
  title =	 {Multiscale and {Multiphysics} {Modeling} of
                  {Additive} {Manufacturing} of {Advanced}
                  {Materials}},
  url =
                  {http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20150003789.pdf},
  urldate =	 {2015-12-29},
  author =	 {Liou, Frank and Newkirk, Joseph and Fan, Zhiqiang
                  and Sparks, Todd and Chen, Xueyang and Fletcher,
                  Kenneth and Zhang, Jingwei and Zhang, Yunlu and
                  Kumar, Kannan Suresh and Karnati, Sreekar},
  year =	 2015,
  institution =	 {NASA},
  file =
                  {20150003789.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/EFHKPBCF/20150003789.pdf:application/pdf}
}

@book{lord_introduction_2014,
  title =	 {An {Introduction} to {Computational} {Stochastic}
                  {PDEs}},
  isbn =	 {978-0-521-89990-1},
  abstract =	 {This book gives a comprehensive introduction to
                  numerical methods and analysis of stochastic
                  processes, random fields and stochastic differential
                  equations, and offers graduate students and
                  researchers powerful tools for understanding
                  uncertainty quantification for risk
                  analysis. Coverage includes traditional stochastic
                  ODEs with white noise forcing, strong and weak
                  approximation, and the multi-level Monte Carlo
                  method. Later chapters apply the theory of random
                  fields to the numerical solution of elliptic PDEs
                  with correlated random data, discuss the Monte Carlo
                  method, and introduce stochastic Galerkin
                  finite-element methods. Finally, stochastic
                  parabolic PDEs are developed. Assuming little
                  previous exposure to probability and statistics,
                  theory is developed in tandem with state-of the art
                  computational methods through worked examples,
                  exercises, theorems and proofs. The set of MATLAB
                  codes included (and downloadable) allows readers to
                  perform computations themselves and solve the test
                  problems discussed. Practical examples are drawn
                  from finance, mathematical biology, neuroscience,
                  fluid flow modeling and materials science.},
  language =	 {en},
  publisher =	 {Cambridge University Press},
  author =	 {Lord, Gabriel J. and Powell, Catherine E. and
                  Shardlow, Tony},
  year =	 2014,
  keywords =	 {Business \& Economics / Finance / General,
                  Mathematics / Differential Equations / General,
                  Mathematics / Discrete Mathematics, Mathematics /
                  Game Theory, Mathematics / General, Mathematics /
                  Numerical Analysis, Mathematics / Probability \&
                  Statistics / General, Mathematics / Probability \&
                  Statistics / Stochastic Processes}
}

@article{lovgren_reduced_2006,
  title =	 {A reduced basis element method for the steady
                  {Stokes} problem},
  volume =	 40,
  issn =	 {0764-583X, 1290-3841},
  url =		 {http://www.esaim-m2an.org/10.1051/m2an:2006021},
  doi =		 {10.1051/m2an:2006021},
  number =	 3,
  urldate =	 {2016-01-15},
  journal =	 {ESAIM: Mathematical Modelling and Numerical
                  Analysis},
  author =	 {L{\o}vgren, Alf Emil and Maday, Yvon and
                  R{\o}nquist, Einar M.},
  year =	 2006,
  pages =	 {529--552},
  file =
                  {m2an0484.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/XRQW7QTZ/m2an0484.pdf:application/pdf}
}

%% hal-00839487, version 2
%% http://hal.archives-ouvertes.fr/hal-00839487

@inproceedings{magnor_spacetime-coherent_2004,
  title =	 {Spacetime-coherent geometry reconstruction from
                  multiple video streams},
  doi =		 {10.1109/TDPVT.2004.1335231},
  abstract =	 {By reconstructing time-varying geometry one frame at
                  a time, one ignores the continuity of natural
                  motion, wasting useful information about the
                  underlying video-image formation process and taking
                  into account temporally discontinuous reconstruction
                  results. In 4D spacetime, the surface of a dynamic
                  object describes a continuous 3D hyper-surface. This
                  hyper-surface can be implicitly defined as the
                  minimum of an energy functional designed to optimize
                  photo-consistency. Based on an Eider-Lagrange
                  reformulation of the problem, we find this
                  hyper-surface from a handful of synchronized video
                  recordings. The resulting object geometry varies
                  smoothly over time, and intermittently invisible
                  object regions are correctly interpolated from
                  previously and/or future frames.},
  booktitle =	 {Proceedings of the 2nd {International} {Symposium}
                  on 3D {Data} {Processing}, {Visualization} and
                  {Transmission}},
  author =	 {Magnor, M. and Goldlucke, B.},
  year =	 2004,
  keywords =	 {3D hyper-surface, computational geometry,
                  Eider-Lagrange reformulation, image reconstruction,
                  Information geometry, Layout, Level set, multiple
                  video stream, object geometry, photo-consistency,
                  Shape, Solid modeling, solid modelling,
                  spacetime-coherent geometry reconstruction,
                  Streaming media, Surface reconstruction,
                  synchronized video recording, temporally
                  discontinuous reconstruction, video-image formation,
                  Video recording, video signal processing},
  pages =	 {365--372},
  file =	 {IEEE Xplore Abstract
                  Record:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/7IUCSIIS/articleDetails.html:text/html;IEEE
                  Xplore Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/AUBECT3M/Magnor
                  and Goldlucke - 2004 - Spacetime-coherent geometry
                  reconstruction from mu.pdf:application/pdf}
}

@article{mandel_algebraic_2005,
  title =	 {An algebraic theory for primal and dual
                  substructuring methods by constraints},
  volume =	 54,
  issn =	 {0168-9274},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0168927404001795},
  doi =		 {10.1016/j.apnum.2004.09.022},
  abstract =	 {{FETI} and {BDD} are two widely used substructuring
                  methods for the solution of large sparse systems of
                  linear algebraic equations arising from
                  discretization of elliptic boundary value
                  problems. The two most advanced variants of these
                  methods are the {FETI-DP} and the {BDDC} methods,
                  whose formulation does not require any information
                  beyond the algebraic system of equations in a
                  substructure form. We formulate the {FETI-DP} and
                  the {BDDC} methods in a common framework as methods
                  based on general constraints between the
                  substructures, and provide a simplified algebraic
                  convergence theory. The basic implementation blocks
                  including transfer operators are common to both
                  methods. It is shown that commonly used properties
                  of the transfer operators in fact determine the
                  operators uniquely. Identical algebraic condition
                  number bounds for both methods are given in terms of
                  a single inequality, and, under natural additional
                  assumptions, it is proved that the eigenvalues of
                  the preconditioned problems are the same. The
                  algebraic bounds imply the usual polylogarithmic
                  bounds for finite elements, independent of
                  coefficient jumps between
                  substructures. Computational experiments confirm the
                  theory.},
  number =	 2,
  urldate =	 {2012-02-27},
  journal =	 {Applied Numerical Mathematics},
  author =	 {Mandel, Jan and Dohrmann, Clark R. and Tezaur,
                  Radek},
  year =	 2005,
  keywords =	 {Balancing domain decomposition, {BDD}, {BDDC},
                  Dual-primal, {FETI}, {FETI-DP}, Finite element
                  tearing and interconnecting, Iterative
                  substructuring, Lagrange multipliers},
  pages =	 {167--193},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/UKIB3D4A/Mandel
                  et al. - 2005 - An algebraic theory for primal and
                  dual substructu.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/4TWHPXDM/Mandel
                  et al. - 2005 - An algebraic theory for primal and
                  dual substructu.html:text/html}
}

@article{mandel_multispace_2008,
  title =	 {Multispace and multilevel {BDDC}},
  volume =	 83,
  issn =	 {0010-485X},
  url =
                  {http://www.springerlink.com/content/112v4w1821r584u0/abstract/},
  doi =		 {10.1007/s00607-008-0014-7},
  abstract =	 {The Balancing Domain Decomposition by Constraints
                  (BDDC) method is the most advanced method from the
                  Balancing family of iterative substructuring methods
                  for the solution of large systems of linear
                  algebraic equations arising from discretization of
                  elliptic boundary value problems. In the case of
                  many substructures, solving the coarse problem
                  exactly becomes a bottleneck. Since the coarse
                  problem in BDDC has the same structure as the
                  original problem, it is straightforward to apply the
                  BDDC method recursively to solve the coarse problem
                  only approximately. In this paper, we formulate a
                  new family of abstract Multispace BDDC methods and
                  give condition number bounds from the abstract
                  additive Schwarz preconditioning theory. The
                  Multilevel BDDC is then treated as a special case of
                  the Multispace BDDC and abstract multilevel
                  condition number bounds are given. The abstract
                  bounds yield polylogarithmic condition number bounds
                  for an arbitrary fixed number of levels and scalar
                  elliptic problems discretized by finite elements in
                  two and three spatial dimensions. Numerical
                  experiments confirm the theory.},
  number =	 2,
  urldate =	 {2012-10-11},
  journal =	 {Computing},
  author =	 {Mandel, Jan and Soused\'ik, Bedřich and Dohrmann,
                  Clark},
  year =	 2008,
  keywords =	 {Computer Science},
  pages =	 {55--85},
  file =	 {SpringerLink Full Text
                  PDF:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/SZJFGZ9U/Mandel
                  et al. - 2008 - Multispace and multilevel
                  BDDC.pdf:application/pdf;SpringerLink
                  Snapshot:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/SRZRIPQS/abstract.html:text/html}
}


@article{mandel_multispace_2008,
  title =	 {Multispace and multilevel {BDDC}},
  volume =	 83,
  issn =	 {0010-{485X}},
  url =
                  {http://www.springerlink.com/content/112v4w1821r584u0/abstract/},
  doi =		 {10.1007/s00607-008-0014-7},
  abstract =	 {The Balancing Domain Decomposition by Constraints
                  ({BDDC)} method is the most advanced method from the
                  Balancing family of iterative substructuring methods
                  for the solution of large systems of linear
                  algebraic equations arising from discretization of
                  elliptic boundary value problems. In the case of
                  many substructures, solving the coarse problem
                  exactly becomes a bottleneck. Since the coarse
                  problem in {BDDC} has the same structure as the
                  original problem, it is straightforward to apply the
                  {BDDC} method recursively to solve the coarse
                  problem only approximately. In this paper, we
                  formulate a new family of abstract Multispace {BDDC}
                  methods and give condition number bounds from the
                  abstract additive Schwarz preconditioning
                  theory. The Multilevel {BDDC} is then treated as a
                  special case of the Multispace {BDDC} and abstract
                  multilevel condition number bounds are given. The
                  abstract bounds yield polylogarithmic condition
                  number bounds for an arbitrary fixed number of
                  levels and scalar elliptic problems discretized by
                  finite elements in two and three spatial
                  dimensions. Numerical experiments confirm the
                  theory.},
  number =	 2,
  urldate =	 {2012-10-11},
  journal =	 {Computing},
  author =	 {Mandel, Jan and Sousedík, Bedřich and Dohrmann,
                  Clark},
  year =	 2008,
  keywords =	 {Computer Science},
  pages =	 {55--85},
  file =	 {SpringerLink Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/X8WI3VIT/Mandel
                  et al. - 2008 - Multispace and multilevel
                  BDDC.pdf:application/pdf;SpringerLink
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/N67RJFGP/abstract.html:text/html}
}

@article{mandel_multispace_2008,
  title =	 {Multispace and multilevel {BDDC}},
  volume =	 83,
  issn =	 {0010-{485X}},
  url =
                  {http://www.springerlink.com/content/112v4w1821r584u0/abstract/},
  doi =		 {10.1007/s00607-008-0014-7},
  abstract =	 {The Balancing Domain Decomposition by Constraints
                  ({BDDC)} method is the most advanced method from the
                  Balancing family of iterative substructuring methods
                  for the solution of large systems of linear
                  algebraic equations arising from discretization of
                  elliptic boundary value problems. In the case of
                  many substructures, solving the coarse problem
                  exactly becomes a bottleneck. Since the coarse
                  problem in {BDDC} has the same structure as the
                  original problem, it is straightforward to apply the
                  {BDDC} method recursively to solve the coarse
                  problem only approximately. In this paper, we
                  formulate a new family of abstract Multispace {BDDC}
                  methods and give condition number bounds from the
                  abstract additive Schwarz preconditioning
                  theory. The Multilevel {BDDC} is then treated as a
                  special case of the Multispace {BDDC} and abstract
                  multilevel condition number bounds are given. The
                  abstract bounds yield polylogarithmic condition
                  number bounds for an arbitrary fixed number of
                  levels and scalar elliptic problems discretized by
                  finite elements in two and three spatial
                  dimensions. Numerical experiments confirm the
                  theory.},
  number =	 2,
  urldate =	 {2012-10-11},
  journal =	 {Computing},
  author =	 {Mandel, Jan and Sousedík, Bedřich and Dohrmann,
                  Clark},
  year =	 2008,
  keywords =	 {Computer Science},
  pages =	 {55--85},
  file =	 {SpringerLink Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/X8WI3VIT/Mandel
                  et al. - 2008 - Multispace and multilevel
                  BDDC.pdf:application/pdf;SpringerLink
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/N67RJFGP/abstract.html:text/html}
}

@article{martin_practical_2000,
	title = {Practical Ray Tracing of Trimmed {NURBS} Surfaces},
	volume = {5},
	issn = {1086-7651},
	url = {http://www-tandfonline-com.recursos.biblioteca.upc.edu/doi/abs/10.1080/10867651.2000.10487519},
	doi = {10.1080/10867651.2000.10487519},
	abstract = {A system is presented for ray tracing trimmed {NURBS} surfaces. While approaches to components are drawn largely from existing literature, their combination within a single framework is novel. This paper also differs from prior work in that the details of an efficient implementation are fleshed out. Throughout, emphasis is placed on practical methods suitable to implementation in general ray-tracing programs.},
	pages = {27--52},
	number = {1},
        journal       = {Journal of Graphics Tools},
	journaltitle = {Journal of Graphics Tools},
	shortjournal = {Journal of Graphics Tools},
	author = {Martin, William and Cohen, Elaine and Fish, Russell and Shirley, Peter},
        year   = {2000},
	urldate = {2017-01-02},
	date = {2000-01-01},
	file = {raynurbs.pdf:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/KEATPZID/raynurbs.pdf:application/pdf}
}

@misc{materials_genome,
  title =	 {{M}aterials {Genome} {Initiative}},
  howpublished = {\url{https://www.whitehouse.gov/mgi}},
  url =		 {https://www.whitehouse.gov/mgi},
  abstract =	 {The Materials Genome Initiative is a multi-agency
                  initiative designed to create a new era of policy,
                  resources, and infrastructure that support
                  U.S. institutions in the effort to discover,
                  manufacture, and deploy advanced materials twice as
                  fast, at a fraction of the cost.},
  urldate =	 {2016-01-15},
  file =
                  {Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/94WVIXR5/mgi.html:text/html}
}

@book{wohlers2016wohlers,
  title={Wohlers Report 2016: 3D Printing and Additive Manufacturing State of the Industry : Annual Worldwide Progress Report},
  author={Wohlers, T.T. and Wohlers Associates and Campbell, R.I. and Caffrey, T.},
  isbn={9780991333226},
  url={https://books.google.es/books?id=YHUJkAEACAAJ},
  year={2016},
  publisher={Wohlers Associates}
}

@article{matous_finite_2004,
  title =	 {Finite element formulation for modelling large
                  deformations in elasto-viscoplastic polycrystals},
  volume =	 60,
  issn =	 {1097-0207},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nme.1045/abstract},
  doi =		 {10.1002/nme.1045},
  abstract =	 {Anisotropic, elasto-viscoplastic behaviour in
                  polycrystalline materials is modelled using a new,
                  updated Lagrangian formulation based on a
                  three-field form of the Hu-Washizu variational
                  principle to create a stable finite element method
                  in the context of nearly incompressible
                  behaviour. The meso-scale is characterized by a
                  representative volume element, which contains grains
                  governed by single crystal behaviour. A new, fully
                  implicit, two-level, backward Euler integration
                  scheme together with an efficient finite element
                  formulation, including consistent linearization, is
                  presented. The proposed finite element model is
                  capable of predicting non-homogeneous meso-fields,
                  which, for example, may impact subsequent
                  recrystallization. Finally, simple deformations
                  involving an aluminium alloy are considered in order
                  to demonstrate the algorithm. Copyright © 2004 John
                  Wiley \& Sons, Ltd.},
  language =	 {en},
  number =	 14,
  urldate =	 {2015-12-31},
  journal =	 {International Journal for Numerical Methods in
                  Engineering},
  author =	 {Matou\v{s}, Karel and Maniatty, Antoinette M.},
  year =	 2004,
  keywords =	 {consistent tangent, finite deformation, Finite
                  element, integration algorithm, polycrystal},
  pages =	 {2313--2333},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/WQ6S3KT9/Matouš
                  and Maniatty - 2004 - Finite element formulation for
                  modelling large
                  def.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/6CAXSKZ4/abstract.html:text/html}
}

@article{michele_val_2,
  title =	 {Numerical simulation and experimental calibration of Additive Manufacturing by blown powder technology. Part I: thermal analysis},
  journal =	 {Rapid Prototyping Journal},
  author =	 {M. Chiumenti and X. Lin and M. Cervera and W. Lei and Y. Zheng and W. Huang},
  year =	 2016,
  volume =	 {In press}
}

@article{mittal_immersed_2005,
  title =	 {Immersed {Boundary} {Methods}},
  volume =	 37,
  url =
                  {http://dx.doi.org/10.1146/annurev.fluid.37.061903.175743},
  doi =		 {10.1146/annurev.fluid.37.061903.175743},
  number =	 1,
  urldate =	 {2016-01-14},
  journal =	 {Annual Review of Fluid Mechanics},
  author =	 {Mittal, Rajat and Iaccarino, Gianluca},
  year =	 2005,
  pages =	 {239--261},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/DM29WPEE/Mittal
                  and Iaccarino - 2005 - Immersed Boundary
                  Methods.pdf:application/pdf}
}

@TechReport{ml-guide,
  author =	 {M. W. Gee and C. M. Siefert and J. J. Hu and
                  R. S. Tuminaro and M. G. Sala},
  title =	 {{ML} 5.0 Smoothed Aggregation User's Guide},
  institution =	 {Sandia National Laboratories},
  year =	 2006,
  number =	 {SAND2006-2649},
}

@article{murakawa_applications_2012,
  title =	 {Applications of inherent strain and interface
                  element to simulation of welding deformation in thin
                  plate structures},
  volume =	 51,
  issn =	 {0927-0256},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S092702561100382X},
  doi =		 {10.1016/j.commatsci.2011.06.040},
  abstract =	 {Welding-induced distortion not only reduces largely
                  manufacturing accuracy but also decreases
                  significantly productivity due to correction
                  works. If welding distortion can be predicted
                  through a simple and practical method beforehand,
                  the predictions will be helpful for taking active as
                  well as appropriate measures to control the
                  dimension accuracy. Based on inherent strain theory
                  and interface element formulation, we developed a
                  practical prediction system to compute the
                  accumulated distortion during the welding assembly
                  process in the current study. Using the developed
                  prediction method, we calculated the welding
                  distortion in a thin plate structure with
                  considering both the shrinkage due to heat input and
                  the gap/misalignment generated during assembly
                  process. Meanwhile, we investigated the influences
                  of assembly sequence and gap correction on the final
                  distortion.},
  number =	 1,
  urldate =	 {2015-12-29},
  journal =	 {Computational Materials Science},
  author =	 {Murakawa, Hidekazu and Deng, Dean and Ma, Ninshu and
                  Wang, Jiangchao},
  year =	 2012,
  keywords =	 {Assembly sequence, Inherent strain, Interface
                  element, Numerical simulation, Welding distortion},
  pages =	 {43--52},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/HU6RM2XJ/Murakawa
                  et al. - 2012 - Applications of inherent strain and
                  interface elem.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/E4QPGRRA/S092702561100382X.html:text/html}
}

@ARTICLE{chiumenti_numerical_2018,
  author =	 {Neiva, E. and Chiumenti, M. and Salsi, E. and Cervera, M. and Badia, S. and Mart\'in, A. F. and Chen, Z. and Lee, C. and Davies, C.},
  title =	 {Numerical modelling of heat transfer and
                  experimental calibration in selective laser melting
                  with the virtual loose-powder model},
  journal =	 {To be submitted to Additive Manufacturing},
  year =	 2018
}


@inproceedings{neugebauer_multi_2014,
  title =	 {Multi scale {FEM} simulation for distortion
                  calculation in additive manufacturing of hardening
                  stainless steel},
  url =
                  {http://www.researchgate.net/profile/Frederik_Feuerhahn/publication/266652527_Multi_Scale_FEM_Simulation_for_Distortion_Calculation_in_Additive_Manufacturing_of_Hardening_Stainless_Steel/links/5436717f0cf2dc341db309f9.pdf},
  urldate =	 {2015-12-29},
  booktitle =	 {Proceedings of the International {Workshop} on
                  {Thermal} forming and welding distortion},
  author =	 {Neugebauer, F. and Keller, N. and Ploshikhin, V. and
                  Feuerhahn, F. and K\"{o}hler, H.},
  year =	 2014,
  file =
                  {5436717f0cf2dc341db309f9.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/U9NCQEK6/5436717f0cf2dc341db309f9.pdf:application/pdf}
}

@article{nicholas_p._lavery_review_2014,
  title =	 {A {Review} of {Computational} {Modelling} of
                  {Additive} {Layer}, {Manufacturing} -
                  {Multi}-{Scale} and {Multi}-{Physics}},
  doi =		 {10.13140/RG.2.1.3103.0884},
  author =	 {Nicholas P. Lavery, Stephen G. R. Brown},
  year =	 2014,
  file =	 {A Review of Computational Modelling of Additive
                  Layer, Manufacturing - Multi-Scale and Multi-Physics
                  (PDF Download
                  Available):/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/7B5VCF6Z/265412821_A_Review_of_Computational_Modelling_of_Additive_Layer_Manufacturing_-_Multi-Scale_and.html:text/html}
}

@techreport{nist,
  title =	 {Measurement Science Roadmap for Metal-Based Additive
                  Manufacturing},
  url =
                  {www.nist.gov/el/isd/upload/NISTAdd_Mfg_Report_FINAL-2.pdf},
  urldate =	 {2015-12-29},
  institution =	 {National Institute of Standards and Technology},
  year =	 2013
}

@article{nobile_anisotropic_2008,
  title =	 {An {Anisotropic} {Sparse} {Grid} {Stochastic}
                  {Collocation} {Method} for {Partial} {Differential}
                  {Equations} with {Random} {Input} {Data}},
  volume =	 46,
  issn =	 {0036-1429},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/070680540},
  doi =		 {10.1137/070680540},
  abstract =	 {This work proposes and analyzes an anisotropic
                  sparse grid stochastic collocation method for
                  solving partial differential equations with random
                  coefficients and forcing terms (input data of the
                  model). The method consists of a Galerkin
                  approximation in the space variables and a
                  collocation, in probability space, on sparse tensor
                  product grids utilizing either Clenshaw-Curtis or
                  Gaussian knots. Even in the presence of
                  nonlinearities, the collocation approach leads to
                  the solution of uncoupled deterministic problems,
                  just as in the Monte Carlo method. This work
                  includes a priori and a posteriori procedures to
                  adapt the anisotropy of the sparse grids to each
                  given problem. These procedures seem to be very
                  effective for the problems under study. The proposed
                  method combines the advantages of isotropic sparse
                  collocation with those of anisotropic full tensor
                  product collocation: the first approach is effective
                  for problems depending on random variables which
                  weigh approximately equally in the solution, while
                  the benefits of the latter approach become apparent
                  when solving highly anisotropic problems depending
                  on a relatively small number of random variables, as
                  in the case where input random variables are
                  Karhunen-Loève truncations of “smooth” random
                  fields. This work also provides a rigorous
                  convergence analysis of the fully discrete problem
                  and demonstrates (sub)exponential convergence in the
                  asymptotic regime and algebraic convergence in the
                  preasymptotic regime, with respect to the total
                  number of collocation points. It also shows that the
                  anisotropic approximation breaks the curse of
                  dimensionality for a wide set of problems. Numerical
                  examples illustrate the theoretical results and are
                  used to compare this approach with several others,
                  including the standard Monte Carlo. In particular,
                  for moderately large-dimensional problems, the
                  sparse grid approach with a properly chosen
                  anisotropy seems to be very efficient and superior
                  to all examined methods.},
  number =	 5,
  urldate =	 {2016-01-15},
  journal =	 {SIAM Journal on Numerical Analysis},
  author =	 {Nobile, F. and Tempone, R. and Webster, C.},
  year =	 2008,
  pages =	 {2411--2442},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/EGRDJNKJ/Nobile
                  et al. - 2008 - An Anisotropic Sparse Grid
                  Stochastic Collocation
                  .pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/W44XKKQZ/070680540.html:text/html}
}

@article{ohman_computational_2011,
  title =	 {Computational mesoscale modeling and homogenization
                  of liquid-phase sintering of particle agglomerates},
  volume =	 32,
  url =
                  {http://www.ovgu.de/ifme/zeitschrift_tm/2012_Heft2_5/33_Oehman.pdf},
  urldate =	 {2015-12-16},
  journal =	 {Technische Mechanik},
  author =	 {\"{O}hman, M. and Runesson, K. and Larsson, F.},
  year =	 2011,
  pages =	 {463--483},
  file =	 {[PDF] from
                  ovgu.de:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/5PRFRV2A/Ohman
                  et al. - 2011 - Computational mesoscale modeling and
                  homogenizatio.pdf:application/pdf}
}

@article{ohman_computational_2013,
  title =	 {Computational homogenization of liquid-phase
                  sintering with seamless transition from macroscopic
                  compressibility to incompressibility},
  volume =	 266,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S004578251300176X},
  doi =		 {10.1016/j.cma.2013.07.006},
  abstract =	 {Liquid phase sintering of particle agglomerates is
                  modeled on the mesoscale as the viscous deformation
                  of particle-particle contact, whereby the single
                  driving force is the surface tension on the
                  particle/pore interface. On the macroscale, a
                  quasistatic equilibrium problem allows for the
                  prediction of the shrinkage of the sintering
                  body. The present paper presents a novel FE2
                  formulation of the two-scale sintering problem
                  allowing for the transition to zero porosity,
                  implying macroscale incompressibility. The seamless
                  transition from compressibility to incompressibility
                  on the macroscale is accomplished by introducing a
                  mixed variational format. This has consequences also
                  for the formulation of the mesoscale problem, that
                  is complemented with an extra constraint equation
                  regarding the prolongation of the volumetric part of
                  the macroscopic rate-of-deformation. The numerical
                  examples shows the sintering of a single
                  representative volume element (RVE) which is sheared
                  beyond the point where the porosity vanishes while
                  subjected to zero macroscopic pressure.},
  urldate =	 {2015-12-15},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {\"{O}hman, Mikael and Larsson, Fredrik and Runesson,
                  Kenneth},
  year =	 2013,
  keywords =	 {FE2, Multiscale, Sintering, Stokes’ flow, Surface
                  tension},
  pages =	 {219--228},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/996KB8EN/Öhman
                  et al. - 2013 - Computational homogenization of
                  liquid-phase sinte.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/DDHT24QU/S004578251300176X.html:text/html}
}

@article{ohman_variationally_2015,
  title =	 {On the variationally consistent computational
                  homogenization of elasticity in the incompressible
                  limit},
  volume =	 2,
  issn =	 {2213-7467},
  url =
                  {http://link.springer.com/article/10.1186/s40323-014-0017-1},
  doi =		 {10.1186/s40323-014-0017-1},
  abstract =	 {Background Computational homogenization is a
                  well-established approach in material modeling with
                  the purpose to account for strong
                  micro-heterogeneity in an approximate fashion
                  without excessive computational cost. However, the
                  case of macroscopically incompressible response is
                  still unresolved. Methods The computational
                  framework for Variationally Consistent
                  Homogenization (VCH) of (near) incompressible solids
                  is discussed. A canonical formulation of the
                  subscale problem, pertinent to a Representative
                  Volume Element (RVE), is established, whereby
                  complete macroscale incompressibility is obtained as
                  the limit situation when all constituents are
                  incompressible. Results Numerical results for single
                  RVEs demonstrate the seamless character of the
                  computational algorithm at the fully incompressible
                  limit. Conclusions The suggested framework can
                  seamlessly handle the transition from
                  (macroscopically) compressible to incompressible
                  response. The framework allows for the classical
                  boundary conditions on the RVE as well as the
                  generalized situation of weakly periodic boundary
                  conditions.},
  language =	 {en},
  number =	 1,
  urldate =	 {2015-12-16},
  journal =	 {Advanced Modeling and Simulation in Engineering
                  Sciences},
  author =	 {\"{O}hman, Mikael and Runesson, Kenneth and Larsson,
                  Fredrik},
  year =	 2015,
  keywords =	 {Classical Continuum Physics, Computational
                  homogenization, Computational Science and
                  Engineering, Incompressibility, Mixed variational
                  formulations, Multiscale, Theoretical and Applied
                  Mechanics},
  pages =	 {1--29},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/VKG6H3AU/Öhman
                  et al. - 2015 - On the variationally consistent
                  computational
                  homo.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/A6C54A6R/10.html:text/html}
}

@article{olm-htsd,
	title = {Simulation of high temperature superconductors and experimental validation},
	url = {http://arxiv.org/abs/1707.09783},
	abstract = {In this work, we present a numerical framework to simulate high temperature superconductor devices. We select the so-called \$H\$-formulation, which uses the magnetic field as a state variable, in order to develop an augmented formulation that guarantees the divergence-free condition. However, nodal elements fail in the \$H-\$formulation modelling of electromagnetic fields along interfaces between regions with high contrast medium properties, thus N{\textbackslash}'ed{\textbackslash}'elec elements (of arbitrary order) are preferred. The composition of a customized, robust, nonlinear solver completes the exposition of the developed tools to tackle the problem. A set of detailed numerical experiments in 2D/3D shows the availability of the finite element approximation to model the phenomena, including a validation against experimental data.},
	urldate = {2018-02-05},
	journal = {arXiv:1707.09783 [physics]},
	author = {Olm, M. and Badia, S. and Martín, A. F.},
	year = {2017},
	note = {arXiv: 1707.09783},
	keywords = {Physics - Computational Physics, Computer Science - Computational Engineering, Finance, and Science},
	annote = {Comment: 26 pages},
	file = {arXiv\:1707.09783 PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/EFN2JBB6/Olm et al. - 2017 - Simulation of high temperature superconductors and.pdf:application/pdf;arXiv.org Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/3PK5FZJB/1707.html:text/html}
}

@article{olson_materials_2014,
  title =	 {Materials genomics: {From} {CALPHAD} to flight},
  volume =	 70,
  issn =	 {1359-6462},
  shorttitle =	 {Materials genomics},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S1359646213004375},
  doi =		 {10.1016/j.scriptamat.2013.08.032},
  abstract =	 {Unique depth of scientific knowledge has allowed
                  physical metallurgy to lead the development of a
                  successful computational materials design
                  methodology grounded in a system of fundamental
                  databases. Expansion of the approach to embrace
                  acceleration of the full development and
                  qualification cycle has resulted in the first fully
                  computationally designed and qualified material
                  reaching the flight stage. The example of
                  high-performance alloys provides the template for a
                  general methodology of computational design for all
                  materials.},
  urldate =	 {2016-01-28},
  journal =	 {Scripta Materialia},
  author =	 {Olson, G. B. and Kuehmann, C. J.},
  year =	 2014,
  keywords =	 {Accelerated insertion of materials, Integrated
                  computational materials engineering, Materials
                  design, Materials genome},
  pages =	 {25--30},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/94FPNUH5/Olson
                  and Kuehmann - 2014 - Materials genomics From
                  CALPHAD to flight.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/HSTTG8V9/S1359646213004375.html:text/html}
}

@article{ompss,
  title =	 {Extending {OpenMP} to survive the Heterogeneous
                  Multi-Core Era},
  volume =	 38,
  issn =	 {0885-7458, 1573-7640},
  url =
                  {http://link.springer.com/article/10.1007/s10766-010-0135-4},
  doi =		 {10.1007/s10766-010-0135-4},
  abstract =	 {This paper advances the state-of-the-art in
                  programming models for exploiting task-level
                  parallelism on heterogeneous many-core systems,
                  presenting a number of extensions to the OpenMP
                  language inspired in the StarSs programming
                  model. The proposed extensions allow the programmer
                  to write portable code easily for a number of
                  different platforms, relieving him/her from
                  developing the specific code to off-load tasks to
                  the accelerators and the synchronization of
                  tasks. Our results obtained from the StarSs
                  instantiations for SMPs, the Cell, and GPUs report
                  reasonable parallel performance. However, the real
                  impact of our approach in is the productivity gains
                  it yields for the programmer.},
  language =	 {en},
  number =	 {5-6},
  urldate =	 {2016-01-20},
  journal =	 {International Journal of Parallel Programming},
  author =	 {Ayguad\'e, Eduard and Badia, Rosa M. and Bellens,
                  Pieter and Cabrera, Daniel and Duran, Alejandro and
                  Ferrer, Roger and Gonz\`alez, Marc and Igual,
                  Francisco and Jim\'enez-Gonz\'alez, Daniel and
                  Labarta, Jes\'us and Martinell, Luis and Martorell,
                  Xavier and Mayo, Rafael and P\'erez, Josep M. and
                  Planas, Judit and Quintana-Ort\'i, Enrique S.},
  year =	 2010,
  keywords =	 {Hardware accelerators, Heterogeneous computing,
                  Multi-core processors, parallel computing, Processor
                  Architectures, Programming models, Runtime systems,
                  Software Engineering/Programming and Operating
                  Systems, Task-level parallelism, Theory of
                  Computation},
  pages =	 {440--459},
  file =	 {Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/8FUAZTMW/Ayguadé
                  et al. - 2010 - Extending OpenMP to Survive the
                  Heterogeneous
                  Mult.pdf:application/pdf;Snapshot:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/CDJBK8AD/10.html:text/html}
}

@article{opstal_goal-oriented_2015,
  title =	 {Goal-oriented model adaptivity for viscous
                  incompressible flows},
  volume =	 55,
  issn =	 {0178-7675, 1432-0924},
  url =
                  {http://link.springer.com/article/10.1007/s00466-015-1146-1},
  doi =		 {10.1007/s00466-015-1146-1},
  abstract =	 {In van Opstal et al. (Comput Mech 50:779-788, 2012)
                  airbag inflation simulations were performed where
                  the flow was approximated by Stokes flow. Inside the
                  intricately folded initial geometry the Stokes
                  assumption is argued to hold. This linearity
                  assumption leads to a boundary-integral
                  representation, the key to bypassing mesh generation
                  and remeshing. It therefore enables very large
                  displacements with near-contact. However, such a
                  coarse assumption cannot hold throughout the domain,
                  where it breaks down one needs to revert to the
                  original model. The present work formalizes this
                  idea. A model adaptive approach is proposed, in
                  which the coarse model (a Stokes boundary-integral
                  equation) is locally replaced by the original
                  high-fidelity model (Navier-Stokes) based on
                  a-posteriori estimates of the error in a quantity of
                  interest. This adaptive modeling framework aims at
                  taking away the burden and heuristics of manually
                  partitioning the domain while providing new insight
                  into the physics. We elucidate how challenges
                  pertaining to model disparity can be
                  addressed. Essentially, the solution in the interior
                  of the coarse model domain is reconstructed as a
                  post-processing step. We furthermore present a
                  two-dimensional numerical experiments to show that
                  the error estimator is reliable.},
  language =	 {en},
  number =	 6,
  urldate =	 {2016-01-15},
  journal =	 {Computational Mechanics},
  author =	 {van Opstal, T. M.  and Bauman, P. T. and Prudhomme,
                  S. and van Brummelen, E. H.},
  year =	 2015,
  keywords =	 {Adaptivity, Boundary element, Classical Continuum
                  Physics, Computational Science and Engineering,
                  Finite element, Theoretical and Applied Mechanics},
  pages =	 {1181--1190},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/WGCCKSUX/Opstal
                  et al. - 2015 - Goal-oriented model adaptivity for
                  viscous
                  incompr.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/UPKSJQS9/10.html:text/html}
}

@article{otomo_direct_2014,
	title = {Direct construction of a four-dimensional mesh model from a three-dimensional object with continuous rigid body movement},
	volume = {1},
	issn = {2288-4300},
	url = {http://www.sciencedirect.com/science/article/pii/S2288430014500152},
	doi = {10.7315/JCDE.2014.010},
	abstract = {In the field of design and manufacturing, there are many problems with managing dynamic states of three-dimensional (3D) objects. In order to solve these problems, the four-dimensional (4D) mesh model and its modeling system have been proposed. The 4D mesh model is defined as a 4D object model that is bounded by tetrahedral cells, and can represent spatio-temporal changes of a 3D object continuously. The 4D mesh model helps to solve dynamic problems of 3D models as geometric problems. However, the construction of the 4D mesh model is limited on the time-series 3D voxel data based method. This method is memory-hogging and requires much computing time. In this research, we propose a new method of constructing the 4D mesh model that derives from the 3D mesh model with continuous rigid body movement. This method is realized by making a swept shape of a 3D mesh model in the fourth dimension and its tetrahe-dralization. Here, the rigid body movement is a screwed movement, which is a combination of translational and rotational movement.},
	pages = {96--102},
	number = {2},
        journal      = {Journal of Computational Design and Engineering},
	journaltitle = {Journal of Computational Design and Engineering},
	shortjournal = {Journal of Computational Design and Engineering},
	author = {Otomo, Ikuru and Onosato, Masahiko and Tanaka, Fumiki},
        year = {2014},
	urldate = {2016-12-30},
	date = {2014-04},
	keywords = {Four-dimensional mesh model, Fourth dimension, Rigid body movement, Three-dimensional mesh model},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/8GEX3AVS/Otomo et al. - 2014 - Direct construction of a four-dimensional mesh mod.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/UBU29N7U/S2288430014500152.html:text/html}
}

@article{ozdemir_computational_2008,
  title =	 {Computational homogenization for heat conduction in
                  heterogeneous solids},
  volume =	 73,
  issn =	 {1097-0207},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nme.2068/abstract},
  doi =		 {10.1002/nme.2068},
  abstract =	 {In this paper, a multi-scale analysis method for
                  heat transfer in heterogeneous solids is
                  presented. The principles of the method rely on a
                  two-scale computational homogenization approach
                  which is applied successfully for the stress
                  analysis of multi-phase solids under purely
                  mechanical loading. The present paper extends this
                  methodology to heat conduction problems. The
                  flexibility of the method permits one to take into
                  account local microstructural heterogeneities and
                  thermal anisotropy, including non-linearities which
                  might arise at some stage of the thermal loading
                  history. The resulting complex microstructural
                  response is transferred back to the macro level in a
                  consistent manner. A proper macro to micro
                  transition is established in terms of the applied
                  boundary conditions and likewise a micro to macro
                  transition is formulated in the form of consistent
                  averaging relations. Imposition of boundary
                  conditions and extraction of macroscopic quantities
                  are elaborated in detail. A nested finite element
                  solution procedure is outlined, and the
                  effectiveness of the approach is demonstrated by
                  some illustrative example problems. Copyright © 2007
                  John Wiley \& Sons, Ltd.},
  language =	 {en},
  number =	 2,
  urldate =	 {2015-12-29},
  journal =	 {International Journal for Numerical Methods in
                  Engineering},
  author =	 {\"{O}zdemir, I. and Brekelmans, W. A. M. and Geers,
                  M. G. D.},
  year =	 2008,
  keywords =	 {coarse graining, Computational homogenization, FE2,
                  heat conduction, multi-scale, thermomechanics},
  pages =	 {185--204},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/TC5NGX9R/Özdemir
                  et al. - 2008 - Computational homogenization for
                  heat conduction
                  i.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/ZB4CND3V/abstract.html:text/html}
}

@article{ozdemir_thermo_2008,
	title = {Computational homogenization for the thermo-mechanical analysis of heterogeneous solids},
	volume = {198},
	issn = {0045-7825},
	url = {http://www.sciencedirect.com/science/article/pii/S0045782508003277},
	doi = {10.1016/j.cma.2008.09.008},
	abstract = {This paper presents a two-scale thermo-mechanical analysis framework for heterogeneous solids based on a computational homogenization technique. The evolution of the mechanical and thermal fields at the macroscopic level is resolved through the incorporation of the microstructural response. Within the proposed multi-scale approach, the temperature dependent non-linear thermo-mechanical response is accounted by solving a boundary value problem at the micro-scale, the results of which are properly averaged and transferred to the macro level in a consistent way. The framework does not require explicitly determined homogenized material properties (e.g. macroscopic thermal expansion coefficients) since no constitutive equations are required for the macroscopic stresses and heat fluxes at the macro level. A nested finite element solution procedure with an operator-split implementation is outlined and the effectiveness of the approach is demonstrated by illustrative key examples.},
	pages = {602--613},
	number = {3},
        journal = {Computer Methods in Applied Mechanics and Engineering},
	journaltitle = {Computer Methods in Applied Mechanics and Engineering},
	shortjournal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Özdemir, I. and Brekelmans, W. A. M. and Geers, M. G. D.},
        year = {2008},
	urldate = {2017-01-13},
	date = {2008-12-15},
	keywords = {coarse graining, Computational homogenization, {FE}    2, Heterogeneous solids, Multi-scale analysis, Thermo-mechanics},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/MR3HMDM5/Özdemir et al. - 2008 - computational homogenization for the thermo-mechan.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/BT2NUDQ9/S0045782508003277.html:text/html}
}

@inproceedings{pal_integrated_2013,
  title =	 {An integrated approach to cyber-enabled additive
                  manufacturing using physics based, coupled
                  multi-scale process modeling},
  url =
                  {http://sffsymposium.engr.utexas.edu/Manuscripts/2013/2013-01-Pal.pdf},
  urldate =	 {2015-12-29},
  booktitle =	 {Proceedings of the 25th Annual International Solid
                  FreeForm Fabrication Symposium - An Additive
                  Manufacturing Conference},
  author =	 {Pal, Deepankar and Patil, Nachiket and Nikoukar,
                  Mohammad and Zeng, Kai and Kutty, Khalid Haludeen
                  and Stucker, Brent E.},
  year =	 2013,
  pages =	 {1--18},
  file =
                  {2013-01-Pal.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/P97TSJE9/2013-01-Pal.pdf:application/pdf}
}

@article{parareal,
  title =	 {A ``parareal'' in time discretization of {PDE}'s},
  volume =	 332,
  number =	 7,
  journal =	 {Acad. Sci. Paris},
  author =	 {Lions, J.L. and Maday, Y. and Turinici, A. },
  year =	 2001,
  pages =	 {661-668}
}


@article{pavarino_bddc_2007,
  series =	 {Domain Decomposition Methods: recent advances and
                  new challenges in engineering},
  title =	 {{BDDC} and {FETI-DP} preconditioners for spectral
                  element discretizations},
  volume =	 196,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782506002581},
  doi =		 {10.1016/j.cma.2006.03.009},
  abstract =	 {Two of the most recent and important nonoverlapping
                  domain decomposition methods, the {BDDC} method
                  (Balancing Domain Decomposition by Constraints) and
                  the {FETI-DP} method ({Dual--Primal} Finite Element
                  Tearing and Interconnecting) are here extended to
                  spectral element discretizations of second-order
                  elliptic problems. In spite of the more severe
                  ill-conditioning of the spectral element discrete
                  systems, compared with low-order finite elements and
                  finite differences, these methods retain their good
                  properties of scalability, quasi-optimality and
                  independence on the discontinuities of the elliptic
                  operator coefficients across subdomain interfaces.},
  number =	 8,
  urldate =	 {2014-05-15},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Pavarino, Luca F.},
  year =	 2007,
  keywords =	 {Balancing {Neumann-Neumann}, {BDDC}, Domain
                  decomposition methods, {FETI-DP}, Spectral elements},
  pages =	 {1380--1388},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/36Q67MDT/Pavarino
                  - 2007 - BDDC and FETI-DP preconditioners for
                  spectral elem.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/JGWU6MEE/S0045782506002581.html:text/html}
}

@article{pechstein_weighted_2013,
  title =	 {Weighted {Poincar\'e} inequalities},
  volume =	 33,
  issn =	 {0272-4979, 1464-3642},
  url =
                  {http://imanum.oxfordjournals.org/cgi/doi/10.1093/imanum/drs017},
  doi =		 {10.1093/imanum/drs017},
  language =	 {en},
  number =	 2,
  urldate =	 {2015-12-18},
  journal =	 {IMA Journal of Numerical Analysis},
  author =	 {Pechstein, C. and Scheichl, R.},
  year =	 2013,
  pages =	 {652--686},
  file =	 {Weighted Poincaré
                  inequalities_Scheichl-Pechstein.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/M2P2H55M/Weighted
                  Poincaré
                  inequalities_Scheichl-Pechstein.pdf:application/pdf}
}

@article{perturbed_bddc,
	title = {Balancing Domain Decomposition by Constraints and Perturbation},
	volume = {54},
	issn = {0036-1429},
	url = {http://epubs.siam.org.recursos.biblioteca.upc.edu/doi/abs/10.1137/15M1045648},
	doi = {10.1137/15M1045648},
	abstract = {In this paper, we formulate and analyze a perturbed formulation of the balancing domain decomposition by constraints ({BDDC}) method. We prove that the perturbed {BDDC} has the same polylogarithmic bound for the condition number as the standard formulation. Two types of properly scaled zero-order perturbations are considered: one uses a mass matrix, and the other uses a Robin-type boundary condition, i.e, a mass matrix on the interface. With perturbation, the well-posedness of the local Neumann problems and the global coarse problem is automatically guaranteed, and coarse degrees of freedom can be defined only for convergence purposes but not well-posedness. This allows a much simpler implementation as no complicated corner selection algorithm is needed. Minimal coarse spaces using only face or edge constraints can also be considered. They are very useful in extreme scale calculations where the coarse problem is usually the bottleneck that can jeopardize scalability. The perturbation also adds extra robustness as the perturbed formulation works even when the constraints fail to eliminate a small number of subdomain rigid body modes from the standard {BDDC} space. This is extremely important when solving problems on unstructured meshes partitioned by automatic graph partitioners since arbitrary disconnected subdomains are possible. Numerical results are provided to support the theoretical findings.},
	pages = {3436--3464},
	number = {6},
        journal = {{SIAM} Journal on Numerical Analysis},
	journaltitle = {{SIAM} Journal on Numerical Analysis},
	shortjournal = {{SIAM} J. Numer. Anal.},
	author = {Badia, S. and Nguyen, H.},
	urldate = {2017-01-10},
	date = {2016-01-01},
        year = {2016},
	file = {Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/I2BZK6GD/Badia and Nguyen - 2016 - Balancing Domain Decomposition by Constraints and .pdf:application/pdf;Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/BPDX629H/15M1045648.html:text/html}
}

@Misc{petsc-web-page,
  author =	 {Satish Balay and Mark~F. Adams and Jed Brown and
                  Peter Brune and Kris Buschelman and Victor Eijkhout
                  and William~D. Gropp and Dinesh Kaushik and
                  Matthew~G. Knepley and Lois Curfman McInnes and Karl
                  Rupp and Barry~F. Smith and Hong Zhang},
  title =	 {{PETS}c {W}eb page},
  url =		 {http://www.mcs.anl.gov/petsc},
  howpublished = {\url{http://www.mcs.anl.gov/petsc}},
  year =	 2014
}

@article{pfasst,
  title =	 {Toward an efficient parallel in time method for
                  partial differential equations},
  volume =	 7,
  number =	 1,
  journal =	 {Comm. App. Math. and Comp. Sci.},
  author =	 {Emmett, Matthew and Minion, Michael J.},
  year =	 2012,
  keywords =	 {parallel computing, time parallel, ordinary
                  differential equations, partial differential
                  equations, deferred corrections, parareal},
  pages =	 {105-132}
}

@article{phuong_huynh_static_2013,
  title =	 {A {Static} condensation {Reduced} {Basis} {Element}
                  method : approximation and a posteriori error
                  estimation},
  volume =	 47,
  shorttitle =	 {A {Static} condensation {Reduced} {Basis} {Element}
                  method},
  doi =		 {10.1051/m2an/2012022},
  number =	 1,
  journal =	 {ESAIM: Mathematical Modelling and Numerical
                  Analysis},
  author =	 {Phuong Huynh and Huynh, Dinh Bao Phuong and
                  Knezevic, David J. and Patera, Anthony T.},
  year =	 2013,
  pages =	 39,
  file =	 {ESAIM\: Mathematical Modelling and Numerical
                  Analysis (ESAIM\:
                  M2AN):/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/V9DSM7SD/m2an120022.html:text/html}
}

@techreport{pi_asc_2015,
	title = {{ASC} {ATDM} {Level} 2 {Milestone}\# 5325: {Asynchronous} {Many}-{Task} {Runtime} {System} {Analysis} and {Assessment} for {Next} {Generation} {Platforms}},
	shorttitle = {{ASC} {ATDM} {Level} 2 {Milestone}\# 5325},
	url = {http://www.sci.utah.edu/publications/Ben2015c/ATDM-AMT-L2-Final-SAND2015-8312.pdf},
	number = {SAND2015-8312},
	urldate = {2017-01-13},
	institution = {Sandia National Laboratories},
	author = {PI, Janine Bennett and PM, Robert Clay and Baker, Gavin and Gamell, Marc and Hollman, David and Knight, Samuel and Kolla, Hemanth and Sjaardema, Gregory and Slattengren, Nicole and Teranishi, Keita and {others}},
	year = {2015},
	pages = {1--126},
	file = {ATDM-AMT-L2-Final-SAND2015-8312.pdf:/home/amartin/.zotero/zotero/nq522fur.default/zotero/storage/U458M4B9/ATDM-AMT-L2-Final-SAND2015-8312.pdf:application/pdf}
}

@article{planas_approximation_2011,
  title =	 {Approximation of the inductionless {MHD} problem
                  using a stabilized finite element method},
  volume =	 230,
  issn =	 {0021-9991},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0021999111000088},
  doi =		 {10.1016/j.jcp.2010.12.046},
  abstract =	 {In this work, we present a stabilized formulation to
                  solve the inductionless magnetohydrodynamic (MHD)
                  problem using the finite element (FE) method. The
                  MHD problem couples the Navier–Stokes equations and
                  a Darcy-type system for the electric potential via
                  Lorentz’s force in the momentum equation of the
                  Navier–Stokes equations and the currents generated
                  by the moving fluid in Ohm’s law. The key feature of
                  the FE formulation resides in the design of the
                  stabilization terms, which serve several
                  purposes. First, the formulation is suitable for
                  convection dominated flows. Second, there is no need
                  to use interpolation spaces constrained to a
                  compatibility condition in both sub-problems and
                  therefore, equal-order interpolation spaces can be
                  used for all the unknowns. Finally, this formulation
                  leads to a coupled linear system; this monolithic
                  approach is effective, since the coupling can be
                  dealt by effective preconditioning and iterative
                  solvers that allows to deal with high Hartmann
                  numbers.},
  number =	 8,
  urldate =	 {2012-02-10},
  journal =	 {Journal of Computational Physics},
  author =	 {Planas, Ramon and Badia, Santiago and Codina, Ramon},
  year =	 2011,
  keywords =	 {HCLL test blanket module, Inductionless MHD,
                  Monolithic scheme, Primal–dual formulation,
                  Stabilized finite element formulation, Variational
                  multiscale method},
  pages =	 {2977--2996}
}

@article{podshivalov_design_2013,
  series =	 {First {CIRP} {Conference} on {BioManufacturing}},
  title =	 {Design, {Analysis} and {Additive} {Manufacturing} of
                  {Porous} {Structures} for {Biocompatible}
                  {Micro}-{Scale} {Scaffolds}},
  volume =	 5,
  issn =	 {2212-8271},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S2212827113000504},
  doi =		 {10.1016/j.procir.2013.01.049},
  abstract =	 {Advancements in the fields of biocompatible
                  materials, manufacturing processes, computational
                  methods and medicine have led to the emergence of a
                  new field: micro-scale scaffolds for bone
                  replacement and regeneration. Yet most such
                  scaffolds produced today are characterized by very
                  basic geometry, and their microstructure differs
                  greatly from that of the actual tissue they are
                  intended to replace. In this paper, we propose a
                  novel approach for generating micro-scale scaffolds
                  based on processing actual micro-CT images and then
                  reconstructing a highly accurate geometrical
                  model. This model is manufactured by means of a
                  state-of-the-art 3D additive manufacturing process
                  from biocompatible materials. At the micro-scale
                  level, these scaffolds are very similar to the
                  original tissue, thus interfacing better with the
                  surrounding tissue and facilitating more efficient
                  rehabilitation for the patient. Moreover, the
                  approach facilitates the design and manufacture of
                  patient-specific scaffolds which can copy patients’
                  exact structural and mechanical characteristics,
                  taking into account their physical condition and
                  medical history. By means of multi-resolution
                  volumetric modeling methods, scaffold porosity can
                  also be adapted according to specific mechanical
                  requirements. The process of designing and
                  manufacturing micro-scale scaffolds involves five
                  major stages: (a) building a volumetric
                  multi-resolution model from micro-CT images; (b)
                  generation of surface geometric model in STL format;
                  (c) additive manufacturing of the scaffold; (d)
                  scaffold shape verification relative to the
                  geometric design; and (e) verification of mechanical
                  properties through finite element analysis. In this
                  research, all the proposed stages of the approach
                  were tested. The input included micro-CT scans of
                  porous ceramic structure, which is quite similar to
                  commercial porous scaffolds. The results show that
                  the proposed method is feasible for design and
                  manufacture of micro-scale scaffolds.},
  urldate =	 {2015-12-29},
  journal =	 {Procedia CIRP},
  author =	 {Podshivalov, Lev and Gomes, Cynthia M. and Zocca,
                  Andrea and Guenster, Jens and Bar-Yoseph, Pinhas and
                  Fischer, Anath},
  year =	 2013,
  keywords =	 {additive manufacturing, Ceramics, Micro-scale bone
                  scaffolds, Multiresolution modeling, Multiscale FEA},
  pages =	 {247--252},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/KWI5ZR2B/Podshivalov
                  et al. - 2013 - Design, Analysis and Additive
                  Manufacturing of
                  Por.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/AUJZI5DE/S2212827113000504.html:text/html}
}

@ARTICLE{post_report_2004,
  author =	 {Post, Douglass E. and Batchelor, Donald B. and
                  Bramley, Randall B.  and Cary, John R. and Cohen,
                  Ronald H. and Colella, Phillip and Jardin, Steven
                  C.},
  title =	 {Report of the Fusion Simulation Project Steering
                  Committee},
  journal =	 {Journal of Fusion Energy},
  year =	 2004,
  volume =	 23,
  pages =	 {1--26},
  number =	 1,
  abstract =	 {The Fusion Simulation Project ({FSP)} is envisioned
                  as a 15 year, {\$20M/year} multi-institutional
                  project to develop a comprehensive simulation
                  capability for magnetic fusion experiments with a
                  focus on the International Thermonuclear
                  Experimental Reactor ({ITER).}  The {FSP} would be
                  able to contribute to design decisions, experimental
                  planning and performance optimization for {ITER},
                  substantially increasing {ITERâs} likelihood
                  of success and its value to the {US} Fusion
                  Program. The {FSP} would be jointly supported by the
                  {DOE} Office of Fusion Energy Sciences and the {DOE}
                  Office of Advanced Scientific Computing
                  Research. The potential for developing this
                  simulation capability rests on the exponential
                  growth of computer power over the last 50 years, the
                  progress in physics understanding developed by the
                  international fusion program and the continued
                  progress in computational mathematics that enables
                  the use of the new âultra-scaleâ
                  computers to solve difficult mathematical
                  problems. The initial concept for the {FSP} was
                  developed by the Fusion Energy Sciences Advisory
                  Committee Integrated Simulation and Optimization of
                  Fusion Systems Subcommittee (J. Dahlburg and
                  J. Corones, et al., J. Fusion Energy, 20(4),
                  135â196.). The {DOE} asked the {FSP}
                  Steering Committee to develop a project vision, a
                  governance concept and a roadmap for the {FSP.} The
                  Committee recommends that the {FSP} consist of three
                  elements: a production component, a research and
                  integration component, and a software infrastructure
                  component. The key challenge is developing
                  components that bridge the enormous distance and
                  time scales involved with the disparate physics
                  elements of tokamak performance. The committee
                  recommended that this challenge be met through
                  {âFocused} Integration Initiativesâ
                  that would first seek to integrate different physics
                  packages with disparate distance and time scales. An
                  example is the integration of Radio Frequency ({RF)}
                  Current Drive and Magnetohydrodynamics ({MHD)}
                  components to produce an integrated capability to
                  simulate the use of {RF} current drive to suppress
                  {MHD} instabilities. This report also defines the
                  requirements for a governance structure.  The {FSP}
                  Steering Committee judged that the project begin
                  with a conceptual design phase lasting one or two
                  years and be followed by a staged ramp-up over a few
                  years to the full funding level.},
  doi =		 {10.1007/s10894-004-1868-0},
  file =	 {Full Text
                  PDF:/home/principe/Javier/System/Config/firefox/zotero/storage/MQ2SQQUI/Post
                  et al. - 2004 - Report of the Fusion Simulation
                  Project Steering
                  C.pdf:application/pdf;Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/C7PZT622/10.html:text/html},
  issn =	 {0164-0313, 1572-9591},
  keywords =	 {computer simulation, Fusion, international
                  thermonuclear experimental reactor ({ITER)}, Nuclear
                  Engineering, Tokamak},
  language =	 {en},
  owner =	 {principe},
  timestamp =	 {2014.03.21},
  url =
                  {http://link.springer.com/article/10.1007/s10894-004-1868-0},
  urldate =	 {2014-03-20}
}

@ARTICLE{post_report_2004,
  author =	 {Post, Douglass E. and Batchelor, Donald B. and
                  Bramley, Randall B.  and Cary, John R. and Cohen,
                  Ronald H. and Colella, Phillip and Jardin, Steven
                  C.},
  title =	 {Report of the Fusion Simulation Project Steering
                  Committee},
  journal =	 {Journal of Fusion Energy},
  year =	 2004,
  volume =	 23,
  pages =	 {1--26},
  number =	 1,
  abstract =	 {The Fusion Simulation Project ({FSP)} is envisioned
                  as a 15 year, {\$20M/year} multi-institutional
                  project to develop a comprehensive simulation
                  capability for magnetic fusion experiments with a
                  focus on the International Thermonuclear
                  Experimental Reactor ({ITER).}  The {FSP} would be
                  able to contribute to design decisions, experimental
                  planning and performance optimization for {ITER},
                  substantially increasing {ITERâs} likelihood
                  of success and its value to the {US} Fusion
                  Program. The {FSP} would be jointly supported by the
                  {DOE} Office of Fusion Energy Sciences and the {DOE}
                  Office of Advanced Scientific Computing
                  Research. The potential for developing this
                  simulation capability rests on the exponential
                  growth of computer power over the last 50 years, the
                  progress in physics understanding developed by the
                  international fusion program and the continued
                  progress in computational mathematics that enables
                  the use of the new âultra-scaleâ
                  computers to solve difficult mathematical
                  problems. The initial concept for the {FSP} was
                  developed by the Fusion Energy Sciences Advisory
                  Committee Integrated Simulation and Optimization of
                  Fusion Systems Subcommittee (J. Dahlburg and
                  J. Corones, et al., J. Fusion Energy, 20(4),
                  135â196.). The {DOE} asked the {FSP}
                  Steering Committee to develop a project vision, a
                  governance concept and a roadmap for the {FSP.} The
                  Committee recommends that the {FSP} consist of three
                  elements: a production component, a research and
                  integration component, and a software infrastructure
                  component. The key challenge is developing
                  components that bridge the enormous distance and
                  time scales involved with the disparate physics
                  elements of tokamak performance. The committee
                  recommended that this challenge be met through
                  {âFocused} Integration Initiativesâ
                  that would first seek to integrate different physics
                  packages with disparate distance and time scales. An
                  example is the integration of Radio Frequency ({RF)}
                  Current Drive and Magnetohydrodynamics ({MHD)}
                  components to produce an integrated capability to
                  simulate the use of {RF} current drive to suppress
                  {MHD} instabilities. This report also defines the
                  requirements for a governance structure.  The {FSP}
                  Steering Committee judged that the project begin
                  with a conceptual design phase lasting one or two
                  years and be followed by a staged ramp-up over a few
                  years to the full funding level.},
  doi =		 {10.1007/s10894-004-1868-0},
  file =	 {Full Text
                  PDF:/home/principe/Javier/System/Config/firefox/zotero/storage/MQ2SQQUI/Post
                  et al. - 2004 - Report of the Fusion Simulation
                  Project Steering
                  C.pdf:application/pdf;Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/C7PZT622/10.html:text/html},
  issn =	 {0164-0313, 1572-9591},
  keywords =	 {computer simulation, Fusion, international
                  thermonuclear experimental reactor ({ITER)}, Nuclear
                  Engineering, Tokamak},
  language =	 {en},
  owner =	 {principe},
  timestamp =	 {2014.03.21},
  url =
                  {http://link.springer.com/article/10.1007/s10894-004-1868-0},
  urldate =	 {2014-03-20}
}

@article{prabhakar_computational_2015,
  title =	 {Computational modeling of residual stress formation
                  during the electron beam melting process for
                  {Inconel} 718},
  volume =	 7,
  issn =	 22148604,
  url =
                  {http://linkinghub.elsevier.com/retrieve/pii/S2214860415000160},
  doi =		 {10.1016/j.addma.2015.03.003},
  language =	 {en},
  urldate =	 {2015-12-31},
  journal =	 {Additive Manufacturing},
  author =	 {Prabhakar, P. and Sames, W.J. and Dehoff, R. and
                  Babu, S.S.},
  year =	 2015,
  pages =	 {83--91},
  file =
                  {1-s2.0-S2214860415000160-main.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/ZQRF6NDX/1-s2.0-S2214860415000160-main.pdf:application/pdf}
}

@ARTICLE{previti_solving_2011,
  author =	 {Previti, Alberto and Furfaro, Roberto and Picca,
                  Paolo and Ganapol, Barry D. and Mostacci, Domiziano},
  title =	 {Solving radiative transfer problems in highly
                  heterogeneous media via domain decomposition and
                  convergence acceleration techniques},
  journal =	 {Applied Radiation and Isotopes},
  year =	 2011,
  volume =	 69,
  pages =	 {1146--1150},
  number =	 8,
  abstract =	 {This paper deals with finding accurate solutions for
                  photon transport problems in highly heterogeneous
                  media fastly, efficiently and with modest memory
                  resources. We propose an extended version of the
                  analytical discrete ordinates method, coupled with
                  domain decomposition-derived algorithms and
                  non-linear convergence acceleration
                  techniques. Numerical performances are evaluated
                  using a challenging case study available in the
                  literature. A study of accuracy versus computational
                  time and memory requirements is reported for
                  transport calculations that are relevant for remote
                  sensing applications.},
  doi =		 {10.1016/j.apradiso.2010.11.016},
  file =	 {ScienceDirect Full Text
                  PDF:/home/principe/Javier/System/Config/firefox/zotero/storage/SH7EJ8FN/Previti
                  et al. - 2011 - Solving radiative transfer problems
                  in highly hete.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/EKJAWVMT/S0969804310004458.html:text/html},
  issn =	 {0969-8043},
  keywords =	 {Convergence acceleration, domain decomposition,
                  radiative transfer, Remote sensing, Semi-analytical},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0969804310004458},
  urldate =	 {2014-03-19}
}

@ARTICLE{previti_solving_2011,
  author =	 {Previti, Alberto and Furfaro, Roberto and Picca,
                  Paolo and Ganapol, Barry D. and Mostacci, Domiziano},
  title =	 {Solving radiative transfer problems in highly
                  heterogeneous media via domain decomposition and
                  convergence acceleration techniques},
  journal =	 {Applied Radiation and Isotopes},
  year =	 2011,
  volume =	 69,
  pages =	 {1146--1150},
  number =	 8,
  abstract =	 {This paper deals with finding accurate solutions for
                  photon transport problems in highly heterogeneous
                  media fastly, efficiently and with modest memory
                  resources. We propose an extended version of the
                  analytical discrete ordinates method, coupled with
                  domain decomposition-derived algorithms and
                  non-linear convergence acceleration
                  techniques. Numerical performances are evaluated
                  using a challenging case study available in the
                  literature. A study of accuracy versus computational
                  time and memory requirements is reported for
                  transport calculations that are relevant for remote
                  sensing applications.},
  doi =		 {10.1016/j.apradiso.2010.11.016},
  file =	 {ScienceDirect Full Text
                  PDF:/home/principe/Javier/System/Config/firefox/zotero/storage/SH7EJ8FN/Previti
                  et al. - 2011 - Solving radiative transfer problems
                  in highly hete.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/principe/Javier/System/Config/firefox/zotero/storage/EKJAWVMT/S0969804310004458.html:text/html},
  issn =	 {0969-8043},
  keywords =	 {Convergence acceleration, domain decomposition,
                  radiative transfer, Remote sensing, Semi-analytical},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0969804310004458},
  urldate =	 {2014-03-19}
}

@book{quarteroni_reduced_2014,
  title =	 {Reduced {Order} {Methods} for {Modeling} and
                  {Computational} {Reduction}},
  isbn =	 {978-3-319-02090-7},
  abstract =	 {This monograph addresses the state of the art of
                  reduced order methods for modeling and computational
                  reduction of complex parametrized systems, governed
                  by ordinary and/or partial differential equations,
                  with a special emphasis on real time computing
                  techniques and applications in computational
                  mechanics, bioengineering and computer
                  graphics.Several topics are covered, including:
                  design, optimization, and control theory in
                  real-time with applications in engineering; data
                  assimilation, geometry registration, and parameter
                  estimation with special attention to real-time
                  computing in biomedical engineering and
                  computational physics; real-time visualization of
                  physics-based simulations in computer science; the
                  treatment of high-dimensional problems in state
                  space, physical space, or parameter space; the
                  interactions between different model reduction and
                  dimensionality reduction approaches; the development
                  of general error estimation frameworks which take
                  into account both model and discretization
                  effects.This book is primarily addressed to
                  computational scientists interested in computational
                  reduction techniques for large scale differential
                  problems.},
  language =	 {en},
  publisher =	 {Springer},
  author =	 {Quarteroni, Alfio and Rozza, Gianluigi},
  year =	 2014,
  keywords =	 {Computers / Programming / Algorithms, Mathematics /
                  Applied, Mathematics / Counting \& Numeration,
                  Mathematics / Discrete Mathematics, Mathematics /
                  Numerical Analysis, Mathematics / Probability \&
                  Statistics / Stochastic Processes, Science / Physics
                  / Mathematical \& Computational, Science / System
                  Theory, Technology \& Engineering / Mechanical,
                  Technology \& Engineering / Operations Research}
}

@article{repin_estimates_2009,
	title = {Estimates of deviations from exact solutions of initial boundary value problems for the wave equation},
	volume = {159},
	issn = {1072-3374, 1573-8795},
	url = {http://link.springer.com.recursos.biblioteca.upc.edu/article/10.1007/s10958-009-9435-x},
	doi = {10.1007/s10958-009-9435-x},
	abstract = {We derive computable upper bounds of the difference between an exact solution of the initial boundary value problem for a linear hyperbolic equation and any function in a space-time cylinder that belongs to the respective energy class. We prove that the bounds vanish if and only if the approximate solution coincides with the exact one. Bibliography: 13 titles.},
	pages = {229--240},
	number = {2},
        journal = {Journal of Mathematical Sciences},
	journaltitle = {Journal of Mathematical Sciences},
	shortjournal = {J Math Sci},
	author = {Repin, S.},
        year = {2009},
	urldate = {2017-01-02},
	date = {2009-05-01},
	langid = {english},
	file = {Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/HS9C8FHM/Repin - 2009 - Estimates of deviations from exact solutions of in.pdf:application/pdf;Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/DPKERWFB/s10958-009-9435-x.html:text/html}
}

% Michele

@article{rheinbach_parallel_2009,
  title =	 {Parallel Iterative Substructuring in Structural
                  Mechanics},
  volume =	 16,
  issn =	 {1134-3060, 1886-1784},
  url =
                  {http://link.springer.com/article/10.1007/s11831-009-9035-4},
  doi =		 {10.1007/s11831-009-9035-4},
  abstract =	 {Finite Element Tearing and Interconnecting ({FETI)}
                  methods are a family of nonoverlapping domain
                  decomposition methods which have been proven to be
                  robust and parallel scalable for a variety of
                  elliptic partial differential equations. Here, an
                  introduction to the classical onelevel {FETI}
                  methods is given, as well as to the more recent
                  dual-primal {FETI} methods and some of their
                  variants. With the advent of modern parallel
                  computers with thousands of processors, certain
                  inexact components are needed in these methods to
                  maintain scalability. An introduction to a recent
                  class of inexact dual-primal {FETI} methods is
                  presented. Scalability results for an elasticity
                  problem using 65 536 processor cores of the {JUGENE}
                  supercomputer at Forschungszentrum Jülich show the
                  potential of these methods. A hyperelastic problem
                  from biomechanics is presented as an application of
                  the methods to nonlinear finite element analysis.},
  language =	 {en},
  number =	 4,
  urldate =	 {2013-08-02},
  journal =	 {Archives of Computational Methods in Engineering},
  author =	 {Rheinbach, Oliver},
  year =	 2009,
  keywords =	 {{Appl.Mathematics/Computational} Methods of
                  Engineering},
  pages =	 {425--463},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/0xump8e8.default/zotero/storage/BSGPXUE8/Rheinbach
                  - 2009 - Parallel Iterative Substructuring in
                  Structural
                  Me.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/0xump8e8.default/zotero/storage/CJVBNH5F/10.html:text/html}
}

@article{rheinbach_parallel_2009,
  title =	 {Parallel Iterative Substructuring in Structural
                  Mechanics},
  volume =	 16,
  issn =	 {1134-3060, 1886-1784},
  url =
                  {http://link.springer.com/article/10.1007/s11831-009-9035-4},
  doi =		 {10.1007/s11831-009-9035-4},
  abstract =	 {Finite Element Tearing and Interconnecting ({FETI)}
                  methods are a family of nonoverlapping domain
                  decomposition methods which have been proven to be
                  robust and parallel scalable for a variety of
                  elliptic partial differential equations. Here, an
                  introduction to the classical onelevel {FETI}
                  methods is given, as well as to the more recent
                  dual-primal {FETI} methods and some of their
                  variants. With the advent of modern parallel
                  computers with thousands of processors, certain
                  inexact components are needed in these methods to
                  maintain scalability. An introduction to a recent
                  class of inexact dual-primal {FETI} methods is
                  presented. Scalability results for an elasticity
                  problem using 65 536 processor cores of the {JUGENE}
                  supercomputer at Forschungszentrum Jülich show the
                  potential of these methods. A hyperelastic problem
                  from biomechanics is presented as an application of
                  the methods to nonlinear finite element analysis.},
  language =	 {en},
  number =	 4,
  urldate =	 {2013-08-02},
  journal =	 {Archives of Computational Methods in Engineering},
  author =	 {Rheinbach, Oliver},
  year =	 2009,
  keywords =	 {{Appl.Mathematics/Computational} Methods of
                  Engineering},
  pages =	 {425--463},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/0xump8e8.default/zotero/storage/BSGPXUE8/Rheinbach
                  - 2009 - Parallel Iterative Substructuring in
                  Structural
                  Me.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/0xump8e8.default/zotero/storage/CJVBNH5F/10.html:text/html}
}

%Roadmaps

@article{riedlbauer_modelling_2014,
	title = {Modelling, simulation and experimental validation of heat transfer in selective laser melting of the polymeric material {PA}12},
	volume = {93},
	issn = {0927-0256},
	url = {http://www.sciencedirect.com/science/article/pii/S0927025614004571},
	doi = {10.1016/j.commatsci.2014.06.046},
	abstract = {One of the most promising additive manufacturing techniques is selective laser melting of semi-crystalline thermoplastic powders. In this process the powder is fused in defined, locally-restricted points in the current powder layer by the energy input of a laser beam and, thereby, bonded to the already fused material of previous layers. In this way geometrically complex parts are constructed layer-by-layer. The accuracy of the process and the properties of the resulting parts depend on numerous process parameters and their interactions. To improve the process strategy and to reduce, e.g. eigenstresses and warpage a thorough understanding of the influence of various process parameters is required. In the present contribution one step in this direction is done by analysing the laser energy input into a powder bulk for different process parameters by comparing temperature distributions and the size of melting pools. Experiments with single-line specimens are conducted, analysed and compared to numerical results from finite element simulations of the highly nonlinear thermal problem.},
	pages = {239--248},
        journal      = {Computational Materials Science},
	journaltitle = {Computational Materials Science},
	shortjournal = {Computational Materials Science},
	author       = {Riedlbauer, D. and Drexler, M. and Drummer, D. and Steinmann, P. and Mergheim, J.},
        year         = {2014},
	urldate      = {2017-01-12},
	date         = {2014-10},
	keywords     = {additive manufacturing, Heat transfer simulation, Selective laser melting},
	file         = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/TX6D6ZXX/Riedlbauer et al. - 2014 - Modelling, simulation and experimental validation .pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/6KFHQCKA/S0927025614004571.html:text/html}
}

@article{roberts_three-dimensional_2009,
  title =	 {A three-dimensional finite element analysis of the
                  temperature field during laser melting of metal
                  powders in additive layer manufacturing},
  volume =	 49,
  issn =	 {0890-6955},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0890695509001400},
  doi =		 {10.1016/j.ijmachtools.2009.07.004},
  abstract =	 {Simulating the transient temperature field in
                  additive layer manufacturing (ALM) processes has
                  presented a challenge to many researchers in the
                  field. The transient temperature history is vital
                  for determining the thermal stress distribution and
                  residual stress states in ALM-processed parts that
                  utilise a moving laser heat source. The modelling of
                  the problem involving multiple layers is equally of
                  great importance because the thermal interactions of
                  successive layers affect the temperature gradients,
                  which govern the heat transfer and thermal stress
                  development mechanisms. This paper uses an
                  innovative simulation technique known as element
                  birth and death, in modelling the three-dimensional
                  temperature field in multiple layers in a powder
                  bed. The results indicate that the heated regions
                  undergo rapid thermal cycles that could be
                  associated with commensurate thermal stress
                  cycles. Deposition of successive layers and
                  subsequent laser scanning produces temperature
                  spikes in previous layers. The resultant effect is a
                  steady temperature build-up in the lower layers as
                  the number of layers increases.},
  number =	 {12-13},
  urldate =	 {2015-12-29},
  journal =	 {International Journal of Machine Tools and
                  Manufacture},
  author =	 {Roberts, I. A. and Wang, C. J. and Esterlein, R. and
                  Stanford, M. and Mynors, D. J.},
  year =	 2009,
  keywords =	 {Additive layer manufacturing, Element birth and
                  death, Laser melting, Multilayer},
  pages =	 {916--923},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/NJ8EJ9JQ/Roberts
                  et al. - 2009 - A three-dimensional finite element
                  analysis of the.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/XDWADQS7/S0890695509001400.html:text/html}
}

@article{romano_thermal_2015,
  series =	 {43rd {North} {American} {Manufacturing} {Research}
                  {Conference}, {NAMRC} 43, 8-12 {June} 2015, {UNC}
                  {Charlotte}, {North} {Carolina}, {United} {States}},
  title =	 {Thermal {Modeling} of {Laser} {Based} {Additive}
                  {Manufacturing} {Processes} within {Common}
                  {Materials}},
  volume =	 1,
  issn =	 {2351-9789},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S2351978915010124},
  doi =		 {10.1016/j.promfg.2015.09.012},
  abstract =	 {Process optimization is an important area requiring
                  further research in the field of rapid prototyping
                  and manufacturing. Current research efforts are
                  focused on enhancing metallic powder bed additive
                  manufacturing processes such as Laser Melting and
                  Electron Beam Melting (EBM). Optimizing this class
                  of manufacturing processes can lead to revolutionary
                  changes in part quality and repeatability. Modeling
                  and simulation can be used as a facilitating tool to
                  predict the behavior of materials and processes and
                  alleviate the need for extensive random
                  experiments. This paper presents finite element
                  simulation of thermal modeling thermal modeling of
                  laser melting process to determine the melt pool
                  geometry and temperature distribution in powder
                  bed. This model was used to compare these
                  characteristics between commonly used powder
                  materials to include Ti6Al4 V, Stainless Steel 316L,
                  and 7075 Aluminum powders. Initially, a common set
                  of parameters were used for all materials and it was
                  found that melt pool could not be sustained in
                  aluminum and steel and only titanium process
                  resulted deep and complete melting and
                  solidification. Optimized process parameter sets are
                  suggested to develop consistent melt pools
                  throughout the build process for aluminum and
                  steel. It was discovered that steel powder beds
                  require higher beam power than titanium powder beds
                  to establish a consistent melt pool. Aluminum powder
                  beds need higher beam power than both titanium and
                  steel powder beds and also require a reduced scan
                  speed to maintain a consistent melt pool.},
  urldate =	 {2015-12-29},
  journal =	 {Procedia Manufacturing},
  author =	 {Romano, John and Ladani, Leila and Sadowski, Magda},
  year =	 2015,
  keywords =	 {Al7075, Finite element, Laser melting, Powder Bed,
                  SS316L, Ti6Al4 V, Transient Thermal Analysis},
  pages =	 {238--250},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/T92ISIK5/Romano
                  et al. - 2015 - Thermal Modeling of Laser Based
                  Additive Manufactu.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/FTRG8FGZ/S2351978915010124.html:text/html}
}

@article{ruprecht_extremescale_2013,
  title =	 {Extreme scale space-time parallelism},
  url =
                  {http://sc13.supercomputing.org/sites/default/files/PostersArchive/tech_posters/post148s2-file3.pdf},
  urldate =	 {2014-05-15},
  author =	 {Ruprecht, Daniel and Speck, Robert and Emmett,
                  Matthew and Bolten, Matthias and Krause, Rolf},
  year =	 2013,
  file =
                  {post148s2-file3.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/RG76EF36/post148s2-file3.pdf:application/pdf}
}

@article{ruprecht_extremescale_2013,
  title =	 {Extreme scale space-time parallelism},
  url =
                  {http://sc13.supercomputing.org/sites/default/files/PostersArchive/tech_posters/post148s2-file3.pdf},
  urldate =	 {2014-05-15},
  author =	 {Ruprecht, Daniel and Speck, Robert and Emmett,
                  Matthew and Bolten, Matthias and Krause, Rolf},
  year =	 2013,
  file =
                  {post148s2-file3.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/RG76EF36/post148s2-file3.pdf:application/pdf}
}

@book{saad_iterative_2003,
  edition =	 2,
  title =	 {Iterative Methods for Sparse Linear Systems, Second
                  Edition},
  isbn =	 0898715342,
  publisher =	 {Society for Industrial and Applied Mathematics},
  author =	 {Saad, Yousef},
  year =	 2003,
  file =
                  {IterMethBook_2ndEd.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/M3W4VWTA/IterMethBook_2ndEd.pdf:application/pdf}
}

%FE2 articles

@book{saad_iterative_2003,
  edition =	 2,
  title =	 {Iterative Methods for Sparse Linear Systems, Second
                  Edition},
  isbn =	 0898715342,
  publisher =	 {Society for Industrial and Applied Mathematics},
  author =	 {Saad, Yousef},
  year =	 2003,
  file =
                  {IterMethBook_2ndEd.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/M3W4VWTA/IterMethBook_2ndEd.pdf:application/pdf}
}

@article{sala_new_2008,
  title =	 {A New {Petrov-Galerkin} Smoothed Aggregation
                  Preconditioner for Nonsymmetric Linear Systems},
  volume =	 31,
  issn =	 {1064-8275},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/060659545},
  doi =		 {10.1137/060659545},
  abstract =	 {We propose a new variant of smoothed aggregation
                  ({SA)} suitable for nonsymmetric linear systems. The
                  new algorithm is based on two key generalizations of
                  {SA:} restriction smoothing and local
                  damping. Restriction smoothing refers to the
                  smoothing of a tentative restriction operator via a
                  damped Jacobi-like iteration. Restriction smoothing
                  is analogous to prolongator smoothing in standard
                  {SA} and in fact has the same form as the transpose
                  of prolongator smoothing when the matrix is
                  symmetric. Local damping refers to damping
                  parameters used in the Jacobi-like iteration. In
                  standard {SA}, a single damping parameter is
                  computed via an eigenvalue computation. Here, local
                  damping parameters are computed by considering the
                  minimization of an energy-like quantity for each
                  individual grid transfer basis function. Numerical
                  results are given showing how this method performs
                  on highly nonsymmetric systems., We propose a new
                  variant of smoothed aggregation ({SA)} suitable for
                  nonsymmetric linear systems. The new algorithm is
                  based on two key generalizations of {SA:}
                  restriction smoothing and local damping. Restriction
                  smoothing refers to the smoothing of a tentative
                  restriction operator via a damped Jacobi-like
                  iteration. Restriction smoothing is analogous to
                  prolongator smoothing in standard {SA} and in fact
                  has the same form as the transpose of prolongator
                  smoothing when the matrix is symmetric. Local
                  damping refers to damping parameters used in the
                  Jacobi-like iteration. In standard {SA}, a single
                  damping parameter is computed via an eigenvalue
                  computation. Here, local damping parameters are
                  computed by considering the minimization of an
                  energy-like quantity for each individual grid
                  transfer basis function. Numerical results are given
                  showing how this method performs on highly
                  nonsymmetric systems.},
  number =	 1,
  urldate =	 {2014-05-15},
  journal =	 {{SIAM} Journal on Scientific Computing},
  author =	 {Sala, M. and Tuminaro, R.},
  year =	 2008,
  pages =	 {143--166},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/APDIFJUB/Sala
                  and Tuminaro - 2008 - A New Petrov-Galerkin Smoothed
                  Aggregation
                  Precond.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/2F478DPK/060659545.html:text/html}
}

@article{sala_new_2008,
  title =	 {A New {Petrov-Galerkin} Smoothed Aggregation
                  Preconditioner for Nonsymmetric Linear Systems},
  volume =	 31,
  issn =	 {1064-8275},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/060659545},
  doi =		 {10.1137/060659545},
  abstract =	 {We propose a new variant of smoothed aggregation
                  ({SA)} suitable for nonsymmetric linear systems. The
                  new algorithm is based on two key generalizations of
                  {SA:} restriction smoothing and local
                  damping. Restriction smoothing refers to the
                  smoothing of a tentative restriction operator via a
                  damped Jacobi-like iteration. Restriction smoothing
                  is analogous to prolongator smoothing in standard
                  {SA} and in fact has the same form as the transpose
                  of prolongator smoothing when the matrix is
                  symmetric. Local damping refers to damping
                  parameters used in the Jacobi-like iteration. In
                  standard {SA}, a single damping parameter is
                  computed via an eigenvalue computation. Here, local
                  damping parameters are computed by considering the
                  minimization of an energy-like quantity for each
                  individual grid transfer basis function. Numerical
                  results are given showing how this method performs
                  on highly nonsymmetric systems., We propose a new
                  variant of smoothed aggregation ({SA)} suitable for
                  nonsymmetric linear systems. The new algorithm is
                  based on two key generalizations of {SA:}
                  restriction smoothing and local damping. Restriction
                  smoothing refers to the smoothing of a tentative
                  restriction operator via a damped Jacobi-like
                  iteration. Restriction smoothing is analogous to
                  prolongator smoothing in standard {SA} and in fact
                  has the same form as the transpose of prolongator
                  smoothing when the matrix is symmetric. Local
                  damping refers to damping parameters used in the
                  Jacobi-like iteration. In standard {SA}, a single
                  damping parameter is computed via an eigenvalue
                  computation. Here, local damping parameters are
                  computed by considering the minimization of an
                  energy-like quantity for each individual grid
                  transfer basis function. Numerical results are given
                  showing how this method performs on highly
                  nonsymmetric systems.},
  number =	 1,
  urldate =	 {2014-05-15},
  journal =	 {{SIAM} Journal on Scientific Computing},
  author =	 {Sala, M. and Tuminaro, R.},
  year =	 2008,
  pages =	 {143--166},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/APDIFJUB/Sala
                  and Tuminaro - 2008 - A New Petrov-Galerkin Smoothed
                  Aggregation
                  Precond.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/2F478DPK/060659545.html:text/html}
}

@article{sanderse_accuracy_2012,
  title =	 {Accuracy analysis of explicit {Runge-Kutta} methods
                  applied to the incompressible {Navier-Stokes}
                  equations},
  volume =	 231,
  issn =	 {0021-9991},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0021999111006838},
  doi =		 {10.1016/j.jcp.2011.11.028},
  abstract =	 {This paper investigates the temporal accuracy of the
                  velocity and pressure when explicit {Runge-Kutta}
                  methods are applied to the incompressible
                  {Navier-Stokes} equations. It is shown that, at
                  least up to and including fourth order, the velocity
                  attains the classical order of accuracy without
                  further constraints. However, in case of a
                  time-dependent gradient operator, which can appear
                  in case of time-varying meshes, additional order
                  conditions need to be satisfied to ensure the
                  correct order of accuracy. Furthermore, the pressure
                  is only first-order accurate unless additional order
                  conditions are satisfied. Two new methods that lead
                  to a second-order accurate pressure are proposed,
                  which are applicable to a certain class of three-
                  and four-stage methods. A special case appears when
                  the boundary conditions for the continuity equation
                  are independent of time, since in that case the
                  pressure can be computed to the same accuracy as the
                  velocity field, without additional cost. Relevant
                  computations of decaying vortices and of an actuator
                  disk in a time-dependent inflow support the analysis
                  and the proposed methods.},
  number =	 8,
  urldate =	 {2013-03-12},
  journal =	 {Journal of Computational Physics},
  author =	 {Sanderse, B. and Koren, B.},
  year =	 2012,
  keywords =	 {Differential-algebraic equations, incompressible
                  {Navier-Stokes} equations, Moving meshes,
                  {Runge-Kutta} method, Temporal accuracy, time
                  integration},
  pages =	 {3041--3063},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/JND6I64I/Sanderse
                  y Koren - 2012 - Accuracy analysis of explicit
                  Runge-Kutta methods
                  .pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/2GRDG334/S0021999111006838.html:text/html}
}

% Microstructure, solidification, and melting

@article{sanderse_accuracy_2012,
  title =	 {Accuracy analysis of explicit {Runge-Kutta} methods
                  applied to the incompressible {Navier-Stokes}
                  equations},
  volume =	 231,
  issn =	 {0021-9991},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0021999111006838},
  doi =		 {10.1016/j.jcp.2011.11.028},
  abstract =	 {This paper investigates the temporal accuracy of the
                  velocity and pressure when explicit {Runge-Kutta}
                  methods are applied to the incompressible
                  {Navier-Stokes} equations. It is shown that, at
                  least up to and including fourth order, the velocity
                  attains the classical order of accuracy without
                  further constraints. However, in case of a
                  time-dependent gradient operator, which can appear
                  in case of time-varying meshes, additional order
                  conditions need to be satisfied to ensure the
                  correct order of accuracy. Furthermore, the pressure
                  is only first-order accurate unless additional order
                  conditions are satisfied. Two new methods that lead
                  to a second-order accurate pressure are proposed,
                  which are applicable to a certain class of three-
                  and four-stage methods. A special case appears when
                  the boundary conditions for the continuity equation
                  are independent of time, since in that case the
                  pressure can be computed to the same accuracy as the
                  velocity field, without additional cost. Relevant
                  computations of decaying vortices and of an actuator
                  disk in a time-dependent inflow support the analysis
                  and the proposed methods.},
  number =	 8,
  urldate =	 {2013-03-12},
  journal =	 {Journal of Computational Physics},
  author =	 {Sanderse, B. and Koren, B.},
  year =	 2012,
  keywords =	 {Differential-algebraic equations, incompressible
                  {Navier-Stokes} equations, Moving meshes,
                  {Runge-Kutta} method, Temporal accuracy, time
                  integration},
  pages =	 {3041--3063},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/JND6I64I/Sanderse
                  y Koren - 2012 - Accuracy analysis of explicit
                  Runge-Kutta methods
                  .pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/2GRDG334/S0021999111006838.html:text/html}
}

@article{sauerland_meso-macro_2011,
  title =	 {Meso-{Macro} {Modelling} of {Coated} {Forming}
                  {Tools} under {Thermal} {Shock} {Conditions}},
  volume =	 11,
  copyright =	 {Copyright © 2011 WILEY-VCH Verlag GmbH \& Co. KGaA,
                  Weinheim},
  issn =	 {1617-7061},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/pamm.201110265/abstract},
  doi =		 {10.1002/pamm.201110265},
  abstract =	 {In hybrid-forming processes workpieces are heated up
                  before forming in order to reduce the forming
                  forces. They are innovative methods for the
                  production of components with graded properties,
                  particularly with regard to tailored material
                  properties and geometrical shape. During service
                  life the forming tools are subjected to cyclic
                  thermal shock loading conditions which can result
                  into damage and failure. For improvement of the tool
                  durability in the hybrid-forming process coated
                  forming tools with multilayered coating systems are
                  considered to be applied in future. This
                  contribution shows the actual state of work for the
                  development of a twoscale FE model for the
                  simulation of the multilayered coated forming
                  tool. Within this model the three-dimensional model
                  of the forming tool builds the macromodel. On the
                  macrolevel the multilayered coating is discretized
                  with one element over the coating thickness. The
                  mesomodel of the coating considers the actual layer
                  design with metallic and ceramic layers. The
                  macro-meso transition is realized with a
                  Taylor-assumption. As the microscale is not
                  considered in our model, the constitutive equations
                  are formulated on the mesoscale. The meso-macro
                  transition is done using volume averaging
                  procedures. Furthermore, a damage model is included
                  for particular layers. The scalar damage variable is
                  used in a thermo-mechanical coupled model for
                  simulation of a reduced heat transfer through a
                  partially damaged layer. (© 2011 Wiley-VCH Verlag
                  GmbH \& Co. KGaA, Weinheim)},
  language =	 {en},
  number =	 1,
  urldate =	 {2015-12-29},
  journal =	 {Proceedings in Applied Mathematics and Mechanics},
  author =	 {Sauerland, Kim-Henning and Mahnken, Rolf},
  year =	 2011,
  pages =	 {551--552},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/9QJZ56DH/Sauerland
                  and Mahnken - 2011 - Meso-Macro Modelling of Coated
                  Forming Tools
                  under.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/JSJG6U7P/abstract.html:text/html}
}

%Multiscale phase transition

@ARTICLE{schenk_solving_2004,
  author =	 {Schenk, Olaf and G\"artner, Klaus},
  title =	 {Solving unsymmetric sparse systems of linear
                  equations with {PARDISO}},
  journal =	 {Future Generation Computer Systems},
  year =	 2004,
  volume =	 20,
  pages =	 {475--487},
  number =	 3,
  abstract =	 {Supernode partitioning for unsymmetric matrices
                  together with complete block diagonal supernode
                  pivoting and asynchronous computation can achieve
                  high gigaflop rates for parallel sparse {LU}
                  factorization on shared memory parallel
                  computers. The progress in weighted graph matching
                  algorithms helps to extend these concepts further
                  and unsymmetric prepermutation of rows is used to
                  place large matrix entries on the diagonal. Complete
                  block diagonal supernode pivoting allows dynamical
                  interchanges of columns and rows during the
                  factorization process.  The level-3 {BLAS}
                  efficiency is retained and an advanced two-level
                  left--right looking scheduling scheme results in
                  good speedup on {SMP} machines. These algorithms
                  have been integrated into the recent unsymmetric
                  version of the {PARDISO} solver. Experiments
                  demonstrate that a wide set of unsymmetric linear
                  systems can be solved and high performance is
                  consistently achieved for large sparse unsymmetric
                  matrices from real world applications.},
  doi =		 {10.1016/j.future.2003.07.011},
  file =	 {ScienceDirect Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/29TKA9EX/Schenk
                  y GÃ¤rtner - 2004 - Solving unsymmetric sparse
                  systems of linear
                  equat.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/6APMDPM3/S0167739X03001882.html:text/html},
  issn =	 {0167-{739X}},
  keywords =	 {Computational sciences, Direct solver, Numerical
                  linear algebra, Unsymmetric linear systems},
  owner =	 {principe},
  shorttitle =	 {Selected numerical algorithms},
  timestamp =	 {2014.03.23},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0167739X03001882},
  urldate =	 {2012-09-20}
}

@ARTICLE{schenk_solving_2004,
  author =	 {Schenk, Olaf and G\"artner, Klaus},
  title =	 {Solving unsymmetric sparse systems of linear
                  equations with {PARDISO}},
  journal =	 {Future Generation Computer Systems},
  year =	 2004,
  volume =	 20,
  pages =	 {475--487},
  number =	 3,
  abstract =	 {Supernode partitioning for unsymmetric matrices
                  together with complete block diagonal supernode
                  pivoting and asynchronous computation can achieve
                  high gigaflop rates for parallel sparse {LU}
                  factorization on shared memory parallel
                  computers. The progress in weighted graph matching
                  algorithms helps to extend these concepts further
                  and unsymmetric prepermutation of rows is used to
                  place large matrix entries on the diagonal. Complete
                  block diagonal supernode pivoting allows dynamical
                  interchanges of columns and rows during the
                  factorization process.  The level-3 {BLAS}
                  efficiency is retained and an advanced two-level
                  left--right looking scheduling scheme results in
                  good speedup on {SMP} machines. These algorithms
                  have been integrated into the recent unsymmetric
                  version of the {PARDISO} solver. Experiments
                  demonstrate that a wide set of unsymmetric linear
                  systems can be solved and high performance is
                  consistently achieved for large sparse unsymmetric
                  matrices from real world applications.},
  doi =		 {10.1016/j.future.2003.07.011},
  file =	 {ScienceDirect Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/29TKA9EX/Schenk
                  y GÃ¤rtner - 2004 - Solving unsymmetric sparse
                  systems of linear
                  equat.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/6APMDPM3/S0167739X03001882.html:text/html},
  issn =	 {0167-{739X}},
  keywords =	 {Computational sciences, Direct solver, Numerical
                  linear algebra, Unsymmetric linear systems},
  owner =	 {principe},
  shorttitle =	 {Selected numerical algorithms},
  timestamp =	 {2014.03.23},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0167739X03001882},
  urldate =	 {2012-09-20}
}

@article{schillinger_finite_2014,
  title =	 {The {Finite} {Cell} {Method}: {A} {Review} in the
                  {Context} of {Higher}-{Order} {Structural}
                  {Analysis} of {CAD} and {Image}-{Based} {Geometric}
                  {Models}},
  volume =	 22,
  issn =	 {1134-3060, 1886-1784},
  shorttitle =	 {The {Finite} {Cell} {Method}},
  url =
                  {http://link.springer.com/article/10.1007/s11831-014-9115-y},
  doi =		 {10.1007/s11831-014-9115-y},
  abstract =	 {The finite cell method is an embedded domain method,
                  which combines the fictitious domain approach with
                  higher-order finite elements, adaptive integration,
                  and weak enforcement of unfitted essential boundary
                  conditions. Its core idea is to use a simple
                  unfitted structured mesh of higher-order basis
                  functions for the approximation of the solution
                  fields, while the geometry is captured by means of
                  adaptive quadrature points. This eliminates the need
                  for boundary conforming meshes that require
                  time-consuming and error-prone mesh generation
                  procedures, and opens the door for a seamless
                  integration of very complex geometric models into
                  finite element analysis. At the same time, the
                  finite cell method achieves full accuracy,
                  i.e. optimal rates of convergence, when the mesh is
                  refined, and exponential rates of convergence, when
                  the polynomial degree is increased. Due to the
                  flexibility of the quadrature based geometry
                  approximation, the finite cell method can operate
                  with almost any geometric model, ranging from
                  boundary representations in computer aided geometric
                  design to voxel representations obtained from
                  medical imaging technologies. In this review
                  article, we first provide a concise introduction to
                  the basics of the finite cell method. We then
                  summarize recent developments of the technology,
                  with particular emphasis on the research topics in
                  which we have been actively involved. These include
                  the finite cell method with B-spline and NURBS basis
                  functions, the treatment of geometric nonlinearities
                  for large deformation analysis, the weak enforcement
                  of boundary and coupling conditions, and local
                  refinement schemes. We illustrate the capabilities
                  and advantages of the finite cell method with
                  several challenging examples, e.g. the image-based
                  analysis of foam-like structures, the
                  patient-specific analysis of a human femur bone, the
                  analysis of volumetric structures based on CAD
                  boundary representations, and the isogeometric
                  treatment of trimmed NURBS surfaces. We conclude our
                  review by briefly discussing some key aspects for
                  the efficient implementation of the finite cell
                  method.},
  language =	 {en},
  number =	 3,
  urldate =	 {2015-12-31},
  journal =	 {Archives of Computational Methods in Engineering},
  author =	 {Schillinger, Dominik and Ruess, Martin},
  year =	 2014,
  keywords =	 {Appl.Mathematics/Computational Methods of
                  Engineering},
  pages =	 {391--455},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/932F66U9/Schillinger
                  and Ruess - 2014 - The Finite Cell Method A Review
                  in the Context
                  of.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/V4UZNEDT/s11831-014-9115-y.html:text/html}
}

@incollection{schoberl_domain_2013,
  series =	 {Lecture Notes in Applied and Computational
                  Mechanics},
  title =	 {Domain Decomposition Preconditioning for High Order
                  Hybrid Discontinuous Galerkin Methods on Tetrahedral
                  Meshes},
  copyright =	 {©2013 Springer-Verlag Berlin Heidelberg},
  isbn =	 {978-3-642-30315-9, 978-3-642-30316-6},
  url =
                  {http://link.springer.com/chapter/10.1007/978-3-642-30316-6_2},
  abstract =	 {Hybrid discontinuous Galerkin methods are popular
                  discretization methods in applications from fluid
                  dynamics and many others. Often large scale linear
                  systems arising from elliptic operators have to be
                  solved. We show that standard p-version domain
                  decomposition techniques can be applied, but we have
                  to develop new technical tools to prove
                  poly-logarithmic condition number estimates, in
                  particular on tetrahedral meshes.},
  number =	 66,
  urldate =	 {2014-05-15},
  booktitle =	 {Advanced Finite Element Methods and Applications},
  publisher =	 {Springer Berlin Heidelberg},
  author =	 {Sch\"oberl, Joachim and Lehrenfeld, Christoph},
  editor =	 {Apel, Thomas and Steinbach, Olaf},
  year =	 2013,
  keywords =	 {{Appl.Mathematics/Computational} Methods of
                  Engineering, Computational Mathematics and Numerical
                  Analysis, Mechanics, Theoretical and Applied
                  Mechanics},
  pages =	 {27--56},
  file =
                  {Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/K2U8337D/10.html:text/html}
}


% Phase field modelling

@techreport{schraad_asc_2015,
  title =	 {{ASC} {Additive} {Manufacturing}},
  url =
                  {http://permalink.lanl.gov/object/tr?what=info:lanl-repo/lareport/LA-UR-15-24605},
  urldate =	 {2015-12-29},
  institution =	 {Los Alamos National Laboratory (LANL)},
  author =	 {Schraad, Mark William and Fran\c{c}ois, Marianne M.},
  year =	 2015,
  file =
                  {tr.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/BBREQ6JZ/tr.pdf:application/pdf}
}

@incollection{schroder_numerical_2014,
  series =	 {{CISM} {International} {Centre} for {Mechanical}
                  {Sciences}},
  title =	 {A numerical two-scale homogenization scheme: the
                  {FE}$^2$-method},
  copyright =	 {©2014 CISM Udine},
  isbn =	 {978-3-7091-1624-1 978-3-7091-1625-8},
  shorttitle =	 {A numerical two-scale homogenization scheme},
  url =
                  {http://link.springer.com/chapter/10.1007/978-3-7091-1625-8_1},
  abstract =	 {A wide class of micro-heterogeneous materials is
                  designed to satisfy the advanced challenges of
                  modern materials occurring in a variety of technical
                  applications. The effective macroscopic properties
                  of such materials are governed by the complex
                  interaction of the individual constituents of the
                  associated microstructure. A sufficient macroscopic
                  phenomenological description of these materials up
                  to a certain order of accuracy can be very
                  complicated or even impossible. On the contrary, a
                  whole resolution of the fine scale for the
                  macroscopic boundary value problem by means of a
                  classical discretization technique seems to be too
                  elaborate. Instead of developing a macroscopic
                  phenomenological constitutive law, it is possible to
                  attach a representative volume element (RVE) of the
                  microstructure at each point of the macrostructure;
                  this results in a two-scale modeling scheme. A
                  discrete version of this scheme performing finite
                  element (FE) discretizations of the boundary value
                  problems on both scales, the macro- and the
                  micro-scale, is denoted as the FE2-method or as the
                  multilevel finite element method. The main advantage
                  of this procedure is based on the fact that we do
                  not have to define a macroscopic phenomenological
                  constitutive law; this is replaced by suitable
                  averages of stress measures and deformation tensors
                  over the microstructure. Details concerning the
                  definition of the macroscopic quantities in terms of
                  their microscopic counterparts, the
                  definition/construction of boundary conditions on
                  the RVE as well as the consistent linearization of
                  the macroscopic constitutive equations are discussed
                  in this contribution. Furthermore, remarks
                  concerning stability problems on both scales as well
                  as their interactions are given and representative
                  numerical examples for elasto-plastic
                  microstructures are discussed.},
  language =	 {en},
  volume =	 550,
  urldate =	 {2015-12-29},
  booktitle =	 {Plasticity and {Beyond}},
  publisher =	 {Springer},
  author =	 {Schr\"{o}der, J\"{o}rg},
  editor =	 {Schr\"{o}der, J\"{o}rg and Hackl, Klaus},
  year =	 2014,
  keywords =	 {Characterization and Evaluation of Materials,
                  Continuum Mechanics and Mechanics of Materials,
                  Mathematical Modeling and Industrial Mathematics},
  pages =	 {1--64},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/3GSXFU9U/Schröder
                  - 2014 - A numerical two-scale homogenization scheme
                  the
                  F.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/3TZWQMWA/10.html:text/html}
}

@article{schwab_sparse_2011,
  title =	 {Sparse tensor discretizations of high-dimensional
                  parametric and stochastic {PDEs}},
  volume =	 20,
  issn =	 {1474-0508},
  url =
                  {http://journals.cambridge.org/article_S0962492911000055},
  doi =		 {10.1017/S0962492911000055},
  abstract =	 {Partial differential equations (PDEs) with random
                  input data, such as random loadings and
                  coefficients, are reformulated as parametric,
                  deterministic PDEs on parameter spaces of high,
                  possibly infinite dimension. Tensorized operator
                  equations for spatial and temporal k-point
                  correlation functions of their random solutions are
                  derived. Parametric, deterministic PDEs for the laws
                  of the random solutions are derived. Representations
                  of the random solutions' laws on
                  infinite-dimensional parameter spaces in terms of
                  ‘generalized polynomial chaos’ (GPC) series are
                  established. Recent results on the regularity of
                  solutions of these parametric PDEs are
                  presented. Convergence rates of best N-term
                  approximations, for adaptive stochastic Galerkin and
                  collocation discretizations of the parametric,
                  deterministic PDEs, are established. Sparse tensor
                  products of hierarchical (multi-level)
                  discretizations in physical space (and time), and
                  GPC expansions in parameter space, are shown to
                  converge at rates which are independent of the
                  dimension of the parameter space. A convergence
                  analysis of multi-level Monte Carlo (MLMC)
                  discretizations of PDEs with random coefficients is
                  presented. Sufficient conditions on the random
                  inputs for superiority of sparse tensor
                  discretizations over MLMC discretizations are
                  established for linear elliptic, parabolic and
                  hyperbolic PDEs with random coefficients.},
  urldate =	 {2016-01-15},
  journal =	 {Acta Numerica},
  author =	 {Schwab, Christoph and Gittelson, Claude Jeffrey},
  year =	 2011,
  pages =	 {291--467},
  file =	 {Cambridge Journals
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/GJEAX2QD/displayAbstract.html:text/html}
}

%Reviews
@article{sederberg_watertight_2008,
abstract = {This paper addresses the long-standing problem of the unavoidable gaps that arise when expressing the intersection of two NURBS surfaces using conventional trimmed-NURBS representation. The solution converts each trimmed NURBS into an untrimmed T-Spline, and then merges the untrimmed T-Splines into a single, watertight model. The solution enables watertight fillets of NURBS models, as well as arbitrary feature curves that do not have to follow iso-parameter curves. The resulting T-Spline representation can be exported without error as a collection of NURBS surfaces. {\textcopyright} 2008 ACM.},
author = {Sederberg, Thomas W. and Finnigan, G. Thomas and Li, Xin and Lin, Hongwei and Ipson, Heather},
doi = {10.1145/1360612.1360678},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {Booleans,NURBS,Surface intersection,T-splines},
mendeley-groups = {ComputationalGeometry},
month = {aug},
number = {3},
title = {{Watertight trimmed NURBS}},
volume = {27},
year = {2008}
}

@article{seidel_simulation_2014,
  series =	 {8th {International} {Conference} on {Digital}
                  {Enterprise} {Technology} - {DET} 2014 {Disruptive}
                  {Innovation} in {Manufacturing} {Engineering}
                  towards the 4th {Industrial} {Revolution}},
  title =	 {Simulation of the {Laser} {Beam} {Melting} {Process}
                  - {Approaches} for an {Efficient} {Modelling} of the
                  {Beam}-material {Interaction}},
  volume =	 25,
  issn =	 {2212-8271},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S2212827114010543},
  doi =		 {10.1016/j.procir.2014.10.023},
  abstract =	 {Currently, the main field of application of additive
                  manufacturing processes is shifting from research
                  laboratories to production facilities. Simulation
                  models can foster this transition by providing
                  support in process development and design. This
                  paper introduces approaches to modelling the
                  beam-material interaction in laser beam melting on a
                  level of detail that allows the simulation of the
                  whole build-up process of parts, not only of single
                  laser tracks. Thus both the achievable result
                  accuracy and the needed calculation time are
                  discussed. For this purpose, fundamental
                  correlations to link process characteristics with
                  model parameters are explained. Subsequently, four
                  modelling approaches are analysed. After an
                  introduction of the well-known method of applying a
                  uniform load on a whole layer compound (e. g. [1]),
                  the developed methods are discussed which allow
                  modelling the beam-material interaction on a more
                  detailed level. Thereby, the focus lies on the
                  ability to model load gradients perpendicular to the
                  build direction. This article is completed with a
                  discussion of simulated temperature curves for
                  selected monitoring points using two different
                  modelling approaches.},
  urldate =	 {2015-12-29},
  journal =	 {Procedia CIRP},
  author =	 {Seidel, C. and Zaeh, M. F. and Wunderer, M. and
                  Weirather, J. and Krol, T. A. and Ott, M.},
  year =	 2014,
  keywords =	 {additive manufacturing, Laser beam melting,
                  Modelling},
  pages =	 {146--153},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/7PKTDRXI/Seidel
                  et al. - 2014 - Simulation of the Laser Beam Melting
                  Process - App.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/XK5X9U5T/S2212827114010543.html:text/html}
}

@article{sharf_space-time_2008,
  title =	 {Space-time {Surface} {Reconstruction} {Using}
                  {Incompressible} {Flow}},
  author =	 {Sharf, Andrei and Alcantara, Dan A. and Lewiner,
                  Thomas and Greif, Chen and Sheffer, Alla and Amenta,
                  Nina and Cohen-Or, Daniel},
  year =	 2008,
  journal =	 {{ACM} Transactions on Graphics - Proceedings of ACM
                  SIGGRAPH Asia 2008},
  volume =	 27,
  number =	 5,
  pages =	 110
}

@article{sistek_application_2011,
  title =	 {Application of the parallel {BDDC} preconditioner to
                  the Stokes flow},
  volume =	 46,
  issn =	 {0045-7930},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045793011000053},
  doi =		 {10.1016/j.compfluid.2011.01.002},
  abstract =	 {A parallel implementation of the Balancing Domain
                  Decomposition by Constraints ({BDDC)} method is
                  described. It is based on formulation of {BDDC} with
                  global matrices without explicit coarse problem. The
                  implementation is based on the {MUMPS} parallel
                  solver for computing the approximate inverse used
                  for preconditioning. It is successfully applied to
                  several problems of Stokes flow discretized by
                  {Taylor-Hood} finite elements and {BDDC} is shown to
                  be a promising method also for this class of
                  problems.},
  number =	 1,
  urldate =	 {2012-08-07},
  journal =	 {Computers \& Fluids},
  author =	 {J. \v{S}\'istek and B. Soused\'ik and P . Burda and
                  J. Mandel and J. Novotn\'y},
  year =	 2011,
  keywords =	 {{BDDC}, domain decomposition, Iterative
                  substructuring, Stokes flow},
  pages =	 {429--435},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/H2QZH832/Šístek
                  et al. - 2011 - Application of the parallel BDDC
                  preconditioner to.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/UHWC6HEV/S0045793011000053.html:text/html}
}

@article{sistek_application_2011,
  title =	 {Application of the parallel {BDDC} preconditioner to
                  the Stokes flow},
  volume =	 46,
  issn =	 {0045-7930},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045793011000053},
  doi =		 {10.1016/j.compfluid.2011.01.002},
  abstract =	 {A parallel implementation of the Balancing Domain
                  Decomposition by Constraints ({BDDC)} method is
                  described. It is based on formulation of {BDDC} with
                  global matrices without explicit coarse problem. The
                  implementation is based on the {MUMPS} parallel
                  solver for computing the approximate inverse used
                  for preconditioning. It is successfully applied to
                  several problems of Stokes flow discretized by
                  {Taylor-Hood} finite elements and {BDDC} is shown to
                  be a promising method also for this class of
                  problems.},
  number =	 1,
  urldate =	 {2012-08-07},
  journal =	 {Computers \& Fluids},
  author =	 {Šístek, Jakub and Sousedík, Bedřich and Burda, Pavel
                  and Mandel, Jan and Novotný, Jaroslav},
  year =	 2011,
  keywords =	 {{BDDC}, domain decomposition, Iterative
                  substructuring, Stokes flow},
  pages =	 {429--435},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/H2QZH832/Šístek
                  et al. - 2011 - Application of the parallel BDDC
                  preconditioner to.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/UHWC6HEV/S0045793011000053.html:text/html}
}

% Surrogate models

@ARTICLE{sistek_face-based_????,
  author =	 {\v{S}\'istek, Jakub and \v{C}ert\'ikov\'a, Marta and
                  Burda, Pavel and Novotn\'y, Jaroslav},
  title =	 {Face-based selection of corners in {3D}
                  substructuring},
  journal =	 {Mathematics and Computers in Simulation},
  year =	 2012,
  volume =	 82,
  pages =	 {1799--1811},
  number =	 10,
  abstract =	 {In most recent substructuring methods, a fundamental
                  role is played by the coarse space. For some of
                  these methods (e.g. {BDDC} and {FETI-DP)}, its
                  definition relies on a ‘minimal’ set of coarse nodes
                  (sometimes called corners) which assures
                  invertibility of local subdomain problems and also
                  of the global coarse problem. This basic set is
                  typically enhanced by enforcing continuity of
                  functions at some generalized degrees of freedom,
                  such as average values on edges or faces of
                  subdomains.  We revisit existing algorithms for
                  selection of corners. The main contribution of this
                  paper consists of proposing a new heuristic
                  algorithm for this purpose. Considering faces as the
                  basic building blocks of the interface, inherent
                  parallelism, and better robustness with respect to
                  disconnected subdomains are among features of the
                  new technique. The advantages of the presented
                  algorithm in comparison to some earlier approaches
                  are demonstrated on three engineering problems of
                  structural analysis solved by the {BDDC} method.},
  doi =		 {10.1016/j.matcom.2011.06.007},
  file =	 {ScienceDirect Full Text
                  PDF:/Users/sbadia/Work/zotero/storage/7DKXW8GQ/Šístek
                  et al. - Face-based selection of corners in 3D
                  substructuri.pdf:application/pdf;ScienceDirect
                  Snapshot:/Users/sbadia/Work/zotero/storage/IH7A63J9/S0378475411001820.html:text/html},
  issn =	 {378--4754},
  keywords =	 {{BDDC}, Corner selection, domain decomposition,
                  Iterative substructuring, Parallel algorithms},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0378475411001820}
}

% Thermo-mechanical simulation

@ARTICLE{sistek_face-based_????,
  author =	 {\v{S}\'istek, Jakub and \v{C}ert\'ikov\'a, Marta and
                  Burda, Pavel and Novotn\'y, Jaroslav},
  title =	 {Face-based selection of corners in {3D}
                  substructuring},
  journal =	 {Mathematics and Computers in Simulation},
  year =	 2012,
  volume =	 82,
  pages =	 {1799--1811},
  number =	 10,
  abstract =	 {In most recent substructuring methods, a fundamental
                  role is played by the coarse space. For some of
                  these methods (e.g. {BDDC} and {FETI-DP)}, its
                  definition relies on a ‘minimal’ set of coarse nodes
                  (sometimes called corners) which assures
                  invertibility of local subdomain problems and also
                  of the global coarse problem. This basic set is
                  typically enhanced by enforcing continuity of
                  functions at some generalized degrees of freedom,
                  such as average values on edges or faces of
                  subdomains.  We revisit existing algorithms for
                  selection of corners. The main contribution of this
                  paper consists of proposing a new heuristic
                  algorithm for this purpose. Considering faces as the
                  basic building blocks of the interface, inherent
                  parallelism, and better robustness with respect to
                  disconnected subdomains are among features of the
                  new technique. The advantages of the presented
                  algorithm in comparison to some earlier approaches
                  are demonstrated on three engineering problems of
                  structural analysis solved by the {BDDC} method.},
  doi =		 {10.1016/j.matcom.2011.06.007},
  file =	 {ScienceDirect Full Text
                  PDF:/Users/sbadia/Work/zotero/storage/7DKXW8GQ/Šístek
                  et al. - Face-based selection of corners in 3D
                  substructuri.pdf:application/pdf;ScienceDirect
                  Snapshot:/Users/sbadia/Work/zotero/storage/IH7A63J9/S0378475411001820.html:text/html},
  issn =	 {378--4754},
  keywords =	 {{BDDC}, Corner selection, domain decomposition,
                  Iterative substructuring, Parallel algorithms},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0378475411001820}
}

@ARTICLE{sistek_face-based_????,
  author =	 {\v{S}\'istek, Jakub and \v{C}ert\'ikov\'a, Marta and
                  Burda, Pavel and Novotn\'y, Jaroslav},
  title =	 {Face-based selection of corners in {3D}
                  substructuring},
  journal =	 {Mathematics and Computers in Simulation},
  year =	 2012,
  volume =	 82,
  pages =	 {1799--1811},
  number =	 10,
  abstract =	 {In most recent substructuring methods, a fundamental
                  role is played by the coarse space. For some of
                  these methods (e.g. {BDDC} and {FETI-DP)}, its
                  definition relies on a ‘minimal’ set of coarse nodes
                  (sometimes called corners) which assures
                  invertibility of local subdomain problems and also
                  of the global coarse problem. This basic set is
                  typically enhanced by enforcing continuity of
                  functions at some generalized degrees of freedom,
                  such as average values on edges or faces of
                  subdomains.  We revisit existing algorithms for
                  selection of corners. The main contribution of this
                  paper consists of proposing a new heuristic
                  algorithm for this purpose. Considering faces as the
                  basic building blocks of the interface, inherent
                  parallelism, and better robustness with respect to
                  disconnected subdomains are among features of the
                  new technique. The advantages of the presented
                  algorithm in comparison to some earlier approaches
                  are demonstrated on three engineering problems of
                  structural analysis solved by the {BDDC} method.},
  doi =		 {10.1016/j.matcom.2011.06.007},
  file =	 {ScienceDirect Full Text
                  PDF:/Users/sbadia/Work/zotero/storage/7DKXW8GQ/Šístek
                  et al. - Face-based selection of corners in 3D
                  substructuri.pdf:application/pdf;ScienceDirect
                  Snapshot:/Users/sbadia/Work/zotero/storage/IH7A63J9/S0378475411001820.html:text/html},
  issn =	 {378--4754},
  keywords =	 {{BDDC}, Corner selection, domain decomposition,
                  Iterative substructuring, Parallel algorithms},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0378475411001820}
}

@article{smith_linking_2016,
  title =	 {Linking process, structure, property, and
                  performance for metal-based additive manufacturing:
                  computational approaches with experimental support},
  issn =	 {0178-7675, 1432-0924},
  shorttitle =	 {Linking process, structure, property, and
                  performance for metal-based additive manufacturing},
  url =		 {http://link.springer.com/10.1007/s00466-015-1240-4},
  doi =		 {10.1007/s00466-015-1240-4},
  language =	 {en},
  urldate =	 {2016-01-15},
  journal =	 {Computational Mechanics},
  author =	 {Smith, Jacob and Xiong, Wei and Yan, Wentao and Lin,
                  Stephen and Cheng, Puikei and Kafka, Orion L. and
                  Wagner, Gregory J. and Cao, Jian and Liu, Wing Kam},
  year =	 2016,
  file =
                  {art%3A10.1007%2Fs00466-015-1240-4.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/NNBVV3T2/art%3A10.1007%2Fs00466-015-1240-4.pdf:application/pdf}
}

@article{smolentsev_approach_2015,
  title =	 {An approach to verification and validation of {MHD}
                  codes for fusion applications},
  volume =	 100,
  issn =	 09203796,
  url =
                  {http://linkinghub.elsevier.com/retrieve/pii/S0920379614003263},
  doi =		 {10.1016/j.fusengdes.2014.04.049},
  language =	 {en},
  urldate =	 {2015-10-22},
  journal =	 {Fusion Engineering and Design},
  author =	 {Smolentsev, S. and Badia, S. and Bhattacharyay,
                  R. and B\"uhler, L. and Chen, L. and Huang, Q. and
                  Jin, H.-G. and Krasnov, D. and Lee, D.-W. and de les
                  Valls, E. Mas and Mistrangelo, C. and Munipalli,
                  R. and Ni, M.-J. and Pashkevich, D. and Patel,
                  A. and Pulugundla, G. and Satyamurthy, P. and
                  Snegirev, A. and Sviridov, V. and Swain, P. and
                  Zhou, T. and Zikanov, O.},
  year =	 2015,
  pages =	 {65--72},
  file =
                  {1-s2.0-S0920379614003263-main.pdf:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/SSP235CH/1-s2.0-S0920379614003263-main.pdf:application/pdf}
}

@article{sousedik_adaptive-multilevel_2013,
  title =	 {Adaptive-Multilevel {BDDC} and its parallel
                  implementation},
  volume =	 95,
  issn =	 {0010-{485X}, 1436-5057},
  url =
                  {http://link.springer.com/article/10.1007/s00607-013-0293-5},
  doi =		 {10.1007/s00607-013-0293-5},
  abstract =	 {We combine the adaptive and multilevel approaches to
                  the {BDDC} and formulate a method which allows an
                  adaptive selection of constraints on each
                  decomposition level. We also present a strategy for
                  the solution of local eigenvalue problems in the
                  adaptive algorithm using the {LOBPCG} method with a
                  preconditioner based on standard components of the
                  {BDDC.} The effectiveness of the method is
                  illustrated on several engineering problems. It
                  appears that the Adaptive-Multilevel {BDDC}
                  algorithm is able to effectively detect troublesome
                  parts on each decomposition level and improve
                  convergence of the method. The developed open-source
                  parallel implementation shows a good scalability as
                  well as applicability to very large problems and
                  core counts.},
  number =	 12,
  urldate =	 {2014-02-07},
  journal =	 {Computing},
  author =	 {Soused\'ik, Bed\v{r}ich and \v{S}\'istek, Jakub and
                  Mandel, Jan},
  year =	 2013,
  keywords =	 {{65M55}, {65N55}, {65Y05}, Adaptive constraints,
                  Artificial Intelligence (incl. Robotics), {BDDC},
                  Computer Appl. in Administrative Data Processing,
                  Computer Communication Networks, Computer Science,
                  general, domain decomposition, Information Systems
                  Applications (incl. Internet), Iterative
                  substructuring, Multilevel algorithms, Parallel
                  algorithms, Software Engineering},
  pages =	 {1087--1119},
  file =	 {Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/R3872F64/Sousedík
                  et al. - 2013 - Adaptive-Multilevel BDDC and its
                  parallel
                  implemen.pdf:application/pdf;Snapshot:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/3N5HH2EN/10.html:text/html}
}

@article{sousedik_adaptive-multilevel_2013,
  title =	 {Adaptive-Multilevel {BDDC} and its parallel
                  implementation},
  volume =	 95,
  issn =	 {0010-{485X}, 1436-5057},
  url =
                  {http://link.springer.com/article/10.1007/s00607-013-0293-5},
  doi =		 {10.1007/s00607-013-0293-5},
  abstract =	 {We combine the adaptive and multilevel approaches to
                  the {BDDC} and formulate a method which allows an
                  adaptive selection of constraints on each
                  decomposition level. We also present a strategy for
                  the solution of local eigenvalue problems in the
                  adaptive algorithm using the {LOBPCG} method with a
                  preconditioner based on standard components of the
                  {BDDC.} The effectiveness of the method is
                  illustrated on several engineering problems. It
                  appears that the Adaptive-Multilevel {BDDC}
                  algorithm is able to effectively detect troublesome
                  parts on each decomposition level and improve
                  convergence of the method. The developed open-source
                  parallel implementation shows a good scalability as
                  well as applicability to very large problems and
                  core counts.},
  number =	 12,
  urldate =	 {2014-02-07},
  journal =	 {Computing},
  author =	 {Soused\'ik, Bed\v{r}ich and \v{S}\'istek, Jakub and
                  Mandel, Jan},
  year =	 2013,
  keywords =	 {{65M55}, {65N55}, {65Y05}, Adaptive constraints,
                  Artificial Intelligence (incl. Robotics), {BDDC},
                  Computer Appl. in Administrative Data Processing,
                  Computer Communication Networks, Computer Science,
                  general, domain decomposition, Information Systems
                  Applications (incl. Internet), Iterative
                  substructuring, Multilevel algorithms, Parallel
                  algorithms, Software Engineering},
  pages =	 {1087--1119},
  file =	 {Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/R3872F64/Sousedík
                  et al. - 2013 - Adaptive-Multilevel BDDC and its
                  parallel
                  implemen.pdf:application/pdf;Snapshot:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/3N5HH2EN/10.html:text/html}
}



% Unclassified

@article{speck_space-time_2013,
  title =	 {A space-time parallel solver for the
                  three-dimensional heat equation},
  url =		 {http://arxiv.org/abs/1307.7867},
  doi =		 {10.3233/978-1-61499-381-0-263},
  abstract =	 {The paper presents a combination of the
                  time-parallel "parallel full approximation scheme in
                  space and time" (PFASST) with a parallel multigrid
                  method (PMG) in space, resulting in a mesh-based
                  solver for the three-dimensional heat equation with
                  a uniquely high degree of efficient
                  concurrency. Parallel scaling tests are reported on
                  the Cray XE6 machine "Monte Rosa" on up to 16,384
                  cores and on the IBM Blue Gene/Q system "JUQUEEN" on
                  up to 65,536 cores. The efficacy of the combined
                  spatial- and temporal parallelization is shown by
                  demonstrating that using PFASST in addition to PMG
                  significantly extends the strong-scaling
                  limit. Implications of using spatial coarsening
                  strategies in PFASST's multi-level hierarchy in
                  large-scale parallel simulations are discussed.},
  urldate =	 {2014-10-23},
  journal =	 {arXiv:1307.7867 [cs, math]},
  author =	 {Speck, Robert and Ruprecht, Daniel and Emmett,
                  Matthew and Bolten, Matthias and Krause, Rolf},
  year =	 2013,
  keywords =	 {Computer Science - Distributed, Parallel, and
                  Cluster Computing, Computer Science - Numerical
                  Analysis, Mathematics - Numerical Analysis},
  annote =	 {Comment: 10 pages},
  file =	 {arXiv\:1307.7867
                  PDF:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/XXC4A6WC/Speck
                  et al. - 2013 - A space-time parallel solver for the
                  three-dimensi.pdf:application/pdf;arXiv.org
                  Snapshot:/home/molm/.mozilla/firefox/9e0d2esx.default/zotero/storage/PGMGM5AZ/1307.html:text/html}
}

@article{tapia_review_2014,
  title =	 {A {Review} on {Process} {Monitoring} and {Control}
                  in {Metal}-{Based} {Additive} {Manufacturing}},
  volume =	 136,
  issn =	 {1087-1357},
  url =
                  {http://manufacturingscience.asmedigitalcollection.asme.org/article.aspx?doi=10.1115/1.4028540},
  doi =		 {10.1115/1.4028540},
  language =	 {en},
  number =	 6,
  urldate =	 {2015-12-29},
  journal =	 {Journal of Manufacturing Science and Engineering},
  author =	 {Tapia, Gustavo and Elwany, Alaa},
  year =	 2014,
  pages =	 060801,
  file =
                  {5634527e08aeb786b7014182.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/46PMHCGE/5634527e08aeb786b7014182.pdf:application/pdf}
}

@techreport{the_minerals_metals_&_materials_society_modeling_2015,
  title =	 {Modeling {Across} {Scales}: {A} {Roadmapping}
                  {Study} for {Connecting} {Materials} {Models} and
                  {Simulations} {Across} {Length} and {Time} {Scales}},
  shorttitle =	 {Modeling {Across} {Scales}},
  url =
                  {https://www.tms.org/multiscalestudy/10.7449/multiscale_1},
  urldate =	 {2015-12-29},
  institution =	 {The Minerals, Metals \& Materials Society},
  author =	 {{The Minerals, Metals \& Materials Society}},
  year =	 2015,
  file =
                  {MultiscaleStudy.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/IBB8CWV7/MultiscaleStudy.pdf:application/pdf}
}

@article{thompson_overview_2015,
  title =	 {An overview of Direct Laser Deposition for additive
                  manufacturing; {Part} {I}: {Transport} phenomena,
                  modeling and diagnostics},
  volume =	 8,
  issn =	 {2214-8604},
  shorttitle =	 {An overview of {Direct} {Laser} {Deposition} for
                  additive manufacturing; {Part} {I}},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S2214860415000317},
  doi =		 {10.1016/j.addma.2015.07.001},
  urldate =	 {2016-01-27},
  journal =	 {Additive Manufacturing},
  author =	 {Thompson, Scott M. and Bian, Linkan and Shamsaei,
                  Nima and Yadollahi, Aref},
  year =	 2015,
  keywords =	 {Additive manufacturing (AM), Direct Laser Deposition
                  (DLD), Heat transfer, Melt pool, Thermal monitoring},
  pages =	 {36--62},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/M5TWQWH7/Thompson
                  et al. - 2015 - An overview of Direct Laser
                  Deposition for
                  additiv.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/iqmpmzrw.default/zotero/storage/GQTWIIX7/S2214860415000317.html:text/html}
}

@article{tonks_object-oriented_2012,
  title =	 {An object-oriented finite element framework for
                  multiphysics phase field simulations},
  volume =	 51,
  issn =	 09270256,
  url =
                  {http://linkinghub.elsevier.com/retrieve/pii/S0927025611004204},
  doi =		 {10.1016/j.commatsci.2011.07.028},
  language =	 {en},
  number =	 1,
  urldate =	 {2015-12-29},
  journal =	 {Computational Materials Science},
  author =	 {Tonks, Michael R. and Gaston, Derek and Millett,
                  Paul C. and Andrs, David and Talbot, Paul},
  year =	 2012,
  pages =	 {20--29},
  file =
                  {02e7e51e86c6f1c125000000.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/KA6AES83/02e7e51e86c6f1c125000000.pdf:application/pdf}
}

@ARTICLE{tu_balancing_2008,
  author =	 {Tu, Xuemin and Li, Jing},
  title =	 {A balancing domain decomposition method by
                  constraints for advection-diffusion problems},
  journal =	 {Communications in Applied Mathematics and
                  Computational Science},
  year =	 2008,
  volume =	 3,
  pages =	 {25--60},
  number =	 1,
  doi =		 {10.2140/camcos.2008.3.25},
  file =
                  {camcos-v3-n1-p02-s.pdf:/home/principe/Javier/System/Config/firefox/zotero/storage/DJ5PJP6I/camcos-v3-n1-p02-s.pdf:application/pdf;Communications
                  in Applied Mathematics and Computational Science
                  Vol. 3, No. 1,
                  2008:/home/principe/Javier/System/Config/firefox/zotero/storage/TK4DEJID/p02.html:application/xhtml+xml},
  issn =	 {2157-5452, 1559-3940},
  owner =	 {principe},
  timestamp =	 {2014.03.23},
  url =		 {http://msp.org/camcos/2008/3-1/p02.xhtml},
  urldate =	 {2012-11-09}
}

@ARTICLE{tu_balancing_2008,
  author =	 {Tu, Xuemin and Li, Jing},
  title =	 {A balancing domain decomposition method by
                  constraints for advection-diffusion problems},
  journal =	 {Communications in Applied Mathematics and
                  Computational Science},
  year =	 2008,
  volume =	 3,
  pages =	 {25--60},
  number =	 1,
  doi =		 {10.2140/camcos.2008.3.25},
  file =
                  {camcos-v3-n1-p02-s.pdf:/home/principe/Javier/System/Config/firefox/zotero/storage/DJ5PJP6I/camcos-v3-n1-p02-s.pdf:application/pdf;Communications
                  in Applied Mathematics and Computational Science
                  Vol. 3, No. 1,
                  2008:/home/principe/Javier/System/Config/firefox/zotero/storage/TK4DEJID/p02.html:application/xhtml+xml},
  issn =	 {2157-5452, 1559-3940},
  owner =	 {principe},
  timestamp =	 {2014.03.23},
  url =		 {http://msp.org/camcos/2008/3-1/p02.xhtml},
  urldate =	 {2012-11-09}
}

@ARTICLE{tu_three-level_2007,
  author =	 {Tu, Xuemin},
  title =	 {Three-Level {BDDC} in Three Dimensions},
  journal =	 {{SIAM} Journal on Scientific Computing},
  year =	 2007,
  volume =	 29,
  pages =	 {1759--1780},
  number =	 4,
  doi =		 {10.1137/050629902},
  file =	 {Three-Level BDDC in Three Dimensions : SIAM Journal
                  on Scientific Computing: Vol. 29, No. 4 (Society for
                  Industrial and Applied
                  Mathematics):/Users/sbadia/Work/zotero/storage/JB8983CR/050629902.html:text/html},
  issn =	 {1064-8275, 1095-7197},
  url =		 {http://epubs.siam.org/doi/abs/10.1137/050629902},
  urldate =	 {2013-01-22}
}

@book{verfurth_posteriori_2013,
  title =	 {A Posteriori Error Estimation Techniques for Finite
                  Element Methods},
  isbn =	 9780191668760,
  abstract =	 {Self-adaptive discretization methods are now an
                  indispensable tool for the numerical solution of
                  partial differential equations that arise from
                  physical and technical applications. The aim is to
                  obtain a numerical solution within a prescribed
                  tolerance using a minimal amount of work. The main
                  tools in achieving this goal are a posteriori error
                  estimates which give global and local information on
                  the error of the numerical solution and which can
                  easily be computed from the given numerical solution
                  and the data of the differential equation. This book
                  reviews the most frequently used a posteriori error
                  estimation techniques and applies them to a broad
                  class of linear and nonlinear elliptic and parabolic
                  equations. Although there are various approaches to
                  adaptivity and a posteriori error estimation, they
                  are all based on a few common principles. The main
                  aim of the book is to elaborate these basic
                  principles and to give guidelines for developing
                  adaptive schemes for new problems. Chapters 1 and 2
                  are quite elementary and present various error
                  indicators and their use for mesh adaptation in the
                  framework of a simple model problem. The basic
                  principles are introduced using a minimal amount of
                  notations and techniques providing a complete
                  overview for the non-specialist. Chapters 4-6 on the
                  other hand are more advanced and present a
                  posteriori error estimates within a general
                  framework using the technical tools collected in
                  Chapter 3. Most sections close with a
                  bibliographical remark which indicates the
                  historical development and hints at further
                  results.},
  language =	 {en},
  publisher =	 {Oxford University Press},
  author =	 {Verf\"urth, R\"udiger},
  year =	 2013,
  keywords =	 {Mathematics / Applied, Mathematics / Numerical
                  Analysis}
}

@book{verfurth_posteriori_2013,
  title =	 {A Posteriori Error Estimation Techniques for Finite
                  Element Methods},
  isbn =	 9780191668760,
  abstract =	 {Self-adaptive discretization methods are now an
                  indispensable tool for the numerical solution of
                  partial differential equations that arise from
                  physical and technical applications. The aim is to
                  obtain a numerical solution within a prescribed
                  tolerance using a minimal amount of work. The main
                  tools in achieving this goal are a posteriori error
                  estimates which give global and local information on
                  the error of the numerical solution and which can
                  easily be computed from the given numerical solution
                  and the data of the differential equation. This book
                  reviews the most frequently used a posteriori error
                  estimation techniques and applies them to a broad
                  class of linear and nonlinear elliptic and parabolic
                  equations. Although there are various approaches to
                  adaptivity and a posteriori error estimation, they
                  are all based on a few common principles. The main
                  aim of the book is to elaborate these basic
                  principles and to give guidelines for developing
                  adaptive schemes for new problems. Chapters 1 and 2
                  are quite elementary and present various error
                  indicators and their use for mesh adaptation in the
                  framework of a simple model problem. The basic
                  principles are introduced using a minimal amount of
                  notations and techniques providing a complete
                  overview for the non-specialist. Chapters 4-6 on the
                  other hand are more advanced and present a
                  posteriori error estimates within a general
                  framework using the technical tools collected in
                  Chapter 3. Most sections close with a
                  bibliographical remark which indicates the
                  historical development and hints at further
                  results.},
  language =	 {en},
  publisher =	 {Oxford University Press},
  author =	 {Verfürth, Rüdiger},
  year =	 2013,
  keywords =	 {Mathematics / Applied, Mathematics / Numerical
                  Analysis}
}

@inproceedings{veroy_posteriori_2003,
  title =	 {A posteriori error bounds for reduced-basis
                  approximation of parametrized noncoercive and
                  nonlinear elliptic partial differential equations},
  volume =	 3847,
  url =		 {http://arc.aiaa.org/doi/pdf/10.2514/6.2003-3847},
  urldate =	 {2016-01-15},
  booktitle =	 {Proceedings of the 16th {AIAA} computational fluid
                  dynamics conference},
  author =	 {Veroy, Karen and Prud'homme, Christophe and Rovas,
                  Dimitrios V. and Patera, Anthony T.},
  year =	 2003,
  pages =	 {23--26},
  file =
                  {Veroy_etal_2003_AIAA.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/XS43SWKR/Veroy_etal_2003_AIAA.pdf:application/pdf}
}

@article{wang_high-order_2013,
  title =	 {High-order {CFD} methods: current status and
                  perspective},
  volume =	 72,
  issn =	 02712091,
  shorttitle =	 {High-order {CFD} methods},
  url =		 {zotero://attachment/2822/},
  doi =		 {10.1002/fld.3767},
  language =	 {en},
  number =	 8,
  urldate =	 {2014-03-14},
  journal =	 {International Journal for Numerical Methods in
                  Fluids},
  author =	 {Wang, {Z.J.} and Fidkowski, Krzysztof and Abgrall,
                  R\'emi and Bassi, Francesco and Caraeni, Doru and
                  Cary, Andrew and Deconinck, Herman and Hartmann,
                  Ralf and Hillewaert, Koen and Huynh, {H.T.} and
                  Kroll, Norbert and May, Georg and Persson, P-O and
                  van Leer, Bram and Visbal, Miguel},
  year =	 2013,
  pages =	 {811--845},
  file =
                  {fld3767.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/ICC6DB4D/fld3767.pdf:application/pdf;High-order
                  CFD methods\: current status and perspective - Wang
                  - 2013 - International Journal for Numerical Methods
                  in Fluids - Wiley Online
                  Library:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/39SFNT6Z/2822.html:text/html}
}

%Weld pool

@article{wiesner_multigrid_2013,
  title =	 {Multigrid transfers for nonsymmetric systems based
                  on Schur complements and {G}alerkin projections},
  copyright =	 {Copyright © 2013 John Wiley \& Sons, Ltd.},
  issn =	 {1099-1506},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nla.1889/abstract},
  doi =		 {10.1002/nla.1889},
  abstract =	 {A framework is proposed for constructing algebraic
                  multigrid transfer operators suitable for
                  nonsymmetric positive definite linear systems. This
                  framework follows a Schur complement perspective as
                  this is suitable for both symmetric and nonsymmetric
                  systems. In particular, a connection between
                  algebraic multigrid and approximate block
                  factorizations is explored. This connection
                  demonstrates that the convergence rate of a
                  two-level model multigrid iteration is completely
                  governed by how well the coarse discretization
                  approximates a Schur complement operator. The new
                  grid transfer algorithm is then based on computing a
                  Schur complement but restricting the solution space
                  of the corresponding grid transfers in a
                  Galerkin-style so that a far less expensive
                  approximation is obtained. The final algorithm
                  corresponds to a Richardson-type iteration that is
                  used to improve a simple initial prolongator or a
                  simple initial restrictor. Numerical results are
                  presented illustrating the performance of the
                  resulting algebraic multigrid method on highly
                  nonsymmetric systems. Copyright © 2013 John Wiley \&
                  Sons, Ltd.},
  language =	 {en},
  urldate =	 {2014-02-05},
  journal =	 {Numerical Linear Algebra with Applications},
  author =	 {Wiesner, T. A. and Tuminaro, R. S. and Wall,
                  W. A. and Gee, M. W.},
  year =	 2014,
  volume =	 21,
  number =	 3,
  keywords =	 {algebraic multigrid, Galerkin projection,
                  nonsymmetric problems, Schur complement},
  pages =	 {415--438},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/JFW3PWBM/Wiesner
                  et al. - 2013 - Multigrid transfers for nonsymmetric
                  systems
                  based.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/BTKXKGDN/abstract.html:text/html}
}


@article{wiesner_multigrid_2013,
  title =	 {Multigrid transfers for nonsymmetric systems based
                  on Schur complements and {G}alerkin projections},
  copyright =	 {Copyright © 2013 John Wiley \& Sons, Ltd.},
  issn =	 {1099-1506},
  url =
                  {http://onlinelibrary.wiley.com/doi/10.1002/nla.1889/abstract},
  doi =		 {10.1002/nla.1889},
  abstract =	 {A framework is proposed for constructing algebraic
                  multigrid transfer operators suitable for
                  nonsymmetric positive definite linear systems. This
                  framework follows a Schur complement perspective as
                  this is suitable for both symmetric and nonsymmetric
                  systems. In particular, a connection between
                  algebraic multigrid and approximate block
                  factorizations is explored. This connection
                  demonstrates that the convergence rate of a
                  two-level model multigrid iteration is completely
                  governed by how well the coarse discretization
                  approximates a Schur complement operator. The new
                  grid transfer algorithm is then based on computing a
                  Schur complement but restricting the solution space
                  of the corresponding grid transfers in a
                  Galerkin-style so that a far less expensive
                  approximation is obtained. The final algorithm
                  corresponds to a Richardson-type iteration that is
                  used to improve a simple initial prolongator or a
                  simple initial restrictor. Numerical results are
                  presented illustrating the performance of the
                  resulting algebraic multigrid method on highly
                  nonsymmetric systems. Copyright © 2013 John Wiley \&
                  Sons, Ltd.},
  language =	 {en},
  urldate =	 {2014-02-05},
  journal =	 {Numerical Linear Algebra with Applications},
  author =	 {Wiesner, T. A. and Tuminaro, R. S. and Wall,
                  W. A. and Gee, M. W.},
  year =	 2013,
  keywords =	 {algebraic multigrid, Galerkin projection,
                  nonsymmetric problems, Schur complement},
  pages =	 {n/a--n/a},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/JFW3PWBM/Wiesner
                  et al. - 2013 - Multigrid transfers for nonsymmetric
                  systems
                  based.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/BTKXKGDN/abstract.html:text/html}
}

@unpublished{wropp_2015_mpi_x_keynote,
title= {Is {MPI}+{X} {E}nough for {E}xascale?},
author = {William Gropp},
year = {2015},
note= {Keynote for International High Performance Computing Forum, Tianjin, China},
URL= {http://wgropp.cs.illinois.edu/bib/talks/tdata/2015/Gropp-IHPCF-keynote.pdf},
}

@article{xia_reduced_2014,
  title =	 {A reduced multiscale model for nonlinear structural
                  topology optimization},
  volume =	 280,
  issn =	 {0045-7825},
  url =
                  {http://www.sciencedirect.com/science/article/pii/S0045782514002564},
  doi =		 {10.1016/j.cma.2014.07.024},
  abstract =	 {This paper presents a reduced multiscale model for
                  macroscopic structural design considering
                  microscopic material nonlinear microstructures. This
                  work introduces Reduced Order Model (ROM) to
                  alleviate the heavy computational demand of
                  nonlinear nested multiscale procedures, particularly
                  in an optimization framework which requires multiple
                  loops involving similar computations. The surrogate
                  model constructed using Proper Orthogonal
                  Decomposition (POD) and Diffuse Approximation
                  reduces the computational effort for solving the
                  microscopic boundary value problems. Multiscale
                  analysis model (FE2) is applied to link structure
                  and microstructures in the two scales. Maximum
                  stiffness design of the macroscopic structure is
                  realized using a discrete level-set topology
                  optimization model. It is shown by means of
                  numerical tests that the reduced multiscale model
                  provides reasonable designs as compared to those
                  obtained by the unreduced model while with a
                  significantly reduced computational effort.},
  urldate =	 {2016-01-15},
  journal =	 {Computer Methods in Applied Mechanics and
                  Engineering},
  author =	 {Xia, Liang and Breitkopf, Piotr},
  year =	 2014,
  keywords =	 {Diffuse Approximation, Model reduction, Multiscale
                  analysis, parallel computing, Proper Orthogonal
                  Decomposition, Topology optimization},
  pages =	 {117--134},
  file =	 {ScienceDirect Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/E24W3KWD/Xia
                  and Breitkopf - 2014 - A reduced multiscale model
                  for nonlinear
                  structura.pdf:application/pdf;ScienceDirect
                  Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/GC9E2BPI/S0045782514002564.html:text/html}
}

@article{yan_multiscale_2015,
  title =	 {Multiscale modeling of electron beam and substrate
                  interaction: a new heat source model},
  volume =	 56,
  issn =	 {0178-7675, 1432-0924},
  shorttitle =	 {Multiscale modeling of electron beam and substrate
                  interaction},
  url =
                  {http://link.springer.com/article/10.1007/s00466-015-1170-1},
  doi =		 {10.1007/s00466-015-1170-1},
  abstract =	 {An electron beam is a widely applied processing tool
                  in welding and additive manufacturing
                  applications. The heat source model of the electron
                  beam acts as the basis of thermal simulations and
                  predictions of the micro-structures and mechanical
                  properties of the final products. While traditional
                  volumetric and surface heat flux models were
                  developed previously based on the observed shape of
                  the molten pool produced by the beam, a new heat
                  source model with a physically informed foundation
                  has been established in this work. The new model was
                  developed based on Monte Carlo simulations performed
                  to obtain the distribution of absorbed energy
                  through electron-atom collisions for an electron
                  beam with a kinetic energy of 60 keV hitting a
                  Ti-6Al-4V substrate. Thermal simulations of a moving
                  electron beam heating a solid baseboard were
                  conducted to compare the differences between the new
                  heat source model, the traditional surface flux
                  model and the volumetric flux model. Although the
                  molten pool shapes with the three selected models
                  were found to be similar, the predicted peak
                  temperatures were noticeably different, which will
                  influence the evaporation, recoil pressure and
                  molten pool dynamics. The new heat source model was
                  also used to investigate the influence of a static
                  electron beam on a substrate. This investigation
                  indicated that the new heat source model could
                  scientifically explain phenomena that the surface
                  and volumetric models cannot, such as eruption and
                  explosion during electron beam processing.},
  language =	 {en},
  number =	 2,
  urldate =	 {2016-01-15},
  journal =	 {Computational Mechanics},
  author =	 {Yan, Wentao and Smith, Jacob and Ge, Wenjun and Lin,
                  Feng and Liu, Wing Kam},
  year =	 2015,
  keywords =	 {additive manufacturing, Classical Continuum Physics,
                  Computational Science and Engineering, Electron
                  beam, Finite element, Heat source model, Monte Carlo
                  simulation, Multiscale modeling, Theoretical and
                  Applied Mechanics},
  pages =	 {265--276},
  file =	 {Full Text
                  PDF:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/8K8UPICX/Yan
                  et al. - 2015 - Multiscale modeling of electron beam
                  and
                  substrate.pdf:application/pdf;Snapshot:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/PU6J62AD/10.html:text/html}
}

@INCOLLECTION{yang_parallel_2006,
  author =	 {Yang, Ulrike Meier},
  title =	 {Parallel Algebraic Multigrid Methods â High
                  Performance Preconditioners},
  booktitle =	 {Numerical Solution of Partial Differential Equations
                  on Parallel Computers},
  publisher =	 {Springer Berlin Heidelberg},
  year =	 2006,
  editor =	 {Bruaset, Are Magnus and Tveito, Aslak},
  number =	 51,
  series =	 {Lecture Notes in Computational Science and
                  Engineering},
  pages =	 {209--236},
  abstract =	 {The development of high performance, massively
                  parallel computers and the increasing demands of
                  computationally challenging applications have
                  necessitated the development of scalable solvers and
                  preconditioners.  One of the most effective ways to
                  achieve scalability is the use of multigrid or
                  multilevel techniques. Algebraic multigrid ({AMG)}
                  is a very efficient algorithm for solving large
                  problems on unstructured grids. While much of it can
                  be parallelized in a straightforward way, some
                  components of the classical algorithm, particularly
                  the coarsening process and some of the most
                  efficient smoothers, are highly sequential, and
                  require new parallel approaches. This chapter
                  presents the basic principles of {AMG} and gives an
                  overview of various parallel implementations of
                  {AMG}, including descriptions of parallel coarsening
                  schemes and smoothers, some numerical results as
                  well as references to existing software packages.},
  copyright =	 {Â©2006 Springer-Verlag Berlin Heidelberg},
  file =	 {Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/M9HVHJEZ/Yang
                  - 2006 - Parallel Algebraic Multigrid Methods
                  â High
                  Perfor.pdf:application/pdf;Snapshot:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/32DK2JXM/10.html:text/html},
  isbn =	 {978-3-540-29076-6, 978-3-540-31619-0},
  keywords =	 {{Appl.Mathematics/Computational} Methods of
                  Engineering, Computational Mathematics and Numerical
                  Analysis, Computational Science and Engineering,
                  Mathematical and Computational Physics, Mathematics
                  of Computing, Partial Differential Equations},
  owner =	 {principe},
  timestamp =	 {2014.03.21},
  url =
                  {http://link.springer.com/chapter/10.1007/3-540-31619-1_6},
  urldate =	 {2014-02-25}
}

@INCOLLECTION{yang_parallel_2006,
  author =	 {Yang, Ulrike Meier},
  title =	 {Parallel Algebraic Multigrid Methods â High
                  Performance Preconditioners},
  booktitle =	 {Numerical Solution of Partial Differential Equations
                  on Parallel Computers},
  publisher =	 {Springer Berlin Heidelberg},
  year =	 2006,
  editor =	 {Bruaset, Are Magnus and Tveito, Aslak},
  number =	 51,
  series =	 {Lecture Notes in Computational Science and
                  Engineering},
  pages =	 {209--236},
  abstract =	 {The development of high performance, massively
                  parallel computers and the increasing demands of
                  computationally challenging applications have
                  necessitated the development of scalable solvers and
                  preconditioners.  One of the most effective ways to
                  achieve scalability is the use of multigrid or
                  multilevel techniques. Algebraic multigrid ({AMG)}
                  is a very efficient algorithm for solving large
                  problems on unstructured grids. While much of it can
                  be parallelized in a straightforward way, some
                  components of the classical algorithm, particularly
                  the coarsening process and some of the most
                  efficient smoothers, are highly sequential, and
                  require new parallel approaches. This chapter
                  presents the basic principles of {AMG} and gives an
                  overview of various parallel implementations of
                  {AMG}, including descriptions of parallel coarsening
                  schemes and smoothers, some numerical results as
                  well as references to existing software packages.},
  copyright =	 {Â©2006 Springer-Verlag Berlin Heidelberg},
  file =	 {Full Text
                  PDF:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/M9HVHJEZ/Yang
                  - 2006 - Parallel Algebraic Multigrid Methods
                  â High
                  Perfor.pdf:application/pdf;Snapshot:/home/amartin/.mozilla/firefox/beo2tbos.default/zotero/storage/32DK2JXM/10.html:text/html},
  isbn =	 {978-3-540-29076-6, 978-3-540-31619-0},
  keywords =	 {{Appl.Mathematics/Computational} Methods of
                  Engineering, Computational Mathematics and Numerical
                  Analysis, Computational Science and Engineering,
                  Mathematical and Computational Physics, Mathematics
                  of Computing, Partial Differential Equations},
  owner =	 {principe},
  timestamp =	 {2014.03.21},
  url =
                  {http://link.springer.com/chapter/10.1007/3-540-31619-1_6},
  urldate =	 {2014-02-25}
}

@article{yang_pseudo-compressible_2016,
	title = {A pseudo-compressible variational multiscale solver for turbulent incompressible flows},
	volume = {58},
	issn = {0178-7675, 1432-0924},
	url = {http://link.springer.com.recursos.biblioteca.upc.edu/article/10.1007/s00466-016-1332-9},
	doi = {10.1007/s00466-016-1332-9},
	abstract = {In this work, we design an explicit time-stepping solver for the simulation of the incompressible turbulent flow through the combination of {VMS} methods and artificial compressibility. We evaluate the effect of the artificial compressibility on the accuracy of the explicit formulation for under-resolved {LES} simulations. A set of benchmarks have been solved, e.g., the 3D Taylor–Green vortex problem in turbulent regimes. The resulting method is proven to be an effective alternative to implicit methods in some application ranges (in terms of problem size and computational resources), providing comparable results with very low memory requirements. As an example, with the explicit approach, we are able to solve accurately the Taylor-Green vortex benchmark in a fine mesh with 5123512{\textasciicircum}3 cells on a 12 cores 64 {GB} ram machine.},
	pages = {1051--1069},
	number = {6},
	journal = {Computational Mechanics},
	journaltitle = {Computational Mechanics},
	shortjournal = {Comput Mech},
	author = {Yang, Liang and Badia, Santiago and Codina, Ramon},
	urldate = {2017-01-10},
	date = {2016-12-01},
        year = {2016},
	langid = {english},
	file = {Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/JX6GRCPE/Yang et al. - 2016 - A pseudo-compressible variational multiscale solve.pdf:application/pdf;Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/BUN76HJB/s00466-016-1332-9.html:text/html}
}

@techreport{zohdi_recent_????,
  title =	 {Recent {Trends} in {Mechanics}: {Future} {Synergy}
                  between {Computational} {Mechanics} and {Advanced}
                  {Additive} {Manufacturing}},
  shorttitle =	 {Recent {Trends} in {Mechanics}},
  url =
                  {http://sites.nationalacademies.org/cs/groups/pgasite/documents/webpage/pga_166813.pdf},
  year =	 2015,
  institution =	 { {US} {National} {Committee} for {Theoretical} and
                  {Applied} {Mechanics}},
  urldate =	 {2015-12-29},
  author =	 {Zohdi, Tarek I. and Dornfeld, David A.},
  file =
                  {pga_166813.pdf:/home/sbadia/.mozilla/firefox/38g6z3d4.default/zotero/storage/RKBBUWRC/pga_166813.pdf:application/pdf}
}




@article{moore_crystal_2017,
	title = {Crystal plasticity modeling of β phase deformation in {Ti}-6Al-4V},
	volume = {25},
	issn = {0965-0393},
	url = {http://stacks.iop.org/0965-0393/25/i=7/a=075007},
	doi = {10.1088/1361-651X/aa841c},
	abstract = {Ti-6Al-4V is an alloy of titanium that dominates titanium usage in applications ranging from mass-produced consumer goods to high-end aerospace parts. The material’s structure on a microscale is known to affect its mechanical properties but these effects are not fully understood. Specifically, this work will address the effects of low volume fraction intergranular β phase on Ti-6Al-4V’s mechanical response during the transition from elastic to plastic deformation. A crystal plasticity-based finite element model is used to fully resolve the deformation of the β phase for the first time. This high fidelity model captures mechanisms difficult to access via experiments or lower fidelity models. The results are used to assess lower fidelity modeling assumptions and identify phenomena that have ramifications for failure of the material.},
	language = {en},
	number = {7},
	urldate = {2018-01-23},
	journal = {Modelling and Simulation in Materials Science and Engineering},
	author = {Moore, John A. and Barton, Nathan R. and Florando, Jeff and Mulay, Rupalee and Kumar, Mukul},
	year = {2017},
	pages = {075007},
	file = {10.1088@1361-651X@aa841c.pdf:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/WNZBV3YX/10.1088@1361-651X@aa841c.pdf:application/pdf}
}

@article{ghorbanpour_crystal_2017,
	title = {A crystal plasticity model incorporating the effects of precipitates in superalloys: {Application} to tensile, compressive, and cyclic deformation of {Inconel} 718},
	volume = {99},
	issn = {0749-6419},
	shorttitle = {A crystal plasticity model incorporating the effects of precipitates in superalloys},
	url = {http://www.sciencedirect.com/science/article/pii/S0749641917303182},
	doi = {10.1016/j.ijplas.2017.09.006},
	abstract = {An elasto-plastic polycrystal plasticity model is developed and applied to an Inconel 718 (IN718) superalloy that was produced by additive manufacturing (AM). The model takes into account the contributions of solid solution, precipitates shearing, and grain size and shape effects into the initial slip resistance. Non-Schmid effects and backstress are also included in the crystal plasticity model for activating slip. The hardening law for the critical resolved shear stress is based on the evolution of dislocation density. Using the same set of material and physical parameters, the model is compared against a suite of compression, tension, and large-strain cyclic mechanical test data applied in different AM build directions. It is demonstrated that the model is capable of predicting the particularities of both monotonic and cyclic deformation to large strains of the alloy, including decreasing hardening rate during monotonic loading, the non-linear unloading upon the load reversal, the Bauschinger effect, the hardening rate change during loading in the reverse direction as well as plastic anisotropy and the concomitant microstructure evolution. It is anticipated that the general model developed here can be applied to other multiphase alloys containing precipitates.},
	urldate = {2018-01-23},
	journal = {International Journal of Plasticity},
	author = {Ghorbanpour, Saeede and Zecevic, Milovan and Kumar, Anil and Jahedi, Mohammad and Bicknell, Jonathan and Jorgensen, Luke and Beyerlein, Irene J. and Knezevic, Marko},
	year = {2017},
	keywords = {Inconel 718, Crystal plasticity, Microstructures, Numerical algorithms, Polycrystalline material},
	pages = {162--185},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/9Q38NNS8/Ghorbanpour et al. - 2017 - A crystal plasticity model incorporating the effec.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/KY4SR756/S0749641917303182.html:text/html}
}

@article{ghosh_primary_2017,
	title = {On the primary spacing and microsegregation of cellular dendrites in laser deposited {Ni}-{Nb} alloys},
	volume = {25},
	issn = {0965-0393, 1361-651X},
	url = {http://arxiv.org/abs/1705.06669},
	doi = {10.1088/1361-651X/aa7369},
	abstract = {In this study, an alloy phase-field model is used to simulate solidification microstructures at different locations within a solidified molten pool. The temperature gradient \$G\$ and the solidification velocity \$V\$ are obtained from a macroscopic heat transfer finite element simulation and provided as input to the phase-field model. The effects of laser beam speed and the location within the melt pool on the primary arm spacing and on the extent of Nb partitioning at the cell tips are investigated. Simulated steady-state primary spacings are compared with power law and geometrical models. Cell tip compositions are compared to a dendrite growth model. The extent of non-equilibrium interface partitioning of the phase-field model is investigated. Although the phase-field model has an anti-trapping solute flux term meant to maintain local interface equilibrium, we have found that during simulations it was insufficient at maintaining equilibrium. This is due to the fact that the additive manufacturing solidification conditions fall well outside the allowed limits of this flux term.},
	number = {6},
	urldate = {2018-01-23},
	journal = {Modelling and Simulation in Materials Science and Engineering},
	author = {Ghosh, Supriyo and Ma, Li and Ofori-Opoku, Nana and Guyer, Jonathan E.},
	year = {2017},
	note = {arXiv: 1705.06669},
	keywords = {Condensed Matter - Materials Science},
	pages = {065002},
	file = {arXiv\:1705.06669 PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/D59PMDZM/Ghosh et al. - 2017 - On the primary spacing and microsegregation of cel.pdf:application/pdf;arXiv.org Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/YZ6FJYQV/1705.html:text/html}
}

@article{kergasner_modelling_2016,
	title = {Modelling additive manufactured materials using a crystal plasticity model},
	volume = {16},
	issn = {1617-7061},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/pamm.201610166/abstract},
	doi = {10.1002/pamm.201610166},
	abstract = {In this contribution the macroscopic behaviour of additively manufactured Inconel 718 will be identified using a crystal plasticity model on the meso-level. The experimentally observed grain structures will be used to generate a representative volume element. Based on this RVE macroscopic mechanical parameters will be identified and compared with experimental results. (© 2016 Wiley-VCH Verlag GmbH \& Co. KGaA, Weinheim)},
	language = {en},
	number = {1},
	urldate = {2018-01-23},
	journal = {PAMM},
	author = {Kergaßner, Andreas and Mergheim, Julia and Steinmann, Paul},
	year = {2016},
	pages = {355--356},
	file = {Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/KTF38Z29/Kergaßner et al. - 2016 - Modelling additive manufactured materials using a .pdf:application/pdf;Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/RNL2IDD6/abstract.html:text/html}
}

@article{ghosh_predictive_2018,
	title = {Predictive modeling of solidification during laser additive manufacturing of nickel superalloys: {Recent} developments, future directions},
	volume = {5},
	issn = {2053-1591},
	shorttitle = {Predictive modeling of solidification during laser additive manufacturing of nickel superalloys},
	url = {http://arxiv.org/abs/1707.09292},
	doi = {10.1088/2053-1591/aaa04c},
	abstract = {Additive manufacturing (AM) processes produce parts with improved physical, chemical, and mechanical properties compared to conventional manufacturing processes. In AM processes, intricate part geometries are produced from multicomponent alloy powder, in a layer-by-layer fashion with multipass laser melting, solidification, and solid-state phase transformations, in a shorter manufacturing time, with minimal surface finishing, and at a reasonable cost. However, there is an increasing need for post-processing of the manufactured parts via, for example, stress relieving heat treatment and hot isostatic pressing to achieve homogeneous microstructure and properties at all times. Solidification in an AM process controls the size, shape, and distribution of the grains, the growth morphology, the elemental segregation and precipitation, the subsequent solid-state phase changes, and ultimately the material properties. The critical issues in this process are linked with multiphysics (such as fluid flow and diffusion of heat and mass) and multiscale (lengths, times and temperature ranges) challenges that arise due to localized rapid heating and cooling during AM processing. The alloy chemistry-process-microstructure-property-performance correlation in this process will be increasingly better understood through multiscale modeling and simulation.},
	number = {1},
	urldate = {2018-01-23},
	journal = {Materials Research Express},
	author = {Ghosh, Supriyo},
	year = {2018},
	note = {arXiv: 1707.09292},
	keywords = {Condensed Matter - Materials Science},
	pages = {012001},
	annote = {Comment: Accepted for Publication in Materials Research Express on December 8, 2017},
	file = {arXiv\:1707.09292 PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/UW62B3H3/Ghosh - 2018 - Predictive modeling of solidification during laser.pdf:application/pdf;arXiv.org Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/7WE8PIMI/1707.html:text/html}
}

@article{nandy_modeling_2017,
	title = {Modeling of microstructure evolution in direct metal laser sintering: {A} phase field approach},
	volume = {178},
	issn = {1757-8981, 1757-899X},
	shorttitle = {Modeling of microstructure evolution in direct metal laser sintering},
	url = {http://stacks.iop.org/1757-899X/178/i=1/a=012028?key=crossref.4f04f686b0ca4a7b0dbe5ef7120c1c8a},
	doi = {10.1088/1757-899X/178/1/012028},
	urldate = {2018-01-23},
	journal = {IOP Conference Series: Materials Science and Engineering},
	author = {Nandy, Jyotirmoy and Sarangi, Hrushikesh and Sahoo, Seshadev},
	year = {2017},
	pages = {012028},
	file = {pdf.pdf:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/K222EIFY/pdf.pdf:application/pdf}
}

@article{keller_application_2017,
	title = {Application of {Finite} {Element}, {Phase}-field, and {CALPHAD}-based {Methods} to {Additive} {Manufacturing} of {Ni}-based {Superalloys}},
	volume = {139},
	issn = {13596454},
	url = {http://arxiv.org/abs/1705.02016},
	doi = {10.1016/j.actamat.2017.05.003},
	abstract = {Numerical simulations are used in this work to investigate aspects of microstructure and microsegregation during rapid solidification of a Ni-based superalloy in a laser powder bed fusion additive manufacturing process. Thermal modeling by finite element analysis simulates the laser melt pool, with surface temperatures in agreement with in situ thermographic measurements on Inconel 625. Geometric and thermal features of the simulated melt pools are extracted and used in subsequent mesoscale simulations. Solidification in the melt pool is simulated on two length scales. For the multicomponent alloy Inconel 625, microsegregation between dendrite arms is calculated using the Scheil-Gulliver solidification model and DICTRA software. Phase-field simulations, using Ni-Nb as a binary analogue to Inconel 625, produced microstructures with primary cellular/dendritic arm spacings in agreement with measured spacings in experimentally observed microstructures and a lesser extent of microsegregation than predicted by DICTRA simulations. The composition profiles are used to compare thermodynamic driving forces for nucleation against experimentally observed precipitates identified by electron and X-ray diffraction analyses. Our analysis lists the precipitates that may form from FCC phase of enriched interdendritic compositions and compares these against experimentally observed phases from 1 h heat treatments at two temperatures: stress relief at 1143 K (870\{{\textbackslash}deg\}C) or homogenization at 1423 K (1150\{{\textbackslash}deg\}C).},
	urldate = {2018-01-23},
	journal = {Acta Materialia},
	author = {Keller, Trevor and Lindwall, Greta and Ghosh, Supriyo and Ma, Li and Lane, Brandon M. and Zhang, Fan and Kattner, Ursula R. and Lass, Eric A. and Heigel, Jarred C. and Idell, Yaakov and Williams, Maureen E. and Allen, Andrew J. and Guyer, Jonathan E. and Levine, Lyle E.},
	year = {2017},
	note = {arXiv: 1705.02016},
	keywords = {Condensed Matter - Materials Science},
	pages = {244--253},
	annote = {Comment: Accepted for publication in Acta Materialia on May 1, 2017},
	file = {arXiv\:1705.02016 PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/92NPE8DE/Keller et al. - 2017 - Application of Finite Element, Phase-field, and CA.pdf:application/pdf;arXiv.org Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/5D9FWEBQ/1705.html:text/html}
}

@article{smith_thermodynamically_2016,
	title = {Thermodynamically consistent microstructure prediction of additively manufactured materials},
	volume = {57},
	issn = {0178-7675, 1432-0924},
	url = {https://link.springer.com/article/10.1007/s00466-015-1243-1},
	doi = {10.1007/s00466-015-1243-1},
	abstract = {Additive manufacturing has risen to the top of research interest in advanced manufacturing in recent years due to process flexibility, achievability of geometric complexity, and the ability to locally modify and optimize materials. The present work is focused on providing an approach for incorporating thermodynamically consistent properties and microstructure evolution for non-equilibrium supercooling, as observed in additive manufacturing processes, into finite element analysis. There are two primary benefits of this work: (1) the resulting prediction is based on the material composition and (2) the nonlinear behavior caused by the thermodynamic properties of the material during the non-equilibrium solution is accounted for with extremely high resolution. The predicted temperature response and microstructure evolution for additively manufactured stainless steel 316L using standard handbook-obtained thermodynamic properties are compared with the thermodynamic properties calculated using the CALculation of PHAse Diagrams (CALPHAD) approach. Data transfer from the CALPHAD approach to finite element analysis is discussed.},
	language = {en},
	number = {3},
	urldate = {2018-01-24},
	journal = {Computational Mechanics},
	author = {Smith, Jacob and Xiong, Wei and Cao, Jian and Liu, Wing Kam},
	year = {2016},
	pages = {359--370},
	file = {Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/7VWEQJDN/Smith et al. - 2016 - Thermodynamically consistent microstructure predic.pdf:application/pdf;Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/9VZLTW6W/s00466-015-1243-1.html:text/html}
}

@article{anderson_phase-field_2000,
	title = {A phase-field model of solidification with convection},
	volume = {135},
	issn = {0167-2789},
	url = {http://www.sciencedirect.com/science/article/pii/S0167278999001098},
	doi = {10.1016/S0167-2789(99)00109-8},
	abstract = {We develop a phase-field model for the solidification of a pure material that includes convection in the liquid phase. The model permits the interface to have an anisotropic surface energy, and allows a quasi-incompressible thermodynamic description in which the densities in the solid and liquid phases may each be uniform. The solid phase is modeled as an extremely viscous liquid, and the formalism of irreversible thermodynamics is employed to derive the governing equations. We investigate the behavior of our model in two important simple situations corresponding to the solidification of a planar interface at constant velocity: density change flow and a shear flow. In the former case we obtain a non-equilibrium form of the Clausius–Clapeyron equation and investigate its behavior by both a direct numerical integration of the governing equations, and an asymptotic analysis corresponding to a small density difference between the two phases. In the case of a parallel shear flow we are able to obtain an exact solution which allows us to investigate its behavior in the sharp interface limit, and for large values of the viscosity ratio.},
	number = {1},
	urldate = {2018-01-25},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Anderson, D. M. and McFadden, G. B. and Wheeler, A. A.},
	year = {2000},
	keywords = {Convection, Diffuse interface, Phase-field, Solidification},
	pages = {175--194},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/Y5GD9UM7/Anderson et al. - 2000 - A phase-field model of solidification with convect.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/I6WNQPXM/S0167278999001098.html:text/html}
}

@article{ghosh_predictive_2018-1,
	title = {Predictive modeling of solidification during laser additive manufacturing of nickel superalloys: {Recent} developments, future directions},
	volume = {5},
	issn = {2053-1591},
	shorttitle = {Predictive modeling of solidification during laser additive manufacturing of nickel superalloys},
	url = {http://arxiv.org/abs/1707.09292},
	doi = {10.1088/2053-1591/aaa04c},
	abstract = {Additive manufacturing (AM) processes produce parts with improved physical, chemical, and mechanical properties compared to conventional manufacturing processes. In AM processes, intricate part geometries are produced from multicomponent alloy powder, in a layer-by-layer fashion with multipass laser melting, solidification, and solid-state phase transformations, in a shorter manufacturing time, with minimal surface finishing, and at a reasonable cost. However, there is an increasing need for post-processing of the manufactured parts via, for example, stress relieving heat treatment and hot isostatic pressing to achieve homogeneous microstructure and properties at all times. Solidification in an AM process controls the size, shape, and distribution of the grains, the growth morphology, the elemental segregation and precipitation, the subsequent solid-state phase changes, and ultimately the material properties. The critical issues in this process are linked with multiphysics (such as fluid flow and diffusion of heat and mass) and multiscale (lengths, times and temperature ranges) challenges that arise due to localized rapid heating and cooling during AM processing. The alloy chemistry-process-microstructure-property-performance correlation in this process will be increasingly better understood through multiscale modeling and simulation.},
	number = {1},
	urldate = {2018-01-26},
	journal = {Materials Research Express},
	author = {Ghosh, Supriyo},
	year = {2018},
	note = {arXiv: 1707.09292},
	keywords = {Condensed Matter - Materials Science},
	pages = {012001},
	annote = {Comment: Accepted for Publication in Materials Research Express on December 8, 2017},
	file = {arXiv\:1707.09292 PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/FPBGWRC3/Ghosh - 2018 - Predictive modeling of solidification during laser.pdf:application/pdf;arXiv.org Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/5FHPIK2J/1707.html:text/html}
}

@article{uehara_phase_2008,
	series = {the {Proceedings} of the 15th {International} {Conference} on {Crystal} {Growth} ({ICCG}-15) in conjunction with the {International} {Conference} on {Vapor} {Growth} and {Epitaxy} and the {US} {Biennial} {Workshop} on {Organometallic} {Vapor} {Phase} {Epitaxy}},
	title = {Phase field simulations of stress distributions in solidification structures},
	volume = {310},
	issn = {0022-0248},
	url = {http://www.sciencedirect.com/science/article/pii/S0022024807012109},
	doi = {10.1016/j.jcrysgro.2007.12.035},
	abstract = {Phase field simulations of stress evolution during solidification processes are carried out, resulting in the generation of complicated stress distributions in solidification microstructures. The governing equations, which include the coupling relations among phase transformation, temperature and stress/strain, are numerically solved using the two-dimensional finite element method. To avoid complexity, mechanical properties of the liquid are applied under a simplified assumption that sufficiently small elastic constants can approximate the liquid behavior. A dendritic microstructure is considered in this study as a typical solidification microstructure. As a result, a complicated stress distribution in a dendrite is obtained. Especially, strong stresses are revealed to distribute at the bottom of sidebranches of a dendrite. Subsequently, a microstructure constructed with two dendrites is simulated, and it reveals that high stresses are generated in the regions where the liquid remains till the very last stage of the solidification. Finally, two types of dendritic patterns are investigated, and the stresses are revealed to strongly depend on the morphology of the microstructures.},
	number = {7},
	urldate = {2018-01-26},
	journal = {Journal of Crystal Growth},
	author = {Uehara, Takuya and Fukui, Motoshi and Ohno, Nobutada},
	year = {2008},
	keywords = {A1. Computer simulation, A1. Dendrites, A1. Phase field model, A1. Solidification, A1. Stresses},
	pages = {1331--1336},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/3D43749D/Uehara et al. - 2008 - Phase field simulations of stress distributions in.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/4GX8ZN7J/S0022024807012109.html:text/html}
}

@article{chiumenti_numerical_2017,
	title = {Numerical modelling and experimental validation in {Selective} {Laser} {Melting}},
	volume = {18},
	issn = {2214-8604},
	url = {http://www.sciencedirect.com/science/article/pii/S2214860417302087},
	doi = {10.1016/j.addma.2017.09.002},
	abstract = {In this work a finite-element framework for the numerical simulation of the heat transfer analysis of additive manufacturing processes by powder-bed technologies, such as Selective Laser Melting, is presented. These kind of technologies allow for a layer-by-layer metal deposition process to cost-effectively create, directly from a CAD model, complex functional parts such as turbine blades, fuel injectors, heat exchangers, medical implants, among others. The numerical model proposed accounts for different heat dissipation mechanisms through the surrounding environment and is supplemented by a finite-element activation strategy, based on the born-dead elements technique, to follow the growth of the geometry driven by the metal deposition process, in such a way that the same scanning pattern sent to the numerical control system of the AM machine is used. An experimental campaign has been carried out at the Monash Centre for Additive Manufacturing using an EOSINT-M280 machine where it was possible to fabricate different benchmark geometries, as well as to record the temperature measurements at different thermocouple locations. The experiment consisted in the simultaneous printing of two walls with a total deposition volume of 107cm3 in 992 layers and about 33,500s build time. A large number of numerical simulations have been carried out to calibrate the thermal FE framework in terms of the thermophysical properties of both solid and powder materials and suitable boundary conditions. Furthermore, the large size of the experiment motivated the investigation of two different model reduction strategies: exclusion of the powder-bed from the computational domain and simplified scanning strategies. All these methods are analysed in terms of accuracy, computational effort and suitable applications.},
	urldate = {2018-01-26},
	journal = {Additive Manufacturing},
	author = {Chiumenti, Michele and Neiva, Eric and Salsi, Emilio and Cervera, Miguel and Badia, Santiago and Moya, Joan and Chen, Zhuoer and Lee, Caroline and Davies, Christopher},
	year = {2017},
	keywords = {Additive manufacturing (AM) process, Finite-element (FE) modelling, Heat transfer analysis, Metal deposition (MD) process, Powder-bed technologies, Selective Laser Melting (SLM)},
	pages = {171--185},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/DJS5UWPJ/Chiumenti et al. - 2017 - Numerical modelling and experimental validation in.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/D29S7LQS/S2214860417302087.html:text/html}
}

@book{rappaz_solidification_2009,
	title = {Solidification},
	isbn = {978-0-8493-8238-3},
	abstract = {Solidification is one of the oldest processes known for producing complex shapes for applications ranging from art to industry, and today it still remains one of the most important commercial technologies for many materials. Since the 1980s, numerous fundamental developments in the understanding of solidification processes and microstructure formation have been derived from both analytical theories and the application of computational techniques using commonly available powerful computers. This book integrates these developments in a comprehensive volume that also presents and places them in the context of more classical theories. Divided into three sections, the text evolves from fundamentals to applications, giving professional engineers and students a firm understanding that they can readily apply.  The first part, Fundamentals and Macroscale Phenomena, presents the thermodynamics of solutions and then builds on that subject to motivate and describe equilibrium phase diagrams. Transport phenomena are discussed next, focusing on the issues of most importance to liquid-solid phase transformations, then moving on to describe in detail both analytical and numerical approaches to solving such problems.  The second part, Microstructure, employs these fundamental concepts for the treatment of nucleation, dendritic growth, microsegregation, eutectic and peritectic solidication, and microstructure competition. This part concludes with a chapter describing the coupling of macro- and microscopic phenomena in microstructure development.  Defects, the third and final section describes various types of defects that may occur — with emphasis on porosity, hot tearing, and macrosegregation — presented using the modeling tools and microstructure descriptions developed earlier in the text. Users of this book can find software, figures, movies, and other supporting materials at the author's website by clicking on the downloads tab above.},
	language = {en},
	publisher = {EPFL Press},
	author = {Rappaz, Michel and Dantzig, Jonathan A.},
	year = {2009},
	keywords = {Nature / Natural Resources, Science / Environmental Science, Science / Physics / General, Technology \& Engineering / Materials Science, Technology \& Engineering / Metallurgy}
}

@article{meidani_3d_2012,
	title = {3D {Phase}-{Field} {Simulation} of {Micropore} {Formation} during {Solidification}: {Morphological} {Analysis}},
	volume = {33},
	issn = {1757-899X},
	shorttitle = {3D {Phase}-{Field} {Simulation} of {Micropore} {Formation} during {Solidification}},
	url = {http://stacks.iop.org/1757-899X/33/i=1/a=012104?key=crossref.846bb4bb6f57caf3f493e2d4855ce09e},
	doi = {10.1088/1757-899X/33/1/012104},
	urldate = {2018-01-26},
	journal = {IOP Conference Series: Materials Science and Engineering},
	author = {Meidani, H and Jacot, A and Rappaz, M},
	year = {2012},
	pages = {012104},
	file = {pdf.pdf:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/6EJMHFEN/pdf.pdf:application/pdf}
}

@article{mower_mechanical_2016,
	title = {Mechanical behavior of additive manufactured, powder-bed laser-fused materials},
	volume = {651},
	issn = {0921-5093},
	url = {http://www.sciencedirect.com/science/article/pii/S092150931530530X},
	doi = {10.1016/j.msea.2015.10.068},
	abstract = {Mechanical behavior of four metallic alloys fabricated with layered, laser-heated methods of additive manufacturing (AM) was compared to that of similar alloys produced with conventional methods (wrought and machined). AM materials were produced by a leading commercial service provider, as opposed to incorporating material specimens produced by unique or specially-adapted equipment. The elastic moduli were measured in flexure, stress–strain characteristics were measured in tensile deformation, and fatigue strengths were measured in fully reversed bending. The effects of fabrication orientation, surface polishing, and hot isostatic pressing upon mechanical behavior were studied. The fatigue strengths exhibited by SLM AlSi10Mg and DMLS Ti6Al4V in the as-fabricated condition proved to be significantly inferior to that of conventional material. These lower fatigue strengths are a consequence of multiple fatigue cracks initiating at surface defects, internal voids and microcracks, and growing simultaneously during cyclic loading. Measured fatigue strengths of DMLS 316L and 17-4PH approached those of corresponding wrought materials when subjected to principal stresses aligned with the build planes. When cyclic stresses were applied across the build planes of the DMLS stainless steels, fatigue fractures often developed prematurely by separation of material. Post-processing the DMLS Ti6Al4V and SS316L with hot isostatic pressure elevated the fatigue strength significantly. Measurements of surface roughness with an optical profilometer, examinations of the material microstructures, and fractography contribute to an understanding of the mechanical behavior of the additive materials.},
	urldate = {2018-01-26},
	journal = {Materials Science and Engineering: A},
	author = {Mower, Todd M. and Long, Michael J.},
	year = {2016},
	keywords = {Aluminum alloys, Cyclic fatigue, Powder-bed fusion, Stainless steel alloys, Titanium alloys},
	pages = {198--213},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/Z3WLWFJU/Mower and Long - 2016 - Mechanical behavior of additive manufactured, powd.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/Z2UWNZKL/S092150931530530X.html:text/html}
}

@article{cruzado_modeling_2017,
	title = {Modeling cyclic deformation of inconel 718 superalloy by means of crystal plasticity and computational homogenization},
	volume = {122-123},
	issn = {0020-7683},
	url = {http://www.sciencedirect.com/science/article/pii/S0020768317302792},
	doi = {10.1016/j.ijsolstr.2017.06.014},
	abstract = {A crystal plasticity computational homogenization framework is proposed to simulate the cyclic deformation of polycrystalline alloys that exhibit Bauschinger effect, mean stress relaxation, ratcheting and cyclic softening, as it happens in many Nickel based superalloys. The response of the crystals is taken into account by means of a phenomenological viscoplastic crystal plasticity model that includes the contributions of isotropic softening and kinematic hardening. The effective behavior of the polycrystal is computed through the numerical simulation of a representative volume element of the microstructure. A linear cyclic jump approach is developed in order to reduce the computational cost for simulating a large number of cycles. The model is validated for a wrought polycrystalline IN718 superalloy subjected to cyclic deformation under strain control at different cyclic strain amplitudes with Rϵ 0 and −1. The actual microstructural features (grain size and orientation distribution) are included in the model through the representative volume element of the microstructure, while the parameters of the crystal plasticity model are determined using an inverse optimization strategy based on the Levenberg–Marquardt algorithm. The model is shown to predict accurately the evolution of the stress–strain hysteresis loops with the number of cycles, as well as the mean stress relaxation and the cyclic softening observed in the experiments.},
	urldate = {2018-01-26},
	journal = {International Journal of Solids and Structures},
	author = {Cruzado, A. and LLorca, J. and Segurado, J.},
	year = {2017},
	keywords = {Computational homogenization, Crystal plasticity, Cyclic behavior, Inconel 718 superalloy, Mean stress relaxation},
	pages = {148--161},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/L69ZQSNC/Cruzado et al. - 2017 - Modeling cyclic deformation of inconel 718 superal.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/V5BF6BQP/S0020768317302792.html:text/html}
}

@article{badia_aggregated_2017,
abstract = {Unfitted finite element techniques are valuable tools in different applications where the generation of body-fitted meshes is difficult. However, these techniques are prone to severe ill conditioning problems that obstruct the efficient use of iterative Krylov methods and, in consequence, hindersthe practical usage of unfitted methods for realistic large scale applications. In this work, we present a technique that addresses such conditioning problems by constructing enhanced finite element spaces based on a cell aggregation technique. The presented method, called aggregated unfitted finite element method, is easy to implement, and can be used, in contrast to previous works, in Galerkin approximations of coercive problems with conforming Lagrangian finite element spaces. The mathematical analysis of the method states that the condition number of the resulting linear system matrix scales as in standard finite elements for body-fitted meshes, without being affected by small cut cells, and that the method leads to the optimal finite element convergence order. These theoretical results are confirmed with 2D and 3D numerical experiments.},
archivePrefix = {arXiv},
arxivId = {1709.09122},
author = {Badia, Santiago and Verdugo, Francesc and Mart{\'{i}}n, Alberto F.},
doi = {10.1016/j.cma.2018.03.022},
eprint = {1709.09122},
file = {:home/santiago/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Badia, Verdugo, Mart{\'{i}}n - 2018 - The aggregated unfitted finite element method for elliptic problems(2).pdf:pdf},
issn = {00457825},
journal = {Computer Methods in Applied Mechanics and Engineering},
keywords = {Embedded boundary methods,Ill-conditioning,Q1,Unfitted finite elements},
mendeley-tags = {Q1},
month = {jul},
pages = {533--553},
title = {{The aggregated unfitted finite element method for elliptic problems}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782518301476},
volume = {336},
year = {2018}
}

@article{badia_robust_2017,
	title = {Robust and scalable domain decomposition solvers for unfitted finite element methods},
	issn = {0377-0427},
	url = {http://www.sciencedirect.com/science/article/pii/S037704271730465X},
	doi = {10.1016/j.cam.2017.09.034},
	abstract = {Unfitted finite element methods, e.g., extended finite element techniques or the so-called finite cell method, have a great potential for large scale simulations, since they avoid the generation of body-fitted meshes and the use of graph partitioning techniques, two main bottlenecks for problems with non-trivial geometries. However, the linear systems that arise from these discretizations can be much more ill-conditioned, due to the so-called small cut cell problem. The state-of-the-art approach is to rely on sparse direct methods, which have quadratic complexity and are thus not well suited for large scale simulations. In order to solve this situation, in this work we investigate the use of domain decomposition preconditioners (balancing domain decomposition by constraints) for unfitted methods. We observe that a straightforward application of these preconditioners to the unfitted case has a very poor behavior. As a result, we propose a customization of the classical BDDC methods based on the stiffness weighting operator and an improved definition of the coarse degrees of freedom in the definition of the preconditioner. These changes lead to a robust and algorithmically scalable solver able to deal with unfitted grids. A complete set of complex 3D numerical experiments shows the good performance of the proposed preconditioners.},
	urldate = {2018-01-29},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Badia, Santiago and Verdugo, Francesc},
	year = {2017},
	keywords = {Domain decomposition, Embedded boundary methods, Linear solvers, Parallel computing, Unfitted finite elements},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/IIMRST9V/Badia and Verdugo - 2017 - Robust and scalable domain decomposition solvers f.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/7MPP6YKU/S037704271730465X.html:text/html}
}

@article{badia_stokes_2018,
abstract = {In this work, we consider unfitted finite element methods for the numerical approximation of the Stokes problem. It is well-known that these kinds of methods lead to arbitrarily ill-conditioned sys...},
author = {Badia, Santiago and Martin, Alberto F. and Verdugo, Francesc},
doi = {10.1137/18M1185624},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {65N12,65N15,65N30,Stokes,conditioning,embedded boundary,inf-sup,unfitted finite elements},
month = {jan},
number = {6},
pages = {B1541--B1576},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Mixed Aggregated Finite Element Methods for the Unfitted Discretization of the Stokes Problem}},
url = {https://epubs.siam.org/doi/10.1137/18M1185624},
volume = {40},
year = {2018}
}

@ARTICLE{badia_stokes_2018,
  author =	 {Badia, S.  and Verdugo, F. and Mart\'in, A. F.},
  title =	 {Cell aggregated unfitted finite element methods for the Stokes problem},
  journal =	 {In preparation},
  year =	 2018
}

@ARTICLE{martin_badia_2018,
  author =	 {Mart\'in, A. F. and Badia, S. and Neiva, E. and Verdugo, F.},
  title =	 {Integrating \texttt{p4est} with the \texttt{FEMPAR} finite element library},
  journal =	 {In preparation},
  year =	 2018
}



@article{peherstorfer_localized_2014,
	title = {Localized {Discrete} {Empirical} {Interpolation} {Method}},
	volume = {36},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/130924408},
	doi = {10.1137/130924408},
	language = {en},
	number = {1},
	urldate = {2018-01-30},
	journal = {SIAM Journal on Scientific Computing},
	author = {Peherstorfer, Benjamin and Butnaru, Daniel and Willcox, Karen and Bungartz, Hans-Joachim},
	year = {2014},
	pages = {A168--A192},
	file = {130924408.pdf:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/VEW95FUS/130924408.pdf:application/pdf}
}

@article{zahr_multilevel_2017,
	title = {A multilevel projection-based model order reduction framework for nonlinear dynamic multiscale problems in structural and solid mechanics},
	volume = {112},
	issn = {1097-0207},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/nme.5535/abstract},
	doi = {10.1002/nme.5535},
	abstract = {A reduction/hyper reduction framework is presented for dramatically accelerating the solution of nonlinear dynamic multiscale problems in structural and solid mechanics. At each scale, the dimensionality of the governing equations is reduced using the method of snapshots for proper orthogonal decomposition, and computational efficiency is achieved for the evaluation of the nonlinear reduced-order terms using a carefully designed configuration of the energy conserving sampling and weighting method. Periodic boundary conditions at the microscales are treated as linear multipoint constraints and reduced via projection onto the span of a basis formed from the singular value decomposition of Lagrange multiplier snapshots. Most importantly, information is efficiently transmitted between the scales without incurring high-dimensional operations. In this proposed proper orthogonal decomposition–energy conserving sampling and weighting nonlinear model reduction framework, training is performed in two steps. First, a microscale hyper reduced-order model is constructed in situ, or using a mesh coarsening strategy, in order to achieve significant speedups even in non-parametric settings. Next, a classical offline–online training approach is performed to build a parametric hyper reduced-order macroscale model, which completes the construction of a fully hyper reduced-order parametric multiscale model capable of fast and accurate multiscale simulations. A notable feature of this computational framework is the minimization, at the macroscale level, of the cost of the offline training using the in situ or coarsely trained hyper reduced-order microscale model to accelerate snapshot acquisition. The effectiveness of the proposed hyper reduction framework at accelerating the solution of nonlinear dynamic multiscale problems is demonstrated for two problems in structural and solid mechanics. Speedup factors as high as five orders of magnitude are shown to be achievable. Copyright © 2017 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {8},
	urldate = {2018-02-01},
	journal = {International Journal for Numerical Methods in Engineering},
	author = {Zahr, Matthew J. and Avery, Philip and Farhat, Charbel},
	year = {2017},
	keywords = {hyper reduction, multiscale, nonlinear model order reduction, POD},
	pages = {855--881},
	file = {Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/N7C3SI8H/Zahr et al. - 2017 - A multilevel projection-based model order reductio.pdf:application/pdf;Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/Q7DGDS8A/abstract.html:text/html}
}

@article{farhat_structure-preserving_2015,
	title = {Structure-preserving, stability, and accuracy properties of the energy-conserving sampling and weighting method for the hyper reduction of nonlinear finite element dynamic models},
	volume = {102},
	issn = {1097-0207},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/nme.4820/abstract},
	doi = {10.1002/nme.4820},
	abstract = {The computational efficiency of a typical, projection-based, nonlinear model reduction method hinges on the efficient approximation, for explicit computations, of the scalar projections onto a subspace of a residual vector. For implicit computations, it also hinges on the additional efficient approximation of similar projections of the Jacobian of this residual with respect to the solution. The computation of both approximations is often referred to in the literature as hyper reduction. To this effect, this paper focuses on the analysis and comparative performance study for nonlinear finite element reduced-order models of solids and structures of the recently developed energy-conserving mesh sampling and weighting (ECSW) hyper reduction method. Unlike most alternative approaches, this method approximates the scalar projections of residuals and/or Jacobians directly, instead of approximating first these vectors and matrices then projecting the resulting approximations onto the subspaces of interest. In this paper, it is shown that ECSW distinguishes itself furthermore from other hyper reduction methods through its preservation of the Lagrangian structure associated with Hamilton's principle. For second-order dynamical systems, this enables it to also preserve the numerical stability properties of the discrete system to which it is applied. It is also shown that for a fixed set of parameter values, the approximation error committed online by ECSW is bounded by its counterpart error committed off-line during the training of this method. Therefore, this error can be estimated in this case a priori and controlled. The performance of ECSW is demonstrated first for two academic but nevertheless interesting nonlinear dynamic response problems. For both of them, ECSW is shown to preserve numerical stability and deliver the desired level of accuracy, whereas the discrete empirical interpolation method and its recently introduced unassembled variant are shown to be susceptible to failure because of numerical instability. The potential of ECSW for enabling the effective reduction of nonlinear finite element dynamic models of solids and structures is also highlighted with the realistic simulation of the nonlinear transient dynamic response of a complete car engine to thermal and combustion pressure loads using an implicit scheme. For this simulation, ECSW is reported to enable the reduction of the CPU time required by the high-dimensional nonlinear finite element dynamic analysis by more than four orders of magnitude, while achieving a very good level of accuracy. Copyright © 2015 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2018-02-01},
	journal = {International Journal for Numerical Methods in Engineering},
	author = {Farhat, Charbel and Chapman, Todd and Avery, Philip},
	year = {2015},
	keywords = {ECSW, Galerkin projection, Hamilton's principle, hyper reduction, mesh sampling, model reduction, nonlinear dynamics, proper orthogonal decomposition, structure preserving},
	pages = {1077--1110},
	file = {Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/HBWJ68QU/Farhat et al. - 2015 - Structure-preserving, stability, and accuracy prop.pdf:application/pdf;Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/TJFIWNFP/abstract.html:text/html}
}

@article{chaturantabut_nonlinear_2010,
	title = {Nonlinear {Model} {Reduction} via {Discrete} {Empirical} {Interpolation}},
	volume = {32},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/090766498},
	doi = {10.1137/090766498},
	language = {en},
	number = {5},
	urldate = {2018-02-01},
	journal = {SIAM Journal on Scientific Computing},
	author = {Chaturantabut, Saifon and Sorensen, Danny C.},
	year = {2010},
	pages = {2737--2764},
	file = {090766498.pdf:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/DDGXSSPJ/090766498.pdf:application/pdf}
}

@article{barrault_empirical_2004,
	title = {An ‘empirical interpolation’ method: application to efficient reduced-basis discretization of partial differential equations},
	volume = {339},
	issn = {1631-073X},
	shorttitle = {An ‘empirical interpolation’ method},
	url = {http://www.sciencedirect.com/science/article/pii/S1631073X04004248},
	doi = {10.1016/j.crma.2004.08.006},
	abstract = {We present an efficient reduced-basis discretization procedure for partial differential equations with nonaffine parameter dependence. The method replaces nonaffine coefficient functions with a collateral reduced-basis expansion which then permits an (effectively affine) offline–online computational decomposition. The essential components of the approach are (i) a good collateral reduced-basis approximation space, (ii) a stable and inexpensive interpolation procedure, and (iii) an effective a posteriori estimator to quantify the newly introduced errors. Theoretical and numerical results respectively anticipate and confirm the good behavior of the technique. To cite this article: M. Barrault et al., C. R. Acad. Sci. Paris, Ser. I 339 (2004).
Résumé
Nous présentons dans cette Note une méthode rapide de base réduite pour la résolution d'équations aux dérivées partielles ayant une dépendance non affine en ses paramètres. L'approche propose de remplacer le calcul des fonctionelles non affines par un développement en base réduite annexe qui conduit à une évaluation en ligne effectivement affine. Les points essentiels de cette approche sont (i) un bon système de base réduite annexe, (ii) une méthode stable et peu coûteuse d'interpolation dans cette base, et (iii) un estimateur a posteriori pertinent pour quantifier les nouvelles erreurs introduites. Des résultats théoriques et numériques viennent anticiper puis confirmer le bon comportement de cette technique. Pour citer cet article : M. Barrault et al., C. R. Acad. Sci. Paris, Ser. I 339 (2004).},
	number = {9},
	urldate = {2018-02-01},
	journal = {Comptes Rendus Mathematique},
	author = {Barrault, Maxime and Maday, Yvon and Nguyen, Ngoc Cuong and Patera, Anthony T.},
	year = {2004},
	pages = {667--672},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/VNRVK9QP/Barrault et al. - 2004 - An ‘empirical interpolation’ method application t.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/HYF6IVHN/S1631073X04004248.html:text/html}
}

@book{bishop_pattern_2006,
	title = {Pattern {Recognition} and {Machine} {Learning}},
	isbn = {978-0-387-31073-2},
	abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	note = {Google-Books-ID: kTNoQgAACAAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Computers / Intelligence (AI) \& Semantics, Computers / Computer Graphics, Computers / Computer Vision \& Pattern Recognition}
}



@article{saye_high-order_2014,
	title = {High-order methods for computing distances to implicitly defined surfaces},
	volume = {9},
	issn = {2157-5452},
	url = {https://msp.org/camcos/2014/9-1/p03.xhtml},
	doi = {10.2140/camcos.2014.9.107},
	number = {1},
	urldate = {2018-02-05},
	journal = {Communications in Applied Mathematics and Computational Science},
	author = {Saye, Robert},
	year = {2014},
	pages = {107--141},
	file = {Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/NKXSR6ZW/p03.html:application/xhtml+xml}
}

@book{rasmussen_gaussian_2006,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2006},
	note = {OCLC: ocm61285753},
	keywords = {Data processing, Gaussian processes, Machine learning, Mathematical models},
	file = {RW.pdf:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/ZVI9CLYE/RW.pdf:application/pdf}
}


@article{badia_nonlinear_2017,
	title = {Nonlinear parallel-in-time {Schur} complement solvers for ordinary differential equations},
	issn = {0377-0427},
	url = {http://www.sciencedirect.com/science/article/pii/S0377042717304648},
	doi = {10.1016/j.cam.2017.09.033},
	abstract = {In this work, we propose a parallel-in-time solver for linear and nonlinear ordinary differential equations. The approach is based on an efficient multilevel solver of the Schur complement related to a multilevel time partition. For linear problems, the scheme leads to a fast direct method. Next, two different strategies for solving nonlinear ODEs are proposed. First, we consider a Newton method over the global nonlinear ODE, using the multilevel Schur complement solver at every nonlinear iteration. Second, we state the global nonlinear problem in terms of the nonlinear Schur complement (at an arbitrary level), and perform nonlinear iterations over it. Numerical experiments show that the proposed schemes are weakly scalable, i.e., we can efficiently exploit increasing computational resources to solve for more time steps the same problem.},
	urldate = {2018-02-05},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Badia, Santiago and Olm, Marc},
	year = {2017},
	keywords = {Time parallelism, Nonlinear solver, Domain decomposition, Ordinary differential equations, Scalability},
	file = {ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/FYX3ECTB/S0377042717304648.html:text/html}
}




@article{cruzado_multiscale_2015,
	title = {Multiscale modeling of the mechanical behavior of {IN}718 superalloy based on micropillar compression and computational homogenization},
	volume = {98},
	issn = {1359-6454},
	url = {http://www.sciencedirect.com/science/article/pii/S1359645415004681},
	doi = {10.1016/j.actamat.2015.07.006},
	abstract = {A multiscale modeling strategy is presented to determine the effective mechanical properties of polycrystalline Ni-based superalloys. They are obtained by computational homogenization of a representative volume element of the microstructure which was built from the grain size, shape and orientation distributions of the material. The mechanical behavior of each grain was simulated by means of a crystal plasticity model, and the model parameters that dictate the evolution of the critical resolved shear stress in each slip system (including viscoplastic effects as well as self and latent hardening) were obtained from compression tests in micropillars milled from grains of the polycrystal in different orientations suited for single, double (coplanar and non coplanar) and multiple slip. The multiscale model predictions of the compressive strength of wrought IN718 were in good agreement with the experimental results.},
	urldate = {2018-02-08},
	journal = {Acta Materialia},
	author = {Cruzado, A. and Gan, B. and Jiménez, M. and Barba, D. and Ostolaza, K. and Linaza, A. and Molina-Aldareguia, J. M. and Llorca, J. and Segurado, J.},
	year = {2015},
	keywords = {Computational homogenization, Crystal plasticity, Micropillar, Multiscale modeling, Ni-based superalloys},
	pages = {242--253},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/HAD39MZE/Cruzado et al. - 2015 - Multiscale modeling of the mechanical behavior of .pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/3YWQD4ZR/S1359645415004681.html:text/html}
}

@article{cruzado_microstructure-based_2018,
	title = {Microstructure-based fatigue life model of metallic alloys with bilinear {Coffin}-{Manson} behavior},
	volume = {107},
	issn = {0142-1123},
	url = {http://www.sciencedirect.com/science/article/pii/S0142112317304061},
	doi = {10.1016/j.ijfatigue.2017.10.014},
	abstract = {A microstructure-based model is presented to predict the fatigue life of polycrystalline metallic alloys which present a bilinear Coffin-Manson relationship. The model is based in the determination of the maximum value of a fatigue indicator parameter obtained from the plastic energy dissipated by cycle in the microstructure. The fatigue indicator parameter was obtained by means of the computational homogenization of a representative volume element of the microstructure using a crystal-plasticity finite element model. The microstructure-based model was applied to predict the low cyclic fatigue behavior of IN718 alloy at 400 °C which exhibits a bilinear Coffin-Manson relationship under the assumption that this behavior is triggered by a transition from highly localized plasticity at low cyclic strain ranges to more homogeneous deformation at high cyclic strain ranges. The model predictions were in very good agreement with the experimental results for a wide range of cyclic strain ranges and two strain ratios (Rε=0 and −1) and corroborated the initial hypothesis. Moreover, they provided a micromechanical explanation for the influence of the strain ratio on the fatigue life at low cyclic strain ranges.},
	urldate = {2018-02-08},
	journal = {International Journal of Fatigue},
	author = {Cruzado, A. and Lucarini, S. and LLorca, J. and Segurado, J.},
	year = {2018},
	keywords = {Bilinear Coffin-Manson relationship, Computational homogenization, Crystal plasticity, IN718, Low cycle fatigue, Microstructure, Polycrystal},
	pages = {40--48},
	file = {ScienceDirect Full Text PDF:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/H67TET6W/Cruzado et al. - 2018 - Microstructure-based fatigue life model of metalli.pdf:application/pdf;ScienceDirect Snapshot:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/FY96I56T/S0142112317304061.html:text/html}
}




@unpublished{lucarini_accuracy_2018,
	title = {On the accuracy of spectral solvers for micromechanics based fatigue modeling},
	abstract = {},
        year = {2018},
	urldate = {2017-01-10},
	date = {2016-11},
	note = {Submitted to Computational Mechanics},
	author = {Lucarini, S. and Segurado, J.},
	keywords = {adaptive coarse space, {BDDC}, heterogeneous problem, parallel, parallel solver, preconditioner},
	file = {HAL PDF Full Text:/home/sbadia/.zotero/zotero/q366twt9.default/zotero/storage/QCT6UEC4/Badia et al. - 2016 - PHYSICS-BASED BALANCING DOMAIN DECOMPOSITION BY CO.pdf:application/pdf}
}
